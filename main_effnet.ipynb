{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento - 2024-12-01 22:28:51\n",
      "Configuración del entrenamiento:\n",
      "Batch size: 128\n",
      "Epochs: 400\n",
      "Learning rate: 0.1\n",
      "Weight decay: 1e-05\n",
      "Data augmentation: True\n",
      "GPU disponible: True\n",
      "--------------------------------------------------\n",
      "\n",
      "Arquitectura del modelo:\n",
      "EfficientNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
      "      )\n",
      "      (3): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (8): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=True)\n",
      "    (1): Linear(in_features=1280, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "--------------------------------------------------\n",
      "Using train transform: Compose(\n",
      "    RandomCrop(size=(32, 32), padding=4)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))\n",
      ")\n",
      "Using validation transform: Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))\n",
      ")\n",
      "0.1\n",
      "Epoch 1/400\n",
      "Iteración 35 - Lote 35/352 - Pérdida de Entrenamiento: 5.1093, Precisión de Entrenamiento: 0.1047\n",
      "Iteración 70 - Lote 70/352 - Pérdida de Entrenamiento: 4.7744, Precisión de Entrenamiento: 0.1009\n",
      "Iteración 105 - Lote 105/352 - Pérdida de Entrenamiento: 4.3715, Precisión de Entrenamiento: 0.1018\n",
      "Iteración 140 - Lote 140/352 - Pérdida de Entrenamiento: 4.0917, Precisión de Entrenamiento: 0.1026\n",
      "Iteración 175 - Lote 175/352 - Pérdida de Entrenamiento: 3.9367, Precisión de Entrenamiento: 0.1021\n",
      "Iteración 210 - Lote 210/352 - Pérdida de Entrenamiento: 3.7675, Precisión de Entrenamiento: 0.1024\n",
      "Iteración 245 - Lote 245/352 - Pérdida de Entrenamiento: 3.6267, Precisión de Entrenamiento: 0.1041\n",
      "Iteración 280 - Lote 280/352 - Pérdida de Entrenamiento: 3.5192, Precisión de Entrenamiento: 0.1062\n",
      "Iteración 315 - Lote 315/352 - Pérdida de Entrenamiento: 3.4168, Precisión de Entrenamiento: 0.1083\n",
      "Iteración 350 - Lote 350/352 - Pérdida de Entrenamiento: 3.3322, Precisión de Entrenamiento: 0.1111\n",
      "Val loss: 2.3148, Val acc: 0.1416\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_1.pth\n",
      "Checkpoint del mejor modelo guardado en la época 1\n",
      "Gradientes para features.0.0.weight: min=-0.005421748850494623, max=0.006882319692522287, mean=1.2939923180965707e-05, std=0.0005350033170543611\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.0017782548675313592, max=0.001841292018070817, mean=-1.5070099834701978e-07, std=7.136815111152828e-05\n",
      "Gradientes para classifier.1.weight: min=-0.0929640531539917, max=0.09092315286397934, mean=2.2351741291171123e-10, std=0.00738780340179801\n",
      "0.1\n",
      "Epoch 2/400\n",
      "Iteración 387 - Lote 35/352 - Pérdida de Entrenamiento: 2.4401, Precisión de Entrenamiento: 0.1357\n",
      "Iteración 422 - Lote 70/352 - Pérdida de Entrenamiento: 2.4040, Precisión de Entrenamiento: 0.1478\n",
      "Iteración 457 - Lote 105/352 - Pérdida de Entrenamiento: 2.3782, Precisión de Entrenamiento: 0.1535\n",
      "Iteración 492 - Lote 140/352 - Pérdida de Entrenamiento: 2.3471, Precisión de Entrenamiento: 0.1600\n",
      "Iteración 527 - Lote 175/352 - Pérdida de Entrenamiento: 2.3220, Precisión de Entrenamiento: 0.1641\n",
      "Iteración 562 - Lote 210/352 - Pérdida de Entrenamiento: 2.2977, Precisión de Entrenamiento: 0.1699\n",
      "Iteración 597 - Lote 245/352 - Pérdida de Entrenamiento: 2.2773, Precisión de Entrenamiento: 0.1732\n",
      "Iteración 632 - Lote 280/352 - Pérdida de Entrenamiento: 2.2580, Precisión de Entrenamiento: 0.1755\n",
      "Iteración 667 - Lote 315/352 - Pérdida de Entrenamiento: 2.2426, Precisión de Entrenamiento: 0.1785\n",
      "Iteración 702 - Lote 350/352 - Pérdida de Entrenamiento: 2.2297, Precisión de Entrenamiento: 0.1804\n",
      "Val loss: 2.0792, Val acc: 0.2032\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_2.pth\n",
      "Checkpoint del mejor modelo guardado en la época 2\n",
      "0.1\n",
      "Epoch 3/400\n",
      "Iteración 739 - Lote 35/352 - Pérdida de Entrenamiento: 2.1073, Precisión de Entrenamiento: 0.2051\n",
      "Iteración 774 - Lote 70/352 - Pérdida de Entrenamiento: 2.1027, Precisión de Entrenamiento: 0.2031\n",
      "Iteración 809 - Lote 105/352 - Pérdida de Entrenamiento: 2.0956, Precisión de Entrenamiento: 0.2054\n",
      "Iteración 844 - Lote 140/352 - Pérdida de Entrenamiento: 2.0923, Precisión de Entrenamiento: 0.2092\n",
      "Iteración 879 - Lote 175/352 - Pérdida de Entrenamiento: 2.0913, Precisión de Entrenamiento: 0.2117\n",
      "Iteración 914 - Lote 210/352 - Pérdida de Entrenamiento: 2.0897, Precisión de Entrenamiento: 0.2135\n",
      "Iteración 949 - Lote 245/352 - Pérdida de Entrenamiento: 2.0850, Precisión de Entrenamiento: 0.2149\n",
      "Iteración 984 - Lote 280/352 - Pérdida de Entrenamiento: 2.0820, Precisión de Entrenamiento: 0.2157\n",
      "Iteración 1019 - Lote 315/352 - Pérdida de Entrenamiento: 2.0798, Precisión de Entrenamiento: 0.2159\n",
      "Iteración 1054 - Lote 350/352 - Pérdida de Entrenamiento: 2.0774, Precisión de Entrenamiento: 0.2169\n",
      "Val loss: 2.0380, Val acc: 0.2344\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_3.pth\n",
      "Checkpoint del mejor modelo guardado en la época 3\n",
      "0.1\n",
      "Epoch 4/400\n",
      "Iteración 1091 - Lote 35/352 - Pérdida de Entrenamiento: 2.0499, Precisión de Entrenamiento: 0.2194\n",
      "Iteración 1126 - Lote 70/352 - Pérdida de Entrenamiento: 2.0548, Precisión de Entrenamiento: 0.2210\n",
      "Iteración 1161 - Lote 105/352 - Pérdida de Entrenamiento: 2.0518, Precisión de Entrenamiento: 0.2255\n",
      "Iteración 1196 - Lote 140/352 - Pérdida de Entrenamiento: 2.0483, Precisión de Entrenamiento: 0.2268\n",
      "Iteración 1231 - Lote 175/352 - Pérdida de Entrenamiento: 2.0467, Precisión de Entrenamiento: 0.2283\n",
      "Iteración 1266 - Lote 210/352 - Pérdida de Entrenamiento: 2.0431, Precisión de Entrenamiento: 0.2291\n",
      "Iteración 1301 - Lote 245/352 - Pérdida de Entrenamiento: 2.0380, Precisión de Entrenamiento: 0.2314\n",
      "Iteración 1336 - Lote 280/352 - Pérdida de Entrenamiento: 2.0353, Precisión de Entrenamiento: 0.2332\n",
      "Iteración 1371 - Lote 315/352 - Pérdida de Entrenamiento: 2.0315, Precisión de Entrenamiento: 0.2351\n",
      "Iteración 1406 - Lote 350/352 - Pérdida de Entrenamiento: 2.0308, Precisión de Entrenamiento: 0.2349\n",
      "Val loss: 1.9951, Val acc: 0.2558\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_4.pth\n",
      "Checkpoint del mejor modelo guardado en la época 4\n",
      "0.1\n",
      "Epoch 5/400\n",
      "Iteración 1443 - Lote 35/352 - Pérdida de Entrenamiento: 1.9985, Precisión de Entrenamiento: 0.2511\n",
      "Iteración 1478 - Lote 70/352 - Pérdida de Entrenamiento: 1.9935, Precisión de Entrenamiento: 0.2555\n",
      "Iteración 1513 - Lote 105/352 - Pérdida de Entrenamiento: 1.9915, Precisión de Entrenamiento: 0.2562\n",
      "Iteración 1548 - Lote 140/352 - Pérdida de Entrenamiento: 1.9917, Precisión de Entrenamiento: 0.2555\n",
      "Iteración 1583 - Lote 175/352 - Pérdida de Entrenamiento: 1.9956, Precisión de Entrenamiento: 0.2545\n",
      "Iteración 1618 - Lote 210/352 - Pérdida de Entrenamiento: 1.9921, Precisión de Entrenamiento: 0.2558\n",
      "Iteración 1653 - Lote 245/352 - Pérdida de Entrenamiento: 1.9905, Precisión de Entrenamiento: 0.2545\n",
      "Iteración 1688 - Lote 280/352 - Pérdida de Entrenamiento: 1.9870, Precisión de Entrenamiento: 0.2564\n",
      "Iteración 1723 - Lote 315/352 - Pérdida de Entrenamiento: 1.9854, Precisión de Entrenamiento: 0.2579\n",
      "Iteración 1758 - Lote 350/352 - Pérdida de Entrenamiento: 1.9826, Precisión de Entrenamiento: 0.2589\n",
      "Val loss: 1.9282, Val acc: 0.2908\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_5.pth\n",
      "Checkpoint del mejor modelo guardado en la época 5\n",
      "Gradientes para features.0.0.weight: min=-0.0022583389654755592, max=0.0010193277848884463, mean=-2.9493667170754634e-05, std=0.00029836627072654665\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.0057152071967720985, max=0.008985457010567188, mean=1.2173284176242305e-06, std=0.0003231540904380381\n",
      "Gradientes para classifier.1.weight: min=-0.012163697741925716, max=0.010732799768447876, mean=5.8207661780829145e-12, std=0.0016397637082263827\n",
      "0.1\n",
      "Epoch 6/400\n",
      "Iteración 1795 - Lote 35/352 - Pérdida de Entrenamiento: 1.9489, Precisión de Entrenamiento: 0.2719\n",
      "Iteración 1830 - Lote 70/352 - Pérdida de Entrenamiento: 1.9549, Precisión de Entrenamiento: 0.2711\n",
      "Iteración 1865 - Lote 105/352 - Pérdida de Entrenamiento: 1.9547, Precisión de Entrenamiento: 0.2714\n",
      "Iteración 1900 - Lote 140/352 - Pérdida de Entrenamiento: 1.9593, Precisión de Entrenamiento: 0.2694\n",
      "Iteración 1935 - Lote 175/352 - Pérdida de Entrenamiento: 1.9547, Precisión de Entrenamiento: 0.2714\n",
      "Iteración 1970 - Lote 210/352 - Pérdida de Entrenamiento: 1.9512, Precisión de Entrenamiento: 0.2710\n",
      "Iteración 2005 - Lote 245/352 - Pérdida de Entrenamiento: 1.9489, Precisión de Entrenamiento: 0.2707\n",
      "Iteración 2040 - Lote 280/352 - Pérdida de Entrenamiento: 1.9457, Precisión de Entrenamiento: 0.2717\n",
      "Iteración 2075 - Lote 315/352 - Pérdida de Entrenamiento: 1.9430, Precisión de Entrenamiento: 0.2730\n",
      "Iteración 2110 - Lote 350/352 - Pérdida de Entrenamiento: 1.9397, Precisión de Entrenamiento: 0.2748\n",
      "Val loss: 1.8792, Val acc: 0.2850\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_6.pth\n",
      "Checkpoint del mejor modelo guardado en la época 6\n",
      "0.1\n",
      "Epoch 7/400\n",
      "Iteración 2147 - Lote 35/352 - Pérdida de Entrenamiento: 1.9157, Precisión de Entrenamiento: 0.2877\n",
      "Iteración 2182 - Lote 70/352 - Pérdida de Entrenamiento: 1.9126, Precisión de Entrenamiento: 0.2876\n",
      "Iteración 2217 - Lote 105/352 - Pérdida de Entrenamiento: 1.9004, Precisión de Entrenamiento: 0.2900\n",
      "Iteración 2252 - Lote 140/352 - Pérdida de Entrenamiento: 1.8980, Precisión de Entrenamiento: 0.2901\n",
      "Iteración 2287 - Lote 175/352 - Pérdida de Entrenamiento: 1.8967, Precisión de Entrenamiento: 0.2904\n",
      "Iteración 2322 - Lote 210/352 - Pérdida de Entrenamiento: 1.8924, Precisión de Entrenamiento: 0.2920\n",
      "Iteración 2357 - Lote 245/352 - Pérdida de Entrenamiento: 1.8893, Precisión de Entrenamiento: 0.2938\n",
      "Iteración 2392 - Lote 280/352 - Pérdida de Entrenamiento: 1.8844, Precisión de Entrenamiento: 0.2941\n",
      "Iteración 2427 - Lote 315/352 - Pérdida de Entrenamiento: 1.8821, Precisión de Entrenamiento: 0.2941\n",
      "Iteración 2462 - Lote 350/352 - Pérdida de Entrenamiento: 1.8759, Precisión de Entrenamiento: 0.2973\n",
      "Val loss: 1.8111, Val acc: 0.3264\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_7.pth\n",
      "Checkpoint del mejor modelo guardado en la época 7\n",
      "0.1\n",
      "Epoch 8/400\n",
      "Iteración 2499 - Lote 35/352 - Pérdida de Entrenamiento: 1.8218, Precisión de Entrenamiento: 0.3163\n",
      "Iteración 2534 - Lote 70/352 - Pérdida de Entrenamiento: 1.8346, Precisión de Entrenamiento: 0.3127\n",
      "Iteración 2569 - Lote 105/352 - Pérdida de Entrenamiento: 1.8364, Precisión de Entrenamiento: 0.3112\n",
      "Iteración 2604 - Lote 140/352 - Pérdida de Entrenamiento: 1.8403, Precisión de Entrenamiento: 0.3085\n",
      "Iteración 2639 - Lote 175/352 - Pérdida de Entrenamiento: 1.8401, Precisión de Entrenamiento: 0.3090\n",
      "Iteración 2674 - Lote 210/352 - Pérdida de Entrenamiento: 1.8382, Precisión de Entrenamiento: 0.3096\n",
      "Iteración 2709 - Lote 245/352 - Pérdida de Entrenamiento: 1.8328, Precisión de Entrenamiento: 0.3117\n",
      "Iteración 2744 - Lote 280/352 - Pérdida de Entrenamiento: 1.8314, Precisión de Entrenamiento: 0.3130\n",
      "Iteración 2779 - Lote 315/352 - Pérdida de Entrenamiento: 1.8274, Precisión de Entrenamiento: 0.3133\n",
      "Iteración 2814 - Lote 350/352 - Pérdida de Entrenamiento: 1.8239, Precisión de Entrenamiento: 0.3152\n",
      "Val loss: 1.7776, Val acc: 0.3366\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_8.pth\n",
      "Checkpoint del mejor modelo guardado en la época 8\n",
      "0.1\n",
      "Epoch 9/400\n",
      "Iteración 2851 - Lote 35/352 - Pérdida de Entrenamiento: 1.7887, Precisión de Entrenamiento: 0.3433\n",
      "Iteración 2886 - Lote 70/352 - Pérdida de Entrenamiento: 1.7952, Precisión de Entrenamiento: 0.3325\n",
      "Iteración 2921 - Lote 105/352 - Pérdida de Entrenamiento: 1.7955, Precisión de Entrenamiento: 0.3289\n",
      "Iteración 2956 - Lote 140/352 - Pérdida de Entrenamiento: 1.7938, Precisión de Entrenamiento: 0.3271\n",
      "Iteración 2991 - Lote 175/352 - Pérdida de Entrenamiento: 1.7943, Precisión de Entrenamiento: 0.3279\n",
      "Iteración 3026 - Lote 210/352 - Pérdida de Entrenamiento: 1.7941, Precisión de Entrenamiento: 0.3278\n",
      "Iteración 3061 - Lote 245/352 - Pérdida de Entrenamiento: 1.7931, Precisión de Entrenamiento: 0.3289\n",
      "Iteración 3096 - Lote 280/352 - Pérdida de Entrenamiento: 1.7935, Precisión de Entrenamiento: 0.3289\n",
      "Iteración 3131 - Lote 315/352 - Pérdida de Entrenamiento: 1.7926, Precisión de Entrenamiento: 0.3283\n",
      "Iteración 3166 - Lote 350/352 - Pérdida de Entrenamiento: 1.7892, Precisión de Entrenamiento: 0.3302\n",
      "Val loss: 1.7182, Val acc: 0.3550\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_9.pth\n",
      "Checkpoint del mejor modelo guardado en la época 9\n",
      "0.1\n",
      "Epoch 10/400\n",
      "Iteración 3203 - Lote 35/352 - Pérdida de Entrenamiento: 1.7534, Precisión de Entrenamiento: 0.3263\n",
      "Iteración 3238 - Lote 70/352 - Pérdida de Entrenamiento: 1.7759, Precisión de Entrenamiento: 0.3262\n",
      "Iteración 3273 - Lote 105/352 - Pérdida de Entrenamiento: 1.7716, Precisión de Entrenamiento: 0.3307\n",
      "Iteración 3308 - Lote 140/352 - Pérdida de Entrenamiento: 1.7689, Precisión de Entrenamiento: 0.3364\n",
      "Iteración 3343 - Lote 175/352 - Pérdida de Entrenamiento: 1.7704, Precisión de Entrenamiento: 0.3364\n",
      "Iteración 3378 - Lote 210/352 - Pérdida de Entrenamiento: 1.7705, Precisión de Entrenamiento: 0.3376\n",
      "Iteración 3413 - Lote 245/352 - Pérdida de Entrenamiento: 1.7719, Precisión de Entrenamiento: 0.3373\n",
      "Iteración 3448 - Lote 280/352 - Pérdida de Entrenamiento: 1.7685, Precisión de Entrenamiento: 0.3382\n",
      "Iteración 3483 - Lote 315/352 - Pérdida de Entrenamiento: 1.7651, Precisión de Entrenamiento: 0.3384\n",
      "Iteración 3518 - Lote 350/352 - Pérdida de Entrenamiento: 1.7643, Precisión de Entrenamiento: 0.3382\n",
      "Val loss: 1.7109, Val acc: 0.3554\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_10.pth\n",
      "Checkpoint del mejor modelo guardado en la época 10\n",
      "Gradientes para features.0.0.weight: min=-0.004353490192443132, max=0.010724661871790886, mean=0.00019126634288113564, std=0.0012440605787560344\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.012456735596060753, max=0.0112070981413126, mean=-9.948737300646826e-08, std=0.00035955148632638156\n",
      "Gradientes para classifier.1.weight: min=-0.00746074179187417, max=0.008160335011780262, mean=1.8626451075975936e-11, std=0.0012915647821500897\n",
      "0.1\n",
      "Epoch 11/400\n",
      "Iteración 3555 - Lote 35/352 - Pérdida de Entrenamiento: 1.7546, Precisión de Entrenamiento: 0.3489\n",
      "Iteración 3590 - Lote 70/352 - Pérdida de Entrenamiento: 1.7546, Precisión de Entrenamiento: 0.3517\n",
      "Iteración 3625 - Lote 105/352 - Pérdida de Entrenamiento: 1.7491, Precisión de Entrenamiento: 0.3542\n",
      "Iteración 3660 - Lote 140/352 - Pérdida de Entrenamiento: 1.7432, Precisión de Entrenamiento: 0.3544\n",
      "Iteración 3695 - Lote 175/352 - Pérdida de Entrenamiento: 1.7417, Precisión de Entrenamiento: 0.3521\n",
      "Iteración 3730 - Lote 210/352 - Pérdida de Entrenamiento: 1.7423, Precisión de Entrenamiento: 0.3518\n",
      "Iteración 3765 - Lote 245/352 - Pérdida de Entrenamiento: 1.7445, Precisión de Entrenamiento: 0.3513\n",
      "Iteración 3800 - Lote 280/352 - Pérdida de Entrenamiento: 1.7415, Precisión de Entrenamiento: 0.3521\n",
      "Iteración 3835 - Lote 315/352 - Pérdida de Entrenamiento: 1.7417, Precisión de Entrenamiento: 0.3521\n",
      "Iteración 3870 - Lote 350/352 - Pérdida de Entrenamiento: 1.7390, Precisión de Entrenamiento: 0.3518\n",
      "Val loss: 1.6706, Val acc: 0.3722\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_11.pth\n",
      "Checkpoint del mejor modelo guardado en la época 11\n",
      "0.1\n",
      "Epoch 12/400\n",
      "Iteración 3907 - Lote 35/352 - Pérdida de Entrenamiento: 1.7300, Precisión de Entrenamiento: 0.3558\n",
      "Iteración 3942 - Lote 70/352 - Pérdida de Entrenamiento: 1.7326, Precisión de Entrenamiento: 0.3551\n",
      "Iteración 3977 - Lote 105/352 - Pérdida de Entrenamiento: 1.7199, Precisión de Entrenamiento: 0.3594\n",
      "Iteración 4012 - Lote 140/352 - Pérdida de Entrenamiento: 1.7224, Precisión de Entrenamiento: 0.3610\n",
      "Iteración 4047 - Lote 175/352 - Pérdida de Entrenamiento: 1.7214, Precisión de Entrenamiento: 0.3608\n",
      "Iteración 4082 - Lote 210/352 - Pérdida de Entrenamiento: 1.7247, Precisión de Entrenamiento: 0.3592\n",
      "Iteración 4117 - Lote 245/352 - Pérdida de Entrenamiento: 1.7263, Precisión de Entrenamiento: 0.3572\n",
      "Iteración 4152 - Lote 280/352 - Pérdida de Entrenamiento: 1.7242, Precisión de Entrenamiento: 0.3581\n",
      "Iteración 4187 - Lote 315/352 - Pérdida de Entrenamiento: 1.7206, Precisión de Entrenamiento: 0.3598\n",
      "Iteración 4222 - Lote 350/352 - Pérdida de Entrenamiento: 1.7219, Precisión de Entrenamiento: 0.3600\n",
      "Val loss: 1.6437, Val acc: 0.3874\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_12.pth\n",
      "Checkpoint del mejor modelo guardado en la época 12\n",
      "0.1\n",
      "Epoch 13/400\n",
      "Iteración 4259 - Lote 35/352 - Pérdida de Entrenamiento: 1.7106, Precisión de Entrenamiento: 0.3520\n",
      "Iteración 4294 - Lote 70/352 - Pérdida de Entrenamiento: 1.7117, Precisión de Entrenamiento: 0.3507\n",
      "Iteración 4329 - Lote 105/352 - Pérdida de Entrenamiento: 1.7098, Precisión de Entrenamiento: 0.3543\n",
      "Iteración 4364 - Lote 140/352 - Pérdida de Entrenamiento: 1.7140, Precisión de Entrenamiento: 0.3550\n",
      "Iteración 4399 - Lote 175/352 - Pérdida de Entrenamiento: 1.7130, Precisión de Entrenamiento: 0.3568\n",
      "Iteración 4434 - Lote 210/352 - Pérdida de Entrenamiento: 1.7129, Precisión de Entrenamiento: 0.3571\n",
      "Iteración 4469 - Lote 245/352 - Pérdida de Entrenamiento: 1.7090, Precisión de Entrenamiento: 0.3598\n",
      "Iteración 4504 - Lote 280/352 - Pérdida de Entrenamiento: 1.7090, Precisión de Entrenamiento: 0.3601\n",
      "Iteración 4539 - Lote 315/352 - Pérdida de Entrenamiento: 1.7075, Precisión de Entrenamiento: 0.3608\n",
      "Iteración 4574 - Lote 350/352 - Pérdida de Entrenamiento: 1.7084, Precisión de Entrenamiento: 0.3608\n",
      "Val loss: 1.6496, Val acc: 0.3912\n",
      "0.1\n",
      "Epoch 14/400\n",
      "Iteración 4611 - Lote 35/352 - Pérdida de Entrenamiento: 1.6966, Precisión de Entrenamiento: 0.3699\n",
      "Iteración 4646 - Lote 70/352 - Pérdida de Entrenamiento: 1.6890, Precisión de Entrenamiento: 0.3710\n",
      "Iteración 4681 - Lote 105/352 - Pérdida de Entrenamiento: 1.6877, Precisión de Entrenamiento: 0.3711\n",
      "Iteración 4716 - Lote 140/352 - Pérdida de Entrenamiento: 1.6879, Precisión de Entrenamiento: 0.3710\n",
      "Iteración 4751 - Lote 175/352 - Pérdida de Entrenamiento: 1.6857, Precisión de Entrenamiento: 0.3713\n",
      "Iteración 4786 - Lote 210/352 - Pérdida de Entrenamiento: 1.6885, Precisión de Entrenamiento: 0.3699\n",
      "Iteración 4821 - Lote 245/352 - Pérdida de Entrenamiento: 1.6878, Precisión de Entrenamiento: 0.3700\n",
      "Iteración 4856 - Lote 280/352 - Pérdida de Entrenamiento: 1.6900, Precisión de Entrenamiento: 0.3716\n",
      "Iteración 4891 - Lote 315/352 - Pérdida de Entrenamiento: 1.6902, Precisión de Entrenamiento: 0.3705\n",
      "Iteración 4926 - Lote 350/352 - Pérdida de Entrenamiento: 1.6896, Precisión de Entrenamiento: 0.3700\n",
      "Val loss: 1.6105, Val acc: 0.4000\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_14.pth\n",
      "Checkpoint del mejor modelo guardado en la época 14\n",
      "0.1\n",
      "Epoch 15/400\n",
      "Iteración 4963 - Lote 35/352 - Pérdida de Entrenamiento: 1.6707, Precisión de Entrenamiento: 0.3864\n",
      "Iteración 4998 - Lote 70/352 - Pérdida de Entrenamiento: 1.6833, Precisión de Entrenamiento: 0.3776\n",
      "Iteración 5033 - Lote 105/352 - Pérdida de Entrenamiento: 1.6782, Precisión de Entrenamiento: 0.3784\n",
      "Iteración 5068 - Lote 140/352 - Pérdida de Entrenamiento: 1.6812, Precisión de Entrenamiento: 0.3754\n",
      "Iteración 5103 - Lote 175/352 - Pérdida de Entrenamiento: 1.6804, Precisión de Entrenamiento: 0.3762\n",
      "Iteración 5138 - Lote 210/352 - Pérdida de Entrenamiento: 1.6775, Precisión de Entrenamiento: 0.3765\n",
      "Iteración 5173 - Lote 245/352 - Pérdida de Entrenamiento: 1.6755, Precisión de Entrenamiento: 0.3763\n",
      "Iteración 5208 - Lote 280/352 - Pérdida de Entrenamiento: 1.6741, Precisión de Entrenamiento: 0.3770\n",
      "Iteración 5243 - Lote 315/352 - Pérdida de Entrenamiento: 1.6720, Precisión de Entrenamiento: 0.3779\n",
      "Iteración 5278 - Lote 350/352 - Pérdida de Entrenamiento: 1.6712, Precisión de Entrenamiento: 0.3782\n",
      "Val loss: 1.5985, Val acc: 0.4062\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_15.pth\n",
      "Checkpoint del mejor modelo guardado en la época 15\n",
      "Gradientes para features.0.0.weight: min=-0.006074956618249416, max=0.005423970054835081, mean=-4.149952656007372e-05, std=0.0007409884128719568\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.009946975857019424, max=0.013882679864764214, mean=3.0485800834867405e-06, std=0.00047951313899829984\n",
      "Gradientes para classifier.1.weight: min=-0.011301860213279724, max=0.009235724806785583, mean=1.1641532356165829e-11, std=0.0016904504736885428\n",
      "0.1\n",
      "Epoch 16/400\n",
      "Iteración 5315 - Lote 35/352 - Pérdida de Entrenamiento: 1.6277, Precisión de Entrenamiento: 0.3926\n",
      "Iteración 5350 - Lote 70/352 - Pérdida de Entrenamiento: 1.6527, Precisión de Entrenamiento: 0.3816\n",
      "Iteración 5385 - Lote 105/352 - Pérdida de Entrenamiento: 1.6631, Precisión de Entrenamiento: 0.3806\n",
      "Iteración 5420 - Lote 140/352 - Pérdida de Entrenamiento: 1.6657, Precisión de Entrenamiento: 0.3795\n",
      "Iteración 5455 - Lote 175/352 - Pérdida de Entrenamiento: 1.6720, Precisión de Entrenamiento: 0.3788\n",
      "Iteración 5490 - Lote 210/352 - Pérdida de Entrenamiento: 1.6644, Precisión de Entrenamiento: 0.3828\n",
      "Iteración 5525 - Lote 245/352 - Pérdida de Entrenamiento: 1.6613, Precisión de Entrenamiento: 0.3851\n",
      "Iteración 5560 - Lote 280/352 - Pérdida de Entrenamiento: 1.6594, Precisión de Entrenamiento: 0.3845\n",
      "Iteración 5595 - Lote 315/352 - Pérdida de Entrenamiento: 1.6609, Precisión de Entrenamiento: 0.3844\n",
      "Iteración 5630 - Lote 350/352 - Pérdida de Entrenamiento: 1.6595, Precisión de Entrenamiento: 0.3842\n",
      "Val loss: 1.6148, Val acc: 0.3992\n",
      "0.1\n",
      "Epoch 17/400\n",
      "Iteración 5667 - Lote 35/352 - Pérdida de Entrenamiento: 1.6619, Precisión de Entrenamiento: 0.3775\n",
      "Iteración 5702 - Lote 70/352 - Pérdida de Entrenamiento: 1.6785, Precisión de Entrenamiento: 0.3801\n",
      "Iteración 5737 - Lote 105/352 - Pérdida de Entrenamiento: 1.6876, Precisión de Entrenamiento: 0.3759\n",
      "Iteración 5772 - Lote 140/352 - Pérdida de Entrenamiento: 1.6784, Precisión de Entrenamiento: 0.3771\n",
      "Iteración 5807 - Lote 175/352 - Pérdida de Entrenamiento: 1.6721, Precisión de Entrenamiento: 0.3785\n",
      "Iteración 5842 - Lote 210/352 - Pérdida de Entrenamiento: 1.6693, Precisión de Entrenamiento: 0.3812\n",
      "Iteración 5877 - Lote 245/352 - Pérdida de Entrenamiento: 1.6707, Precisión de Entrenamiento: 0.3809\n",
      "Iteración 5912 - Lote 280/352 - Pérdida de Entrenamiento: 1.6697, Precisión de Entrenamiento: 0.3808\n",
      "Iteración 5947 - Lote 315/352 - Pérdida de Entrenamiento: 1.6657, Precisión de Entrenamiento: 0.3824\n",
      "Iteración 5982 - Lote 350/352 - Pérdida de Entrenamiento: 1.6657, Precisión de Entrenamiento: 0.3832\n",
      "Val loss: 1.5755, Val acc: 0.4222\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_17.pth\n",
      "Checkpoint del mejor modelo guardado en la época 17\n",
      "0.1\n",
      "Epoch 18/400\n",
      "Iteración 6019 - Lote 35/352 - Pérdida de Entrenamiento: 1.6451, Precisión de Entrenamiento: 0.3893\n",
      "Iteración 6054 - Lote 70/352 - Pérdida de Entrenamiento: 1.6533, Precisión de Entrenamiento: 0.3855\n",
      "Iteración 6089 - Lote 105/352 - Pérdida de Entrenamiento: 1.6490, Precisión de Entrenamiento: 0.3857\n",
      "Iteración 6124 - Lote 140/352 - Pérdida de Entrenamiento: 1.6475, Precisión de Entrenamiento: 0.3878\n",
      "Iteración 6159 - Lote 175/352 - Pérdida de Entrenamiento: 1.6444, Precisión de Entrenamiento: 0.3891\n",
      "Iteración 6194 - Lote 210/352 - Pérdida de Entrenamiento: 1.6449, Precisión de Entrenamiento: 0.3899\n",
      "Iteración 6229 - Lote 245/352 - Pérdida de Entrenamiento: 1.6417, Precisión de Entrenamiento: 0.3912\n",
      "Iteración 6264 - Lote 280/352 - Pérdida de Entrenamiento: 1.6405, Precisión de Entrenamiento: 0.3918\n",
      "Iteración 6299 - Lote 315/352 - Pérdida de Entrenamiento: 1.6408, Precisión de Entrenamiento: 0.3919\n",
      "Iteración 6334 - Lote 350/352 - Pérdida de Entrenamiento: 1.6369, Precisión de Entrenamiento: 0.3917\n",
      "Val loss: 1.5565, Val acc: 0.4244\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_18.pth\n",
      "Checkpoint del mejor modelo guardado en la época 18\n",
      "0.1\n",
      "Epoch 19/400\n",
      "Iteración 6371 - Lote 35/352 - Pérdida de Entrenamiento: 1.6246, Precisión de Entrenamiento: 0.4042\n",
      "Iteración 6406 - Lote 70/352 - Pérdida de Entrenamiento: 1.6185, Precisión de Entrenamiento: 0.4017\n",
      "Iteración 6441 - Lote 105/352 - Pérdida de Entrenamiento: 1.6206, Precisión de Entrenamiento: 0.4031\n",
      "Iteración 6476 - Lote 140/352 - Pérdida de Entrenamiento: 1.6206, Precisión de Entrenamiento: 0.4020\n",
      "Iteración 6511 - Lote 175/352 - Pérdida de Entrenamiento: 1.6211, Precisión de Entrenamiento: 0.4016\n",
      "Iteración 6546 - Lote 210/352 - Pérdida de Entrenamiento: 1.6210, Precisión de Entrenamiento: 0.4015\n",
      "Iteración 6581 - Lote 245/352 - Pérdida de Entrenamiento: 1.6199, Precisión de Entrenamiento: 0.4021\n",
      "Iteración 6616 - Lote 280/352 - Pérdida de Entrenamiento: 1.6194, Precisión de Entrenamiento: 0.4026\n",
      "Iteración 6651 - Lote 315/352 - Pérdida de Entrenamiento: 1.6216, Precisión de Entrenamiento: 0.4007\n",
      "Iteración 6686 - Lote 350/352 - Pérdida de Entrenamiento: 1.6201, Precisión de Entrenamiento: 0.4015\n",
      "Val loss: 1.5490, Val acc: 0.4310\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_19.pth\n",
      "Checkpoint del mejor modelo guardado en la época 19\n",
      "0.1\n",
      "Epoch 20/400\n",
      "Iteración 6723 - Lote 35/352 - Pérdida de Entrenamiento: 1.5902, Precisión de Entrenamiento: 0.4132\n",
      "Iteración 6758 - Lote 70/352 - Pérdida de Entrenamiento: 1.6036, Precisión de Entrenamiento: 0.4085\n",
      "Iteración 6793 - Lote 105/352 - Pérdida de Entrenamiento: 1.6061, Precisión de Entrenamiento: 0.4048\n",
      "Iteración 6828 - Lote 140/352 - Pérdida de Entrenamiento: 1.6099, Precisión de Entrenamiento: 0.4037\n",
      "Iteración 6863 - Lote 175/352 - Pérdida de Entrenamiento: 1.6100, Precisión de Entrenamiento: 0.4037\n",
      "Iteración 6898 - Lote 210/352 - Pérdida de Entrenamiento: 1.6101, Precisión de Entrenamiento: 0.4027\n",
      "Iteración 6933 - Lote 245/352 - Pérdida de Entrenamiento: 1.6124, Precisión de Entrenamiento: 0.4028\n",
      "Iteración 6968 - Lote 280/352 - Pérdida de Entrenamiento: 1.6111, Precisión de Entrenamiento: 0.4041\n",
      "Iteración 7003 - Lote 315/352 - Pérdida de Entrenamiento: 1.6083, Precisión de Entrenamiento: 0.4060\n",
      "Iteración 7038 - Lote 350/352 - Pérdida de Entrenamiento: 1.6069, Precisión de Entrenamiento: 0.4070\n",
      "Val loss: 1.5370, Val acc: 0.4424\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_20.pth\n",
      "Checkpoint del mejor modelo guardado en la época 20\n",
      "Gradientes para features.0.0.weight: min=-0.004176514223217964, max=0.003330664709210396, mean=1.0018696229963098e-05, std=0.0005175096448510885\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.010878283530473709, max=0.010623537003993988, mean=4.6975155783002265e-06, std=0.0005882750265300274\n",
      "Gradientes para classifier.1.weight: min=-0.011345560662448406, max=0.010420138016343117, mean=9.313225537987968e-12, std=0.001727696624584496\n",
      "0.1\n",
      "Epoch 21/400\n",
      "Iteración 7075 - Lote 35/352 - Pérdida de Entrenamiento: 1.5797, Precisión de Entrenamiento: 0.4230\n",
      "Iteración 7110 - Lote 70/352 - Pérdida de Entrenamiento: 1.5879, Precisión de Entrenamiento: 0.4208\n",
      "Iteración 7145 - Lote 105/352 - Pérdida de Entrenamiento: 1.5891, Precisión de Entrenamiento: 0.4169\n",
      "Iteración 7180 - Lote 140/352 - Pérdida de Entrenamiento: 1.5871, Precisión de Entrenamiento: 0.4174\n",
      "Iteración 7215 - Lote 175/352 - Pérdida de Entrenamiento: 1.5857, Precisión de Entrenamiento: 0.4195\n",
      "Iteración 7250 - Lote 210/352 - Pérdida de Entrenamiento: 1.5851, Precisión de Entrenamiento: 0.4191\n",
      "Iteración 7285 - Lote 245/352 - Pérdida de Entrenamiento: 1.5892, Precisión de Entrenamiento: 0.4173\n",
      "Iteración 7320 - Lote 280/352 - Pérdida de Entrenamiento: 1.5927, Precisión de Entrenamiento: 0.4152\n",
      "Iteración 7355 - Lote 315/352 - Pérdida de Entrenamiento: 1.5922, Precisión de Entrenamiento: 0.4140\n",
      "Iteración 7390 - Lote 350/352 - Pérdida de Entrenamiento: 1.5935, Precisión de Entrenamiento: 0.4131\n",
      "Val loss: 1.5211, Val acc: 0.4446\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_21.pth\n",
      "Checkpoint del mejor modelo guardado en la época 21\n",
      "0.1\n",
      "Epoch 22/400\n",
      "Iteración 7427 - Lote 35/352 - Pérdida de Entrenamiento: 1.5800, Precisión de Entrenamiento: 0.4179\n",
      "Iteración 7462 - Lote 70/352 - Pérdida de Entrenamiento: 1.5949, Precisión de Entrenamiento: 0.4078\n",
      "Iteración 7497 - Lote 105/352 - Pérdida de Entrenamiento: 1.5969, Precisión de Entrenamiento: 0.4079\n",
      "Iteración 7532 - Lote 140/352 - Pérdida de Entrenamiento: 1.5931, Precisión de Entrenamiento: 0.4093\n",
      "Iteración 7567 - Lote 175/352 - Pérdida de Entrenamiento: 1.5950, Precisión de Entrenamiento: 0.4104\n",
      "Iteración 7602 - Lote 210/352 - Pérdida de Entrenamiento: 1.5912, Precisión de Entrenamiento: 0.4108\n",
      "Iteración 7637 - Lote 245/352 - Pérdida de Entrenamiento: 1.5902, Precisión de Entrenamiento: 0.4123\n",
      "Iteración 7672 - Lote 280/352 - Pérdida de Entrenamiento: 1.5881, Precisión de Entrenamiento: 0.4129\n",
      "Iteración 7707 - Lote 315/352 - Pérdida de Entrenamiento: 1.5869, Precisión de Entrenamiento: 0.4136\n",
      "Iteración 7742 - Lote 350/352 - Pérdida de Entrenamiento: 1.5876, Precisión de Entrenamiento: 0.4136\n",
      "Val loss: 1.5409, Val acc: 0.4266\n",
      "0.1\n",
      "Epoch 23/400\n",
      "Iteración 7779 - Lote 35/352 - Pérdida de Entrenamiento: 1.5896, Precisión de Entrenamiento: 0.4087\n",
      "Iteración 7814 - Lote 70/352 - Pérdida de Entrenamiento: 1.6029, Precisión de Entrenamiento: 0.4039\n",
      "Iteración 7849 - Lote 105/352 - Pérdida de Entrenamiento: 1.5994, Precisión de Entrenamiento: 0.4079\n",
      "Iteración 7884 - Lote 140/352 - Pérdida de Entrenamiento: 1.5977, Precisión de Entrenamiento: 0.4085\n",
      "Iteración 7919 - Lote 175/352 - Pérdida de Entrenamiento: 1.5889, Precisión de Entrenamiento: 0.4117\n",
      "Iteración 7954 - Lote 210/352 - Pérdida de Entrenamiento: 1.5838, Precisión de Entrenamiento: 0.4128\n",
      "Iteración 7989 - Lote 245/352 - Pérdida de Entrenamiento: 1.5847, Precisión de Entrenamiento: 0.4143\n",
      "Iteración 8024 - Lote 280/352 - Pérdida de Entrenamiento: 1.5838, Precisión de Entrenamiento: 0.4139\n",
      "Iteración 8059 - Lote 315/352 - Pérdida de Entrenamiento: 1.5810, Precisión de Entrenamiento: 0.4156\n",
      "Iteración 8094 - Lote 350/352 - Pérdida de Entrenamiento: 1.5804, Precisión de Entrenamiento: 0.4167\n",
      "Val loss: 1.5046, Val acc: 0.4466\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_23.pth\n",
      "Checkpoint del mejor modelo guardado en la época 23\n",
      "0.1\n",
      "Epoch 24/400\n",
      "Iteración 8131 - Lote 35/352 - Pérdida de Entrenamiento: 1.5826, Precisión de Entrenamiento: 0.4170\n",
      "Iteración 8166 - Lote 70/352 - Pérdida de Entrenamiento: 1.5687, Precisión de Entrenamiento: 0.4256\n",
      "Iteración 8201 - Lote 105/352 - Pérdida de Entrenamiento: 1.5576, Precisión de Entrenamiento: 0.4254\n",
      "Iteración 8236 - Lote 140/352 - Pérdida de Entrenamiento: 1.5617, Precisión de Entrenamiento: 0.4243\n",
      "Iteración 8271 - Lote 175/352 - Pérdida de Entrenamiento: 1.5623, Precisión de Entrenamiento: 0.4248\n",
      "Iteración 8306 - Lote 210/352 - Pérdida de Entrenamiento: 1.5610, Precisión de Entrenamiento: 0.4259\n",
      "Iteración 8341 - Lote 245/352 - Pérdida de Entrenamiento: 1.5615, Precisión de Entrenamiento: 0.4242\n",
      "Iteración 8376 - Lote 280/352 - Pérdida de Entrenamiento: 1.5605, Precisión de Entrenamiento: 0.4245\n",
      "Iteración 8411 - Lote 315/352 - Pérdida de Entrenamiento: 1.5587, Precisión de Entrenamiento: 0.4233\n",
      "Iteración 8446 - Lote 350/352 - Pérdida de Entrenamiento: 1.5619, Precisión de Entrenamiento: 0.4224\n",
      "Val loss: 1.5118, Val acc: 0.4476\n",
      "0.1\n",
      "Epoch 25/400\n",
      "Iteración 8483 - Lote 35/352 - Pérdida de Entrenamiento: 1.5428, Precisión de Entrenamiento: 0.4373\n",
      "Iteración 8518 - Lote 70/352 - Pérdida de Entrenamiento: 1.5484, Precisión de Entrenamiento: 0.4324\n",
      "Iteración 8553 - Lote 105/352 - Pérdida de Entrenamiento: 1.5482, Precisión de Entrenamiento: 0.4343\n",
      "Iteración 8588 - Lote 140/352 - Pérdida de Entrenamiento: 1.5428, Precisión de Entrenamiento: 0.4351\n",
      "Iteración 8623 - Lote 175/352 - Pérdida de Entrenamiento: 1.5456, Precisión de Entrenamiento: 0.4331\n",
      "Iteración 8658 - Lote 210/352 - Pérdida de Entrenamiento: 1.5530, Precisión de Entrenamiento: 0.4303\n",
      "Iteración 8693 - Lote 245/352 - Pérdida de Entrenamiento: 1.5526, Precisión de Entrenamiento: 0.4299\n",
      "Iteración 8728 - Lote 280/352 - Pérdida de Entrenamiento: 1.5506, Precisión de Entrenamiento: 0.4312\n",
      "Iteración 8763 - Lote 315/352 - Pérdida de Entrenamiento: 1.5473, Precisión de Entrenamiento: 0.4327\n",
      "Iteración 8798 - Lote 350/352 - Pérdida de Entrenamiento: 1.5465, Precisión de Entrenamiento: 0.4329\n",
      "Val loss: 1.4828, Val acc: 0.4582\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_25.pth\n",
      "Checkpoint del mejor modelo guardado en la época 25\n",
      "Gradientes para features.0.0.weight: min=-0.016838379204273224, max=0.0025737877003848553, mean=-0.0002449981402605772, std=0.0016383994370698929\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.013437196612358093, max=0.007019605487585068, mean=-5.873566806258168e-06, std=0.0005722863133996725\n",
      "Gradientes para classifier.1.weight: min=-0.013597122393548489, max=0.010962584987282753, mean=-9.313225537987968e-12, std=0.0017875940538942814\n",
      "0.1\n",
      "Epoch 26/400\n",
      "Iteración 8835 - Lote 35/352 - Pérdida de Entrenamiento: 1.5471, Precisión de Entrenamiento: 0.4342\n",
      "Iteración 8870 - Lote 70/352 - Pérdida de Entrenamiento: 1.5333, Precisión de Entrenamiento: 0.4377\n",
      "Iteración 8905 - Lote 105/352 - Pérdida de Entrenamiento: 1.5329, Precisión de Entrenamiento: 0.4371\n",
      "Iteración 8940 - Lote 140/352 - Pérdida de Entrenamiento: 1.5323, Precisión de Entrenamiento: 0.4365\n",
      "Iteración 8975 - Lote 175/352 - Pérdida de Entrenamiento: 1.5314, Precisión de Entrenamiento: 0.4374\n",
      "Iteración 9010 - Lote 210/352 - Pérdida de Entrenamiento: 1.5328, Precisión de Entrenamiento: 0.4353\n",
      "Iteración 9045 - Lote 245/352 - Pérdida de Entrenamiento: 1.5314, Precisión de Entrenamiento: 0.4360\n",
      "Iteración 9080 - Lote 280/352 - Pérdida de Entrenamiento: 1.5327, Precisión de Entrenamiento: 0.4351\n",
      "Iteración 9115 - Lote 315/352 - Pérdida de Entrenamiento: 1.5356, Precisión de Entrenamiento: 0.4330\n",
      "Iteración 9150 - Lote 350/352 - Pérdida de Entrenamiento: 1.5335, Precisión de Entrenamiento: 0.4339\n",
      "Val loss: 1.4678, Val acc: 0.4588\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_26.pth\n",
      "Checkpoint del mejor modelo guardado en la época 26\n",
      "0.1\n",
      "Epoch 27/400\n",
      "Iteración 9187 - Lote 35/352 - Pérdida de Entrenamiento: 1.5467, Precisión de Entrenamiento: 0.4261\n",
      "Iteración 9222 - Lote 70/352 - Pérdida de Entrenamiento: 1.5217, Precisión de Entrenamiento: 0.4377\n",
      "Iteración 9257 - Lote 105/352 - Pérdida de Entrenamiento: 1.5320, Precisión de Entrenamiento: 0.4362\n",
      "Iteración 9292 - Lote 140/352 - Pérdida de Entrenamiento: 1.5292, Precisión de Entrenamiento: 0.4382\n",
      "Iteración 9327 - Lote 175/352 - Pérdida de Entrenamiento: 1.5269, Precisión de Entrenamiento: 0.4406\n",
      "Iteración 9362 - Lote 210/352 - Pérdida de Entrenamiento: 1.5257, Precisión de Entrenamiento: 0.4409\n",
      "Iteración 9397 - Lote 245/352 - Pérdida de Entrenamiento: 1.5270, Precisión de Entrenamiento: 0.4406\n",
      "Iteración 9432 - Lote 280/352 - Pérdida de Entrenamiento: 1.5282, Precisión de Entrenamiento: 0.4405\n",
      "Iteración 9467 - Lote 315/352 - Pérdida de Entrenamiento: 1.5265, Precisión de Entrenamiento: 0.4399\n",
      "Iteración 9502 - Lote 350/352 - Pérdida de Entrenamiento: 1.5247, Precisión de Entrenamiento: 0.4406\n",
      "Val loss: 1.4435, Val acc: 0.4768\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_27.pth\n",
      "Checkpoint del mejor modelo guardado en la época 27\n",
      "0.1\n",
      "Epoch 28/400\n",
      "Iteración 9539 - Lote 35/352 - Pérdida de Entrenamiento: 1.5266, Precisión de Entrenamiento: 0.4335\n",
      "Iteración 9574 - Lote 70/352 - Pérdida de Entrenamiento: 1.5193, Precisión de Entrenamiento: 0.4396\n",
      "Iteración 9609 - Lote 105/352 - Pérdida de Entrenamiento: 1.5166, Precisión de Entrenamiento: 0.4411\n",
      "Iteración 9644 - Lote 140/352 - Pérdida de Entrenamiento: 1.5093, Precisión de Entrenamiento: 0.4475\n",
      "Iteración 9679 - Lote 175/352 - Pérdida de Entrenamiento: 1.5040, Precisión de Entrenamiento: 0.4477\n",
      "Iteración 9714 - Lote 210/352 - Pérdida de Entrenamiento: 1.5115, Precisión de Entrenamiento: 0.4442\n",
      "Iteración 9749 - Lote 245/352 - Pérdida de Entrenamiento: 1.5133, Precisión de Entrenamiento: 0.4434\n",
      "Iteración 9784 - Lote 280/352 - Pérdida de Entrenamiento: 1.5124, Precisión de Entrenamiento: 0.4449\n",
      "Iteración 9819 - Lote 315/352 - Pérdida de Entrenamiento: 1.5131, Precisión de Entrenamiento: 0.4442\n",
      "Iteración 9854 - Lote 350/352 - Pérdida de Entrenamiento: 1.5126, Precisión de Entrenamiento: 0.4442\n",
      "Val loss: 1.5207, Val acc: 0.4498\n",
      "0.1\n",
      "Epoch 29/400\n",
      "Iteración 9891 - Lote 35/352 - Pérdida de Entrenamiento: 1.5136, Precisión de Entrenamiento: 0.4471\n",
      "Iteración 9926 - Lote 70/352 - Pérdida de Entrenamiento: 1.5138, Precisión de Entrenamiento: 0.4446\n",
      "Iteración 9961 - Lote 105/352 - Pérdida de Entrenamiento: 1.5064, Precisión de Entrenamiento: 0.4462\n",
      "Iteración 9996 - Lote 140/352 - Pérdida de Entrenamiento: 1.5060, Precisión de Entrenamiento: 0.4437\n",
      "Iteración 10031 - Lote 175/352 - Pérdida de Entrenamiento: 1.5047, Precisión de Entrenamiento: 0.4444\n",
      "Iteración 10066 - Lote 210/352 - Pérdida de Entrenamiento: 1.5029, Precisión de Entrenamiento: 0.4461\n",
      "Iteración 10101 - Lote 245/352 - Pérdida de Entrenamiento: 1.5021, Precisión de Entrenamiento: 0.4470\n",
      "Iteración 10136 - Lote 280/352 - Pérdida de Entrenamiento: 1.5000, Precisión de Entrenamiento: 0.4482\n",
      "Iteración 10171 - Lote 315/352 - Pérdida de Entrenamiento: 1.5008, Precisión de Entrenamiento: 0.4485\n",
      "Iteración 10206 - Lote 350/352 - Pérdida de Entrenamiento: 1.4998, Precisión de Entrenamiento: 0.4490\n",
      "Val loss: 1.4378, Val acc: 0.4730\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_29.pth\n",
      "Checkpoint del mejor modelo guardado en la época 29\n",
      "0.1\n",
      "Epoch 30/400\n",
      "Iteración 10243 - Lote 35/352 - Pérdida de Entrenamiento: 1.4998, Precisión de Entrenamiento: 0.4444\n",
      "Iteración 10278 - Lote 70/352 - Pérdida de Entrenamiento: 1.4929, Precisión de Entrenamiento: 0.4512\n",
      "Iteración 10313 - Lote 105/352 - Pérdida de Entrenamiento: 1.4917, Precisión de Entrenamiento: 0.4511\n",
      "Iteración 10348 - Lote 140/352 - Pérdida de Entrenamiento: 1.4901, Precisión de Entrenamiento: 0.4529\n",
      "Iteración 10383 - Lote 175/352 - Pérdida de Entrenamiento: 1.4927, Precisión de Entrenamiento: 0.4508\n",
      "Iteración 10418 - Lote 210/352 - Pérdida de Entrenamiento: 1.4950, Precisión de Entrenamiento: 0.4526\n",
      "Iteración 10453 - Lote 245/352 - Pérdida de Entrenamiento: 1.4952, Precisión de Entrenamiento: 0.4526\n",
      "Iteración 10488 - Lote 280/352 - Pérdida de Entrenamiento: 1.4936, Precisión de Entrenamiento: 0.4530\n",
      "Iteración 10523 - Lote 315/352 - Pérdida de Entrenamiento: 1.4934, Precisión de Entrenamiento: 0.4523\n",
      "Iteración 10558 - Lote 350/352 - Pérdida de Entrenamiento: 1.4933, Precisión de Entrenamiento: 0.4521\n",
      "Val loss: 1.4083, Val acc: 0.4818\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_30.pth\n",
      "Checkpoint del mejor modelo guardado en la época 30\n",
      "Gradientes para features.0.0.weight: min=-0.012660227715969086, max=0.0033400619868189096, mean=-6.861157453386113e-05, std=0.0010556387715041637\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.013549075461924076, max=0.01449588593095541, mean=9.855942153080832e-06, std=0.0007053124136291444\n",
      "Gradientes para classifier.1.weight: min=-0.01199399121105671, max=0.015630099922418594, mean=1.1641532356165829e-11, std=0.0018222066573798656\n",
      "0.1\n",
      "Epoch 31/400\n",
      "Iteración 10595 - Lote 35/352 - Pérdida de Entrenamiento: 1.5093, Precisión de Entrenamiento: 0.4446\n",
      "Iteración 10630 - Lote 70/352 - Pérdida de Entrenamiento: 1.4791, Precisión de Entrenamiento: 0.4588\n",
      "Iteración 10665 - Lote 105/352 - Pérdida de Entrenamiento: 1.4721, Precisión de Entrenamiento: 0.4580\n",
      "Iteración 10700 - Lote 140/352 - Pérdida de Entrenamiento: 1.4729, Precisión de Entrenamiento: 0.4600\n",
      "Iteración 10735 - Lote 175/352 - Pérdida de Entrenamiento: 1.4733, Precisión de Entrenamiento: 0.4598\n",
      "Iteración 10770 - Lote 210/352 - Pérdida de Entrenamiento: 1.4760, Precisión de Entrenamiento: 0.4597\n",
      "Iteración 10805 - Lote 245/352 - Pérdida de Entrenamiento: 1.4765, Precisión de Entrenamiento: 0.4576\n",
      "Iteración 10840 - Lote 280/352 - Pérdida de Entrenamiento: 1.4759, Precisión de Entrenamiento: 0.4582\n",
      "Iteración 10875 - Lote 315/352 - Pérdida de Entrenamiento: 1.4748, Precisión de Entrenamiento: 0.4597\n",
      "Iteración 10910 - Lote 350/352 - Pérdida de Entrenamiento: 1.4720, Precisión de Entrenamiento: 0.4611\n",
      "Val loss: 1.4033, Val acc: 0.4878\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_31.pth\n",
      "Checkpoint del mejor modelo guardado en la época 31\n",
      "0.1\n",
      "Epoch 32/400\n",
      "Iteración 10947 - Lote 35/352 - Pérdida de Entrenamiento: 1.4622, Precisión de Entrenamiento: 0.4616\n",
      "Iteración 10982 - Lote 70/352 - Pérdida de Entrenamiento: 1.4568, Precisión de Entrenamiento: 0.4615\n",
      "Iteración 11017 - Lote 105/352 - Pérdida de Entrenamiento: 1.4710, Precisión de Entrenamiento: 0.4583\n",
      "Iteración 11052 - Lote 140/352 - Pérdida de Entrenamiento: 1.4789, Precisión de Entrenamiento: 0.4559\n",
      "Iteración 11087 - Lote 175/352 - Pérdida de Entrenamiento: 1.4737, Precisión de Entrenamiento: 0.4578\n",
      "Iteración 11122 - Lote 210/352 - Pérdida de Entrenamiento: 1.4711, Precisión de Entrenamiento: 0.4592\n",
      "Iteración 11157 - Lote 245/352 - Pérdida de Entrenamiento: 1.4711, Precisión de Entrenamiento: 0.4599\n",
      "Iteración 11192 - Lote 280/352 - Pérdida de Entrenamiento: 1.4723, Precisión de Entrenamiento: 0.4589\n",
      "Iteración 11227 - Lote 315/352 - Pérdida de Entrenamiento: 1.4735, Precisión de Entrenamiento: 0.4583\n",
      "Iteración 11262 - Lote 350/352 - Pérdida de Entrenamiento: 1.4715, Precisión de Entrenamiento: 0.4596\n",
      "Val loss: 1.4072, Val acc: 0.4886\n",
      "0.1\n",
      "Epoch 33/400\n",
      "Iteración 11299 - Lote 35/352 - Pérdida de Entrenamiento: 1.4305, Precisión de Entrenamiento: 0.4737\n",
      "Iteración 11334 - Lote 70/352 - Pérdida de Entrenamiento: 1.4396, Precisión de Entrenamiento: 0.4732\n",
      "Iteración 11369 - Lote 105/352 - Pérdida de Entrenamiento: 1.4533, Precisión de Entrenamiento: 0.4650\n",
      "Iteración 11404 - Lote 140/352 - Pérdida de Entrenamiento: 1.4550, Precisión de Entrenamiento: 0.4653\n",
      "Iteración 11439 - Lote 175/352 - Pérdida de Entrenamiento: 1.4545, Precisión de Entrenamiento: 0.4638\n",
      "Iteración 11474 - Lote 210/352 - Pérdida de Entrenamiento: 1.4541, Precisión de Entrenamiento: 0.4645\n",
      "Iteración 11509 - Lote 245/352 - Pérdida de Entrenamiento: 1.4531, Precisión de Entrenamiento: 0.4645\n",
      "Iteración 11544 - Lote 280/352 - Pérdida de Entrenamiento: 1.4581, Precisión de Entrenamiento: 0.4632\n",
      "Iteración 11579 - Lote 315/352 - Pérdida de Entrenamiento: 1.4606, Precisión de Entrenamiento: 0.4627\n",
      "Iteración 11614 - Lote 350/352 - Pérdida de Entrenamiento: 1.4593, Precisión de Entrenamiento: 0.4633\n",
      "Val loss: 1.4020, Val acc: 0.4886\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_33.pth\n",
      "Checkpoint del mejor modelo guardado en la época 33\n",
      "0.1\n",
      "Epoch 34/400\n",
      "Iteración 11651 - Lote 35/352 - Pérdida de Entrenamiento: 1.4560, Precisión de Entrenamiento: 0.4609\n",
      "Iteración 11686 - Lote 70/352 - Pérdida de Entrenamiento: 1.4704, Precisión de Entrenamiento: 0.4581\n",
      "Iteración 11721 - Lote 105/352 - Pérdida de Entrenamiento: 1.4743, Precisión de Entrenamiento: 0.4572\n",
      "Iteración 11756 - Lote 140/352 - Pérdida de Entrenamiento: 1.4739, Precisión de Entrenamiento: 0.4584\n",
      "Iteración 11791 - Lote 175/352 - Pérdida de Entrenamiento: 1.4689, Precisión de Entrenamiento: 0.4610\n",
      "Iteración 11826 - Lote 210/352 - Pérdida de Entrenamiento: 1.4657, Precisión de Entrenamiento: 0.4602\n",
      "Iteración 11861 - Lote 245/352 - Pérdida de Entrenamiento: 1.4644, Precisión de Entrenamiento: 0.4608\n",
      "Iteración 11896 - Lote 280/352 - Pérdida de Entrenamiento: 1.4663, Precisión de Entrenamiento: 0.4621\n",
      "Iteración 11931 - Lote 315/352 - Pérdida de Entrenamiento: 1.4672, Precisión de Entrenamiento: 0.4616\n",
      "Iteración 11966 - Lote 350/352 - Pérdida de Entrenamiento: 1.4627, Precisión de Entrenamiento: 0.4632\n",
      "Val loss: 1.3995, Val acc: 0.4896\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_34.pth\n",
      "Checkpoint del mejor modelo guardado en la época 34\n",
      "0.1\n",
      "Epoch 35/400\n",
      "Iteración 12003 - Lote 35/352 - Pérdida de Entrenamiento: 1.4550, Precisión de Entrenamiento: 0.4547\n",
      "Iteración 12038 - Lote 70/352 - Pérdida de Entrenamiento: 1.4568, Precisión de Entrenamiento: 0.4605\n",
      "Iteración 12073 - Lote 105/352 - Pérdida de Entrenamiento: 1.4555, Precisión de Entrenamiento: 0.4618\n",
      "Iteración 12108 - Lote 140/352 - Pérdida de Entrenamiento: 1.4538, Precisión de Entrenamiento: 0.4618\n",
      "Iteración 12143 - Lote 175/352 - Pérdida de Entrenamiento: 1.4528, Precisión de Entrenamiento: 0.4649\n",
      "Iteración 12178 - Lote 210/352 - Pérdida de Entrenamiento: 1.4532, Precisión de Entrenamiento: 0.4654\n",
      "Iteración 12213 - Lote 245/352 - Pérdida de Entrenamiento: 1.4499, Precisión de Entrenamiento: 0.4660\n",
      "Iteración 12248 - Lote 280/352 - Pérdida de Entrenamiento: 1.4454, Precisión de Entrenamiento: 0.4688\n",
      "Iteración 12283 - Lote 315/352 - Pérdida de Entrenamiento: 1.4437, Precisión de Entrenamiento: 0.4686\n",
      "Iteración 12318 - Lote 350/352 - Pérdida de Entrenamiento: 1.4445, Precisión de Entrenamiento: 0.4689\n",
      "Val loss: 1.3840, Val acc: 0.4930\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_35.pth\n",
      "Checkpoint del mejor modelo guardado en la época 35\n",
      "Gradientes para features.0.0.weight: min=-0.00568780954927206, max=0.005033878609538078, mean=3.684429657369037e-06, std=0.0006900535081513226\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.009267272427678108, max=0.010546709410846233, mean=-3.311840146125178e-06, std=0.0005765295354649425\n",
      "Gradientes para classifier.1.weight: min=-0.010233714245259762, max=0.008273248560726643, mean=4.656612768993984e-12, std=0.0014661502791568637\n",
      "0.1\n",
      "Epoch 36/400\n",
      "Iteración 12355 - Lote 35/352 - Pérdida de Entrenamiento: 1.4619, Precisión de Entrenamiento: 0.4574\n",
      "Iteración 12390 - Lote 70/352 - Pérdida de Entrenamiento: 1.4636, Precisión de Entrenamiento: 0.4644\n",
      "Iteración 12425 - Lote 105/352 - Pérdida de Entrenamiento: 1.4419, Precisión de Entrenamiento: 0.4740\n",
      "Iteración 12460 - Lote 140/352 - Pérdida de Entrenamiento: 1.4330, Precisión de Entrenamiento: 0.4774\n",
      "Iteración 12495 - Lote 175/352 - Pérdida de Entrenamiento: 1.4364, Precisión de Entrenamiento: 0.4765\n",
      "Iteración 12530 - Lote 210/352 - Pérdida de Entrenamiento: 1.4352, Precisión de Entrenamiento: 0.4769\n",
      "Iteración 12565 - Lote 245/352 - Pérdida de Entrenamiento: 1.4327, Precisión de Entrenamiento: 0.4774\n",
      "Iteración 12600 - Lote 280/352 - Pérdida de Entrenamiento: 1.4344, Precisión de Entrenamiento: 0.4770\n",
      "Iteración 12635 - Lote 315/352 - Pérdida de Entrenamiento: 1.4325, Precisión de Entrenamiento: 0.4769\n",
      "Iteración 12670 - Lote 350/352 - Pérdida de Entrenamiento: 1.4386, Precisión de Entrenamiento: 0.4748\n",
      "Val loss: 1.4128, Val acc: 0.4892\n",
      "0.1\n",
      "Epoch 37/400\n",
      "Iteración 12707 - Lote 35/352 - Pérdida de Entrenamiento: 1.4384, Precisión de Entrenamiento: 0.4710\n",
      "Iteración 12742 - Lote 70/352 - Pérdida de Entrenamiento: 1.4313, Precisión de Entrenamiento: 0.4698\n",
      "Iteración 12777 - Lote 105/352 - Pérdida de Entrenamiento: 1.4220, Precisión de Entrenamiento: 0.4757\n",
      "Iteración 12812 - Lote 140/352 - Pérdida de Entrenamiento: 1.4231, Precisión de Entrenamiento: 0.4756\n",
      "Iteración 12847 - Lote 175/352 - Pérdida de Entrenamiento: 1.4255, Precisión de Entrenamiento: 0.4764\n",
      "Iteración 12882 - Lote 210/352 - Pérdida de Entrenamiento: 1.4285, Precisión de Entrenamiento: 0.4738\n",
      "Iteración 12917 - Lote 245/352 - Pérdida de Entrenamiento: 1.4250, Precisión de Entrenamiento: 0.4763\n",
      "Iteración 12952 - Lote 280/352 - Pérdida de Entrenamiento: 1.4240, Precisión de Entrenamiento: 0.4775\n",
      "Iteración 12987 - Lote 315/352 - Pérdida de Entrenamiento: 1.4246, Precisión de Entrenamiento: 0.4776\n",
      "Iteración 13022 - Lote 350/352 - Pérdida de Entrenamiento: 1.4252, Precisión de Entrenamiento: 0.4773\n",
      "Val loss: 1.3403, Val acc: 0.5154\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_37.pth\n",
      "Checkpoint del mejor modelo guardado en la época 37\n",
      "0.1\n",
      "Epoch 38/400\n",
      "Iteración 13059 - Lote 35/352 - Pérdida de Entrenamiento: 1.4014, Precisión de Entrenamiento: 0.4879\n",
      "Iteración 13094 - Lote 70/352 - Pérdida de Entrenamiento: 1.4015, Precisión de Entrenamiento: 0.4860\n",
      "Iteración 13129 - Lote 105/352 - Pérdida de Entrenamiento: 1.4000, Precisión de Entrenamiento: 0.4890\n",
      "Iteración 13164 - Lote 140/352 - Pérdida de Entrenamiento: 1.4000, Precisión de Entrenamiento: 0.4900\n",
      "Iteración 13199 - Lote 175/352 - Pérdida de Entrenamiento: 1.4023, Precisión de Entrenamiento: 0.4900\n",
      "Iteración 13234 - Lote 210/352 - Pérdida de Entrenamiento: 1.4033, Precisión de Entrenamiento: 0.4899\n",
      "Iteración 13269 - Lote 245/352 - Pérdida de Entrenamiento: 1.4031, Precisión de Entrenamiento: 0.4889\n",
      "Iteración 13304 - Lote 280/352 - Pérdida de Entrenamiento: 1.4055, Precisión de Entrenamiento: 0.4871\n",
      "Iteración 13339 - Lote 315/352 - Pérdida de Entrenamiento: 1.4078, Precisión de Entrenamiento: 0.4867\n",
      "Iteración 13374 - Lote 350/352 - Pérdida de Entrenamiento: 1.4103, Precisión de Entrenamiento: 0.4856\n",
      "Val loss: 1.3684, Val acc: 0.4900\n",
      "0.1\n",
      "Epoch 39/400\n",
      "Iteración 13411 - Lote 35/352 - Pérdida de Entrenamiento: 1.4005, Precisión de Entrenamiento: 0.4915\n",
      "Iteración 13446 - Lote 70/352 - Pérdida de Entrenamiento: 1.4042, Precisión de Entrenamiento: 0.4901\n",
      "Iteración 13481 - Lote 105/352 - Pérdida de Entrenamiento: 1.4063, Precisión de Entrenamiento: 0.4854\n",
      "Iteración 13516 - Lote 140/352 - Pérdida de Entrenamiento: 1.3965, Precisión de Entrenamiento: 0.4905\n",
      "Iteración 13551 - Lote 175/352 - Pérdida de Entrenamiento: 1.4099, Precisión de Entrenamiento: 0.4859\n",
      "Iteración 13586 - Lote 210/352 - Pérdida de Entrenamiento: 1.4095, Precisión de Entrenamiento: 0.4856\n",
      "Iteración 13621 - Lote 245/352 - Pérdida de Entrenamiento: 1.4066, Precisión de Entrenamiento: 0.4870\n",
      "Iteración 13656 - Lote 280/352 - Pérdida de Entrenamiento: 1.4055, Precisión de Entrenamiento: 0.4883\n",
      "Iteración 13691 - Lote 315/352 - Pérdida de Entrenamiento: 1.4074, Precisión de Entrenamiento: 0.4869\n",
      "Iteración 13726 - Lote 350/352 - Pérdida de Entrenamiento: 1.4075, Precisión de Entrenamiento: 0.4869\n",
      "Val loss: 1.3461, Val acc: 0.5086\n",
      "0.1\n",
      "Epoch 40/400\n",
      "Iteración 13763 - Lote 35/352 - Pérdida de Entrenamiento: 1.3795, Precisión de Entrenamiento: 0.5007\n",
      "Iteración 13798 - Lote 70/352 - Pérdida de Entrenamiento: 1.3887, Precisión de Entrenamiento: 0.4965\n",
      "Iteración 13833 - Lote 105/352 - Pérdida de Entrenamiento: 1.3824, Precisión de Entrenamiento: 0.4977\n",
      "Iteración 13868 - Lote 140/352 - Pérdida de Entrenamiento: 1.3796, Precisión de Entrenamiento: 0.4961\n",
      "Iteración 13903 - Lote 175/352 - Pérdida de Entrenamiento: 1.3812, Precisión de Entrenamiento: 0.4974\n",
      "Iteración 13938 - Lote 210/352 - Pérdida de Entrenamiento: 1.3819, Precisión de Entrenamiento: 0.4968\n",
      "Iteración 13973 - Lote 245/352 - Pérdida de Entrenamiento: 1.3795, Precisión de Entrenamiento: 0.4968\n",
      "Iteración 14008 - Lote 280/352 - Pérdida de Entrenamiento: 1.3833, Precisión de Entrenamiento: 0.4963\n",
      "Iteración 14043 - Lote 315/352 - Pérdida de Entrenamiento: 1.3865, Precisión de Entrenamiento: 0.4953\n",
      "Iteración 14078 - Lote 350/352 - Pérdida de Entrenamiento: 1.3842, Precisión de Entrenamiento: 0.4958\n",
      "Val loss: 1.3421, Val acc: 0.5054\n",
      "Gradientes para features.0.0.weight: min=-0.004813927225768566, max=0.015550894662737846, mean=0.00019312916265334934, std=0.0018845264567062259\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.011562159284949303, max=0.017643144354224205, mean=-8.014991180971265e-06, std=0.0008030512253753841\n",
      "Gradientes para classifier.1.weight: min=-0.016238514333963394, max=0.01593882590532303, mean=0.0, std=0.0020063628908246756\n",
      "0.1\n",
      "Epoch 41/400\n",
      "Iteración 14115 - Lote 35/352 - Pérdida de Entrenamiento: 1.3863, Precisión de Entrenamiento: 0.4953\n",
      "Iteración 14150 - Lote 70/352 - Pérdida de Entrenamiento: 1.3859, Precisión de Entrenamiento: 0.4963\n",
      "Iteración 14185 - Lote 105/352 - Pérdida de Entrenamiento: 1.3774, Precisión de Entrenamiento: 0.4975\n",
      "Iteración 14220 - Lote 140/352 - Pérdida de Entrenamiento: 1.3784, Precisión de Entrenamiento: 0.4970\n",
      "Iteración 14255 - Lote 175/352 - Pérdida de Entrenamiento: 1.3847, Precisión de Entrenamiento: 0.4942\n",
      "Iteración 14290 - Lote 210/352 - Pérdida de Entrenamiento: 1.3865, Precisión de Entrenamiento: 0.4945\n",
      "Iteración 14325 - Lote 245/352 - Pérdida de Entrenamiento: 1.3868, Precisión de Entrenamiento: 0.4938\n",
      "Iteración 14360 - Lote 280/352 - Pérdida de Entrenamiento: 1.3881, Precisión de Entrenamiento: 0.4947\n",
      "Iteración 14395 - Lote 315/352 - Pérdida de Entrenamiento: 1.3863, Precisión de Entrenamiento: 0.4956\n",
      "Iteración 14430 - Lote 350/352 - Pérdida de Entrenamiento: 1.3849, Precisión de Entrenamiento: 0.4960\n",
      "Val loss: 1.3474, Val acc: 0.4952\n",
      "0.1\n",
      "Epoch 42/400\n",
      "Iteración 14467 - Lote 35/352 - Pérdida de Entrenamiento: 1.3773, Precisión de Entrenamiento: 0.4944\n",
      "Iteración 14502 - Lote 70/352 - Pérdida de Entrenamiento: 1.3746, Precisión de Entrenamiento: 0.4971\n",
      "Iteración 14537 - Lote 105/352 - Pérdida de Entrenamiento: 1.3759, Precisión de Entrenamiento: 0.4986\n",
      "Iteración 14572 - Lote 140/352 - Pérdida de Entrenamiento: 1.3737, Precisión de Entrenamiento: 0.4979\n",
      "Iteración 14607 - Lote 175/352 - Pérdida de Entrenamiento: 1.3735, Precisión de Entrenamiento: 0.4992\n",
      "Iteración 14642 - Lote 210/352 - Pérdida de Entrenamiento: 1.3767, Precisión de Entrenamiento: 0.4970\n",
      "Iteración 14677 - Lote 245/352 - Pérdida de Entrenamiento: 1.3769, Precisión de Entrenamiento: 0.4985\n",
      "Iteración 14712 - Lote 280/352 - Pérdida de Entrenamiento: 1.3804, Precisión de Entrenamiento: 0.4974\n",
      "Iteración 14747 - Lote 315/352 - Pérdida de Entrenamiento: 1.3826, Precisión de Entrenamiento: 0.4972\n",
      "Iteración 14782 - Lote 350/352 - Pérdida de Entrenamiento: 1.3784, Precisión de Entrenamiento: 0.4988\n",
      "Val loss: 1.3135, Val acc: 0.5170\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_42.pth\n",
      "Checkpoint del mejor modelo guardado en la época 42\n",
      "0.1\n",
      "Epoch 43/400\n",
      "Iteración 14819 - Lote 35/352 - Pérdida de Entrenamiento: 1.3587, Precisión de Entrenamiento: 0.5025\n",
      "Iteración 14854 - Lote 70/352 - Pérdida de Entrenamiento: 1.3640, Precisión de Entrenamiento: 0.4988\n",
      "Iteración 14889 - Lote 105/352 - Pérdida de Entrenamiento: 1.3642, Precisión de Entrenamiento: 0.4999\n",
      "Iteración 14924 - Lote 140/352 - Pérdida de Entrenamiento: 1.3612, Precisión de Entrenamiento: 0.5006\n",
      "Iteración 14959 - Lote 175/352 - Pérdida de Entrenamiento: 1.3623, Precisión de Entrenamiento: 0.5007\n",
      "Iteración 14994 - Lote 210/352 - Pérdida de Entrenamiento: 1.3606, Precisión de Entrenamiento: 0.5015\n",
      "Iteración 15029 - Lote 245/352 - Pérdida de Entrenamiento: 1.3610, Precisión de Entrenamiento: 0.5010\n",
      "Iteración 15064 - Lote 280/352 - Pérdida de Entrenamiento: 1.3625, Precisión de Entrenamiento: 0.5005\n",
      "Iteración 15099 - Lote 315/352 - Pérdida de Entrenamiento: 1.3615, Precisión de Entrenamiento: 0.5014\n",
      "Iteración 15134 - Lote 350/352 - Pérdida de Entrenamiento: 1.3628, Precisión de Entrenamiento: 0.5012\n",
      "Val loss: 1.3032, Val acc: 0.5312\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_43.pth\n",
      "Checkpoint del mejor modelo guardado en la época 43\n",
      "0.1\n",
      "Epoch 44/400\n",
      "Iteración 15171 - Lote 35/352 - Pérdida de Entrenamiento: 1.3762, Precisión de Entrenamiento: 0.4933\n",
      "Iteración 15206 - Lote 70/352 - Pérdida de Entrenamiento: 1.3570, Precisión de Entrenamiento: 0.5001\n",
      "Iteración 15241 - Lote 105/352 - Pérdida de Entrenamiento: 1.3477, Precisión de Entrenamiento: 0.5065\n",
      "Iteración 15276 - Lote 140/352 - Pérdida de Entrenamiento: 1.3433, Precisión de Entrenamiento: 0.5073\n",
      "Iteración 15311 - Lote 175/352 - Pérdida de Entrenamiento: 1.3504, Precisión de Entrenamiento: 0.5071\n",
      "Iteración 15346 - Lote 210/352 - Pérdida de Entrenamiento: 1.3488, Precisión de Entrenamiento: 0.5077\n",
      "Iteración 15381 - Lote 245/352 - Pérdida de Entrenamiento: 1.3484, Precisión de Entrenamiento: 0.5079\n",
      "Iteración 15416 - Lote 280/352 - Pérdida de Entrenamiento: 1.3496, Precisión de Entrenamiento: 0.5073\n",
      "Iteración 15451 - Lote 315/352 - Pérdida de Entrenamiento: 1.3504, Precisión de Entrenamiento: 0.5066\n",
      "Iteración 15486 - Lote 350/352 - Pérdida de Entrenamiento: 1.3510, Precisión de Entrenamiento: 0.5069\n",
      "Val loss: 1.2945, Val acc: 0.5370\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_44.pth\n",
      "Checkpoint del mejor modelo guardado en la época 44\n",
      "0.1\n",
      "Epoch 45/400\n",
      "Iteración 15523 - Lote 35/352 - Pérdida de Entrenamiento: 1.3632, Precisión de Entrenamiento: 0.5054\n",
      "Iteración 15558 - Lote 70/352 - Pérdida de Entrenamiento: 1.3459, Precisión de Entrenamiento: 0.5142\n",
      "Iteración 15593 - Lote 105/352 - Pérdida de Entrenamiento: 1.3386, Precisión de Entrenamiento: 0.5179\n",
      "Iteración 15628 - Lote 140/352 - Pérdida de Entrenamiento: 1.3346, Precisión de Entrenamiento: 0.5199\n",
      "Iteración 15663 - Lote 175/352 - Pérdida de Entrenamiento: 1.3337, Precisión de Entrenamiento: 0.5192\n",
      "Iteración 15698 - Lote 210/352 - Pérdida de Entrenamiento: 1.3364, Precisión de Entrenamiento: 0.5176\n",
      "Iteración 15733 - Lote 245/352 - Pérdida de Entrenamiento: 1.3391, Precisión de Entrenamiento: 0.5151\n",
      "Iteración 15768 - Lote 280/352 - Pérdida de Entrenamiento: 1.3378, Precisión de Entrenamiento: 0.5150\n",
      "Iteración 15803 - Lote 315/352 - Pérdida de Entrenamiento: 1.3378, Precisión de Entrenamiento: 0.5158\n",
      "Iteración 15838 - Lote 350/352 - Pérdida de Entrenamiento: 1.3389, Precisión de Entrenamiento: 0.5150\n",
      "Val loss: 1.3050, Val acc: 0.5208\n",
      "Gradientes para features.0.0.weight: min=-0.005478525068610907, max=0.013014080002903938, mean=0.00013477582251653075, std=0.001282682060264051\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.012009786441922188, max=0.012798639945685863, mean=-3.618604296207195e-06, std=0.0007885774248279631\n",
      "Gradientes para classifier.1.weight: min=-0.007321090903133154, max=0.010117325000464916, mean=0.0, std=0.0014619645662605762\n",
      "0.1\n",
      "Epoch 46/400\n",
      "Iteración 15875 - Lote 35/352 - Pérdida de Entrenamiento: 1.3379, Precisión de Entrenamiento: 0.5174\n",
      "Iteración 15910 - Lote 70/352 - Pérdida de Entrenamiento: 1.3207, Precisión de Entrenamiento: 0.5214\n",
      "Iteración 15945 - Lote 105/352 - Pérdida de Entrenamiento: 1.3229, Precisión de Entrenamiento: 0.5206\n",
      "Iteración 15980 - Lote 140/352 - Pérdida de Entrenamiento: 1.3263, Precisión de Entrenamiento: 0.5181\n",
      "Iteración 16015 - Lote 175/352 - Pérdida de Entrenamiento: 1.3275, Precisión de Entrenamiento: 0.5181\n",
      "Iteración 16050 - Lote 210/352 - Pérdida de Entrenamiento: 1.3251, Precisión de Entrenamiento: 0.5199\n",
      "Iteración 16085 - Lote 245/352 - Pérdida de Entrenamiento: 1.3265, Precisión de Entrenamiento: 0.5199\n",
      "Iteración 16120 - Lote 280/352 - Pérdida de Entrenamiento: 1.3302, Precisión de Entrenamiento: 0.5178\n",
      "Iteración 16155 - Lote 315/352 - Pérdida de Entrenamiento: 1.3294, Precisión de Entrenamiento: 0.5184\n",
      "Iteración 16190 - Lote 350/352 - Pérdida de Entrenamiento: 1.3287, Precisión de Entrenamiento: 0.5187\n",
      "Val loss: 1.3073, Val acc: 0.5202\n",
      "0.1\n",
      "Epoch 47/400\n",
      "Iteración 16227 - Lote 35/352 - Pérdida de Entrenamiento: 1.3276, Precisión de Entrenamiento: 0.5143\n",
      "Iteración 16262 - Lote 70/352 - Pérdida de Entrenamiento: 1.3296, Precisión de Entrenamiento: 0.5110\n",
      "Iteración 16297 - Lote 105/352 - Pérdida de Entrenamiento: 1.3238, Precisión de Entrenamiento: 0.5153\n",
      "Iteración 16332 - Lote 140/352 - Pérdida de Entrenamiento: 1.3225, Precisión de Entrenamiento: 0.5163\n",
      "Iteración 16367 - Lote 175/352 - Pérdida de Entrenamiento: 1.3270, Precisión de Entrenamiento: 0.5154\n",
      "Iteración 16402 - Lote 210/352 - Pérdida de Entrenamiento: 1.3299, Precisión de Entrenamiento: 0.5159\n",
      "Iteración 16437 - Lote 245/352 - Pérdida de Entrenamiento: 1.3301, Precisión de Entrenamiento: 0.5161\n",
      "Iteración 16472 - Lote 280/352 - Pérdida de Entrenamiento: 1.3312, Precisión de Entrenamiento: 0.5160\n",
      "Iteración 16507 - Lote 315/352 - Pérdida de Entrenamiento: 1.3283, Precisión de Entrenamiento: 0.5176\n",
      "Iteración 16542 - Lote 350/352 - Pérdida de Entrenamiento: 1.3273, Precisión de Entrenamiento: 0.5185\n",
      "Val loss: 1.2937, Val acc: 0.5302\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_47.pth\n",
      "Checkpoint del mejor modelo guardado en la época 47\n",
      "0.1\n",
      "Epoch 48/400\n",
      "Iteración 16579 - Lote 35/352 - Pérdida de Entrenamiento: 1.3373, Precisión de Entrenamiento: 0.5214\n",
      "Iteración 16614 - Lote 70/352 - Pérdida de Entrenamiento: 1.3179, Precisión de Entrenamiento: 0.5289\n",
      "Iteración 16649 - Lote 105/352 - Pérdida de Entrenamiento: 1.3164, Precisión de Entrenamiento: 0.5275\n",
      "Iteración 16684 - Lote 140/352 - Pérdida de Entrenamiento: 1.3147, Precisión de Entrenamiento: 0.5274\n",
      "Iteración 16719 - Lote 175/352 - Pérdida de Entrenamiento: 1.3188, Precisión de Entrenamiento: 0.5248\n",
      "Iteración 16754 - Lote 210/352 - Pérdida de Entrenamiento: 1.3202, Precisión de Entrenamiento: 0.5223\n",
      "Iteración 16789 - Lote 245/352 - Pérdida de Entrenamiento: 1.3201, Precisión de Entrenamiento: 0.5220\n",
      "Iteración 16824 - Lote 280/352 - Pérdida de Entrenamiento: 1.3178, Precisión de Entrenamiento: 0.5229\n",
      "Iteración 16859 - Lote 315/352 - Pérdida de Entrenamiento: 1.3187, Precisión de Entrenamiento: 0.5217\n",
      "Iteración 16894 - Lote 350/352 - Pérdida de Entrenamiento: 1.3187, Precisión de Entrenamiento: 0.5220\n",
      "Val loss: 1.2593, Val acc: 0.5418\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_48.pth\n",
      "Checkpoint del mejor modelo guardado en la época 48\n",
      "0.1\n",
      "Epoch 49/400\n",
      "Iteración 16931 - Lote 35/352 - Pérdida de Entrenamiento: 1.3436, Precisión de Entrenamiento: 0.5150\n",
      "Iteración 16966 - Lote 70/352 - Pérdida de Entrenamiento: 1.3368, Precisión de Entrenamiento: 0.5146\n",
      "Iteración 17001 - Lote 105/352 - Pérdida de Entrenamiento: 1.3234, Precisión de Entrenamiento: 0.5197\n",
      "Iteración 17036 - Lote 140/352 - Pérdida de Entrenamiento: 1.3178, Precisión de Entrenamiento: 0.5198\n",
      "Iteración 17071 - Lote 175/352 - Pérdida de Entrenamiento: 1.3167, Precisión de Entrenamiento: 0.5214\n",
      "Iteración 17106 - Lote 210/352 - Pérdida de Entrenamiento: 1.3148, Precisión de Entrenamiento: 0.5227\n",
      "Iteración 17141 - Lote 245/352 - Pérdida de Entrenamiento: 1.3159, Precisión de Entrenamiento: 0.5230\n",
      "Iteración 17176 - Lote 280/352 - Pérdida de Entrenamiento: 1.3122, Precisión de Entrenamiento: 0.5237\n",
      "Iteración 17211 - Lote 315/352 - Pérdida de Entrenamiento: 1.3119, Precisión de Entrenamiento: 0.5234\n",
      "Iteración 17246 - Lote 350/352 - Pérdida de Entrenamiento: 1.3111, Precisión de Entrenamiento: 0.5233\n",
      "Val loss: 1.2433, Val acc: 0.5380\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_49.pth\n",
      "Checkpoint del mejor modelo guardado en la época 49\n",
      "0.1\n",
      "Epoch 50/400\n",
      "Iteración 17283 - Lote 35/352 - Pérdida de Entrenamiento: 1.2905, Precisión de Entrenamiento: 0.5230\n",
      "Iteración 17318 - Lote 70/352 - Pérdida de Entrenamiento: 1.3000, Precisión de Entrenamiento: 0.5250\n",
      "Iteración 17353 - Lote 105/352 - Pérdida de Entrenamiento: 1.2950, Precisión de Entrenamiento: 0.5271\n",
      "Iteración 17388 - Lote 140/352 - Pérdida de Entrenamiento: 1.2953, Precisión de Entrenamiento: 0.5272\n",
      "Iteración 17423 - Lote 175/352 - Pérdida de Entrenamiento: 1.2973, Precisión de Entrenamiento: 0.5275\n",
      "Iteración 17458 - Lote 210/352 - Pérdida de Entrenamiento: 1.3000, Precisión de Entrenamiento: 0.5266\n",
      "Iteración 17493 - Lote 245/352 - Pérdida de Entrenamiento: 1.3026, Precisión de Entrenamiento: 0.5252\n",
      "Iteración 17528 - Lote 280/352 - Pérdida de Entrenamiento: 1.3002, Precisión de Entrenamiento: 0.5273\n",
      "Iteración 17563 - Lote 315/352 - Pérdida de Entrenamiento: 1.2992, Precisión de Entrenamiento: 0.5276\n",
      "Iteración 17598 - Lote 350/352 - Pérdida de Entrenamiento: 1.2993, Precisión de Entrenamiento: 0.5281\n",
      "Val loss: 1.2545, Val acc: 0.5404\n",
      "Gradientes para features.0.0.weight: min=-0.012721952050924301, max=0.003503149840980768, mean=-0.0001978318759938702, std=0.001065603457391262\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.013285934925079346, max=0.010602986440062523, mean=-3.1233842037181603e-06, std=0.0007831312250345945\n",
      "Gradientes para classifier.1.weight: min=-0.013384356163442135, max=0.009421422146260738, mean=-4.656612768993984e-12, std=0.001581758726388216\n",
      "0.1\n",
      "Epoch 51/400\n",
      "Iteración 17635 - Lote 35/352 - Pérdida de Entrenamiento: 1.2755, Precisión de Entrenamiento: 0.5350\n",
      "Iteración 17670 - Lote 70/352 - Pérdida de Entrenamiento: 1.2884, Precisión de Entrenamiento: 0.5304\n",
      "Iteración 17705 - Lote 105/352 - Pérdida de Entrenamiento: 1.2853, Precisión de Entrenamiento: 0.5315\n",
      "Iteración 17740 - Lote 140/352 - Pérdida de Entrenamiento: 1.2803, Precisión de Entrenamiento: 0.5344\n",
      "Iteración 17775 - Lote 175/352 - Pérdida de Entrenamiento: 1.2794, Precisión de Entrenamiento: 0.5343\n",
      "Iteración 17810 - Lote 210/352 - Pérdida de Entrenamiento: 1.2825, Precisión de Entrenamiento: 0.5336\n",
      "Iteración 17845 - Lote 245/352 - Pérdida de Entrenamiento: 1.2841, Precisión de Entrenamiento: 0.5351\n",
      "Iteración 17880 - Lote 280/352 - Pérdida de Entrenamiento: 1.2885, Precisión de Entrenamiento: 0.5336\n",
      "Iteración 17915 - Lote 315/352 - Pérdida de Entrenamiento: 1.2908, Precisión de Entrenamiento: 0.5324\n",
      "Iteración 17950 - Lote 350/352 - Pérdida de Entrenamiento: 1.2929, Precisión de Entrenamiento: 0.5327\n",
      "Val loss: 1.2415, Val acc: 0.5478\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_51.pth\n",
      "Checkpoint del mejor modelo guardado en la época 51\n",
      "0.1\n",
      "Epoch 52/400\n",
      "Iteración 17987 - Lote 35/352 - Pérdida de Entrenamiento: 1.2687, Precisión de Entrenamiento: 0.5388\n",
      "Iteración 18022 - Lote 70/352 - Pérdida de Entrenamiento: 1.2786, Precisión de Entrenamiento: 0.5340\n",
      "Iteración 18057 - Lote 105/352 - Pérdida de Entrenamiento: 1.2744, Precisión de Entrenamiento: 0.5383\n",
      "Iteración 18092 - Lote 140/352 - Pérdida de Entrenamiento: 1.2835, Precisión de Entrenamiento: 0.5333\n",
      "Iteración 18127 - Lote 175/352 - Pérdida de Entrenamiento: 1.2948, Precisión de Entrenamiento: 0.5304\n",
      "Iteración 18162 - Lote 210/352 - Pérdida de Entrenamiento: 1.3021, Precisión de Entrenamiento: 0.5288\n",
      "Iteración 18197 - Lote 245/352 - Pérdida de Entrenamiento: 1.3032, Precisión de Entrenamiento: 0.5287\n",
      "Iteración 18232 - Lote 280/352 - Pérdida de Entrenamiento: 1.3032, Precisión de Entrenamiento: 0.5286\n",
      "Iteración 18267 - Lote 315/352 - Pérdida de Entrenamiento: 1.3019, Precisión de Entrenamiento: 0.5289\n",
      "Iteración 18302 - Lote 350/352 - Pérdida de Entrenamiento: 1.3038, Precisión de Entrenamiento: 0.5286\n",
      "Val loss: 1.2342, Val acc: 0.5466\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_52.pth\n",
      "Checkpoint del mejor modelo guardado en la época 52\n",
      "0.1\n",
      "Epoch 53/400\n",
      "Iteración 18339 - Lote 35/352 - Pérdida de Entrenamiento: 1.2840, Precisión de Entrenamiento: 0.5317\n",
      "Iteración 18374 - Lote 70/352 - Pérdida de Entrenamiento: 1.2878, Precisión de Entrenamiento: 0.5330\n",
      "Iteración 18409 - Lote 105/352 - Pérdida de Entrenamiento: 1.2827, Precisión de Entrenamiento: 0.5344\n",
      "Iteración 18444 - Lote 140/352 - Pérdida de Entrenamiento: 1.2769, Precisión de Entrenamiento: 0.5358\n",
      "Iteración 18479 - Lote 175/352 - Pérdida de Entrenamiento: 1.2820, Precisión de Entrenamiento: 0.5335\n",
      "Iteración 18514 - Lote 210/352 - Pérdida de Entrenamiento: 1.2885, Precisión de Entrenamiento: 0.5334\n",
      "Iteración 18549 - Lote 245/352 - Pérdida de Entrenamiento: 1.2875, Precisión de Entrenamiento: 0.5335\n",
      "Iteración 18584 - Lote 280/352 - Pérdida de Entrenamiento: 1.2899, Precisión de Entrenamiento: 0.5330\n",
      "Iteración 18619 - Lote 315/352 - Pérdida de Entrenamiento: 1.2884, Precisión de Entrenamiento: 0.5341\n",
      "Iteración 18654 - Lote 350/352 - Pérdida de Entrenamiento: 1.2901, Precisión de Entrenamiento: 0.5339\n",
      "Val loss: 1.2491, Val acc: 0.5540\n",
      "0.1\n",
      "Epoch 54/400\n",
      "Iteración 18691 - Lote 35/352 - Pérdida de Entrenamiento: 1.2478, Precisión de Entrenamiento: 0.5520\n",
      "Iteración 18726 - Lote 70/352 - Pérdida de Entrenamiento: 1.2630, Precisión de Entrenamiento: 0.5426\n",
      "Iteración 18761 - Lote 105/352 - Pérdida de Entrenamiento: 1.2640, Precisión de Entrenamiento: 0.5423\n",
      "Iteración 18796 - Lote 140/352 - Pérdida de Entrenamiento: 1.2614, Precisión de Entrenamiento: 0.5445\n",
      "Iteración 18831 - Lote 175/352 - Pérdida de Entrenamiento: 1.2676, Precisión de Entrenamiento: 0.5418\n",
      "Iteración 18866 - Lote 210/352 - Pérdida de Entrenamiento: 1.2691, Precisión de Entrenamiento: 0.5414\n",
      "Iteración 18901 - Lote 245/352 - Pérdida de Entrenamiento: 1.2681, Precisión de Entrenamiento: 0.5414\n",
      "Iteración 18936 - Lote 280/352 - Pérdida de Entrenamiento: 1.2696, Precisión de Entrenamiento: 0.5404\n",
      "Iteración 18971 - Lote 315/352 - Pérdida de Entrenamiento: 1.2715, Precisión de Entrenamiento: 0.5397\n",
      "Iteración 19006 - Lote 350/352 - Pérdida de Entrenamiento: 1.2733, Precisión de Entrenamiento: 0.5397\n",
      "Val loss: 1.2267, Val acc: 0.5564\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_54.pth\n",
      "Checkpoint del mejor modelo guardado en la época 54\n",
      "0.1\n",
      "Epoch 55/400\n",
      "Iteración 19043 - Lote 35/352 - Pérdida de Entrenamiento: 1.2512, Precisión de Entrenamiento: 0.5391\n",
      "Iteración 19078 - Lote 70/352 - Pérdida de Entrenamiento: 1.2524, Precisión de Entrenamiento: 0.5419\n",
      "Iteración 19113 - Lote 105/352 - Pérdida de Entrenamiento: 1.2595, Precisión de Entrenamiento: 0.5397\n",
      "Iteración 19148 - Lote 140/352 - Pérdida de Entrenamiento: 1.2621, Precisión de Entrenamiento: 0.5391\n",
      "Iteración 19183 - Lote 175/352 - Pérdida de Entrenamiento: 1.2604, Precisión de Entrenamiento: 0.5417\n",
      "Iteración 19218 - Lote 210/352 - Pérdida de Entrenamiento: 1.2599, Precisión de Entrenamiento: 0.5436\n",
      "Iteración 19253 - Lote 245/352 - Pérdida de Entrenamiento: 1.2600, Precisión de Entrenamiento: 0.5431\n",
      "Iteración 19288 - Lote 280/352 - Pérdida de Entrenamiento: 1.2611, Precisión de Entrenamiento: 0.5424\n",
      "Iteración 19323 - Lote 315/352 - Pérdida de Entrenamiento: 1.2618, Precisión de Entrenamiento: 0.5421\n",
      "Iteración 19358 - Lote 350/352 - Pérdida de Entrenamiento: 1.2622, Precisión de Entrenamiento: 0.5426\n",
      "Val loss: 1.2328, Val acc: 0.5520\n",
      "Gradientes para features.0.0.weight: min=-0.0095913615077734, max=0.0829138308763504, mean=0.0018876987742260098, std=0.011053483001887798\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.054789092391729355, max=0.08274763077497482, mean=6.067085996619426e-05, std=0.003146838629618287\n",
      "Gradientes para classifier.1.weight: min=-0.010710109025239944, max=0.017898976802825928, mean=-1.8626451075975936e-11, std=0.002258318243548274\n",
      "0.1\n",
      "Epoch 56/400\n",
      "Iteración 19395 - Lote 35/352 - Pérdida de Entrenamiento: 1.2823, Precisión de Entrenamiento: 0.5462\n",
      "Iteración 19430 - Lote 70/352 - Pérdida de Entrenamiento: 1.2794, Precisión de Entrenamiento: 0.5448\n",
      "Iteración 19465 - Lote 105/352 - Pérdida de Entrenamiento: 1.2719, Precisión de Entrenamiento: 0.5463\n",
      "Iteración 19500 - Lote 140/352 - Pérdida de Entrenamiento: 1.2664, Precisión de Entrenamiento: 0.5470\n",
      "Iteración 19535 - Lote 175/352 - Pérdida de Entrenamiento: 1.2591, Precisión de Entrenamiento: 0.5504\n",
      "Iteración 19570 - Lote 210/352 - Pérdida de Entrenamiento: 1.2581, Precisión de Entrenamiento: 0.5500\n",
      "Iteración 19605 - Lote 245/352 - Pérdida de Entrenamiento: 1.2539, Precisión de Entrenamiento: 0.5501\n",
      "Iteración 19640 - Lote 280/352 - Pérdida de Entrenamiento: 1.2509, Precisión de Entrenamiento: 0.5507\n",
      "Iteración 19675 - Lote 315/352 - Pérdida de Entrenamiento: 1.2525, Precisión de Entrenamiento: 0.5491\n",
      "Iteración 19710 - Lote 350/352 - Pérdida de Entrenamiento: 1.2519, Precisión de Entrenamiento: 0.5491\n",
      "Val loss: 1.2163, Val acc: 0.5624\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_56.pth\n",
      "Checkpoint del mejor modelo guardado en la época 56\n",
      "0.1\n",
      "Epoch 57/400\n",
      "Iteración 19747 - Lote 35/352 - Pérdida de Entrenamiento: 1.2516, Precisión de Entrenamiento: 0.5460\n",
      "Iteración 19782 - Lote 70/352 - Pérdida de Entrenamiento: 1.2557, Precisión de Entrenamiento: 0.5459\n",
      "Iteración 19817 - Lote 105/352 - Pérdida de Entrenamiento: 1.2513, Precisión de Entrenamiento: 0.5466\n",
      "Iteración 19852 - Lote 140/352 - Pérdida de Entrenamiento: 1.2520, Precisión de Entrenamiento: 0.5456\n",
      "Iteración 19887 - Lote 175/352 - Pérdida de Entrenamiento: 1.2490, Precisión de Entrenamiento: 0.5480\n",
      "Iteración 19922 - Lote 210/352 - Pérdida de Entrenamiento: 1.2490, Precisión de Entrenamiento: 0.5484\n",
      "Iteración 19957 - Lote 245/352 - Pérdida de Entrenamiento: 1.2503, Precisión de Entrenamiento: 0.5476\n",
      "Iteración 19992 - Lote 280/352 - Pérdida de Entrenamiento: 1.2477, Precisión de Entrenamiento: 0.5494\n",
      "Iteración 20027 - Lote 315/352 - Pérdida de Entrenamiento: 1.2448, Precisión de Entrenamiento: 0.5506\n",
      "Iteración 20062 - Lote 350/352 - Pérdida de Entrenamiento: 1.2470, Precisión de Entrenamiento: 0.5492\n",
      "Val loss: 1.1829, Val acc: 0.5772\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_57.pth\n",
      "Checkpoint del mejor modelo guardado en la época 57\n",
      "0.1\n",
      "Epoch 58/400\n",
      "Iteración 20099 - Lote 35/352 - Pérdida de Entrenamiento: 1.2477, Precisión de Entrenamiento: 0.5504\n",
      "Iteración 20134 - Lote 70/352 - Pérdida de Entrenamiento: 1.2182, Precisión de Entrenamiento: 0.5585\n",
      "Iteración 20169 - Lote 105/352 - Pérdida de Entrenamiento: 1.2295, Precisión de Entrenamiento: 0.5542\n",
      "Iteración 20204 - Lote 140/352 - Pérdida de Entrenamiento: 1.2294, Precisión de Entrenamiento: 0.5550\n",
      "Iteración 20239 - Lote 175/352 - Pérdida de Entrenamiento: 1.2302, Precisión de Entrenamiento: 0.5569\n",
      "Iteración 20274 - Lote 210/352 - Pérdida de Entrenamiento: 1.2323, Precisión de Entrenamiento: 0.5567\n",
      "Iteración 20309 - Lote 245/352 - Pérdida de Entrenamiento: 1.2332, Precisión de Entrenamiento: 0.5571\n",
      "Iteración 20344 - Lote 280/352 - Pérdida de Entrenamiento: 1.2328, Precisión de Entrenamiento: 0.5563\n",
      "Iteración 20379 - Lote 315/352 - Pérdida de Entrenamiento: 1.2330, Precisión de Entrenamiento: 0.5561\n",
      "Iteración 20414 - Lote 350/352 - Pérdida de Entrenamiento: 1.2311, Precisión de Entrenamiento: 0.5559\n",
      "Val loss: 1.2609, Val acc: 0.5328\n",
      "0.1\n",
      "Epoch 59/400\n",
      "Iteración 20451 - Lote 35/352 - Pérdida de Entrenamiento: 1.2553, Precisión de Entrenamiento: 0.5509\n",
      "Iteración 20486 - Lote 70/352 - Pérdida de Entrenamiento: 1.2408, Precisión de Entrenamiento: 0.5525\n",
      "Iteración 20521 - Lote 105/352 - Pérdida de Entrenamiento: 1.2319, Precisión de Entrenamiento: 0.5546\n",
      "Iteración 20556 - Lote 140/352 - Pérdida de Entrenamiento: 1.2399, Precisión de Entrenamiento: 0.5505\n",
      "Iteración 20591 - Lote 175/352 - Pérdida de Entrenamiento: 1.2343, Precisión de Entrenamiento: 0.5524\n",
      "Iteración 20626 - Lote 210/352 - Pérdida de Entrenamiento: 1.2320, Precisión de Entrenamiento: 0.5554\n",
      "Iteración 20661 - Lote 245/352 - Pérdida de Entrenamiento: 1.2315, Precisión de Entrenamiento: 0.5550\n",
      "Iteración 20696 - Lote 280/352 - Pérdida de Entrenamiento: 1.2309, Precisión de Entrenamiento: 0.5552\n",
      "Iteración 20731 - Lote 315/352 - Pérdida de Entrenamiento: 1.2309, Precisión de Entrenamiento: 0.5553\n",
      "Iteración 20766 - Lote 350/352 - Pérdida de Entrenamiento: 1.2316, Precisión de Entrenamiento: 0.5547\n",
      "Val loss: 1.1987, Val acc: 0.5628\n",
      "0.1\n",
      "Epoch 60/400\n",
      "Iteración 20803 - Lote 35/352 - Pérdida de Entrenamiento: 1.2133, Precisión de Entrenamiento: 0.5596\n",
      "Iteración 20838 - Lote 70/352 - Pérdida de Entrenamiento: 1.1985, Precisión de Entrenamiento: 0.5670\n",
      "Iteración 20873 - Lote 105/352 - Pérdida de Entrenamiento: 1.1986, Precisión de Entrenamiento: 0.5651\n",
      "Iteración 20908 - Lote 140/352 - Pérdida de Entrenamiento: 1.2047, Precisión de Entrenamiento: 0.5632\n",
      "Iteración 20943 - Lote 175/352 - Pérdida de Entrenamiento: 1.2116, Precisión de Entrenamiento: 0.5618\n",
      "Iteración 20978 - Lote 210/352 - Pérdida de Entrenamiento: 1.2136, Precisión de Entrenamiento: 0.5612\n",
      "Iteración 21013 - Lote 245/352 - Pérdida de Entrenamiento: 1.2131, Precisión de Entrenamiento: 0.5613\n",
      "Iteración 21048 - Lote 280/352 - Pérdida de Entrenamiento: 1.2115, Precisión de Entrenamiento: 0.5621\n",
      "Iteración 21083 - Lote 315/352 - Pérdida de Entrenamiento: 1.2103, Precisión de Entrenamiento: 0.5631\n",
      "Iteración 21118 - Lote 350/352 - Pérdida de Entrenamiento: 1.2103, Precisión de Entrenamiento: 0.5635\n",
      "Val loss: 1.1584, Val acc: 0.5792\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_60.pth\n",
      "Checkpoint del mejor modelo guardado en la época 60\n",
      "Gradientes para features.0.0.weight: min=-0.0037323972210288048, max=0.013247795403003693, mean=0.0002812781895045191, std=0.0017455271445214748\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.010931924916803837, max=0.010377035476267338, mean=-5.994697403366445e-06, std=0.0008104691514745355\n",
      "Gradientes para classifier.1.weight: min=-0.009719527326524258, max=0.0077915797010064125, mean=-3.492459576745488e-12, std=0.0016241789562627673\n",
      "0.1\n",
      "Epoch 61/400\n",
      "Iteración 21155 - Lote 35/352 - Pérdida de Entrenamiento: 1.2118, Precisión de Entrenamiento: 0.5563\n",
      "Iteración 21190 - Lote 70/352 - Pérdida de Entrenamiento: 1.2086, Precisión de Entrenamiento: 0.5654\n",
      "Iteración 21225 - Lote 105/352 - Pérdida de Entrenamiento: 1.2077, Precisión de Entrenamiento: 0.5657\n",
      "Iteración 21260 - Lote 140/352 - Pérdida de Entrenamiento: 1.1970, Precisión de Entrenamiento: 0.5690\n",
      "Iteración 21295 - Lote 175/352 - Pérdida de Entrenamiento: 1.1970, Precisión de Entrenamiento: 0.5709\n",
      "Iteración 21330 - Lote 210/352 - Pérdida de Entrenamiento: 1.1962, Precisión de Entrenamiento: 0.5709\n",
      "Iteración 21365 - Lote 245/352 - Pérdida de Entrenamiento: 1.2003, Precisión de Entrenamiento: 0.5696\n",
      "Iteración 21400 - Lote 280/352 - Pérdida de Entrenamiento: 1.1996, Precisión de Entrenamiento: 0.5705\n",
      "Iteración 21435 - Lote 315/352 - Pérdida de Entrenamiento: 1.2007, Precisión de Entrenamiento: 0.5693\n",
      "Iteración 21470 - Lote 350/352 - Pérdida de Entrenamiento: 1.2033, Precisión de Entrenamiento: 0.5683\n",
      "Val loss: 1.1717, Val acc: 0.5820\n",
      "0.1\n",
      "Epoch 62/400\n",
      "Iteración 21507 - Lote 35/352 - Pérdida de Entrenamiento: 1.1822, Precisión de Entrenamiento: 0.5714\n",
      "Iteración 21542 - Lote 70/352 - Pérdida de Entrenamiento: 1.1781, Precisión de Entrenamiento: 0.5733\n",
      "Iteración 21577 - Lote 105/352 - Pérdida de Entrenamiento: 1.1962, Precisión de Entrenamiento: 0.5696\n",
      "Iteración 21612 - Lote 140/352 - Pérdida de Entrenamiento: 1.2042, Precisión de Entrenamiento: 0.5642\n",
      "Iteración 21647 - Lote 175/352 - Pérdida de Entrenamiento: 1.2063, Precisión de Entrenamiento: 0.5633\n",
      "Iteración 21682 - Lote 210/352 - Pérdida de Entrenamiento: 1.2028, Precisión de Entrenamiento: 0.5641\n",
      "Iteración 21717 - Lote 245/352 - Pérdida de Entrenamiento: 1.2008, Precisión de Entrenamiento: 0.5645\n",
      "Iteración 21752 - Lote 280/352 - Pérdida de Entrenamiento: 1.2403, Precisión de Entrenamiento: 0.5513\n",
      "Iteración 21787 - Lote 315/352 - Pérdida de Entrenamiento: 1.2588, Precisión de Entrenamiento: 0.5448\n",
      "Iteración 21822 - Lote 350/352 - Pérdida de Entrenamiento: 1.2689, Precisión de Entrenamiento: 0.5404\n",
      "Val loss: 1.2787, Val acc: 0.5366\n",
      "0.1\n",
      "Epoch 63/400\n",
      "Iteración 21859 - Lote 35/352 - Pérdida de Entrenamiento: 1.3177, Precisión de Entrenamiento: 0.5219\n",
      "Iteración 21894 - Lote 70/352 - Pérdida de Entrenamiento: 1.2834, Precisión de Entrenamiento: 0.5316\n",
      "Iteración 21929 - Lote 105/352 - Pérdida de Entrenamiento: 1.2763, Precisión de Entrenamiento: 0.5349\n",
      "Iteración 21964 - Lote 140/352 - Pérdida de Entrenamiento: 1.2727, Precisión de Entrenamiento: 0.5379\n",
      "Iteración 21999 - Lote 175/352 - Pérdida de Entrenamiento: 1.2695, Precisión de Entrenamiento: 0.5409\n",
      "Iteración 22034 - Lote 210/352 - Pérdida de Entrenamiento: 1.2677, Precisión de Entrenamiento: 0.5419\n",
      "Iteración 22069 - Lote 245/352 - Pérdida de Entrenamiento: 1.2599, Precisión de Entrenamiento: 0.5452\n",
      "Iteración 22104 - Lote 280/352 - Pérdida de Entrenamiento: 1.2566, Precisión de Entrenamiento: 0.5461\n",
      "Iteración 22139 - Lote 315/352 - Pérdida de Entrenamiento: 1.2522, Precisión de Entrenamiento: 0.5479\n",
      "Iteración 22174 - Lote 350/352 - Pérdida de Entrenamiento: 1.2489, Precisión de Entrenamiento: 0.5491\n",
      "Val loss: 1.1831, Val acc: 0.5700\n",
      "0.1\n",
      "Epoch 64/400\n",
      "Iteración 22211 - Lote 35/352 - Pérdida de Entrenamiento: 1.2037, Precisión de Entrenamiento: 0.5607\n",
      "Iteración 22246 - Lote 70/352 - Pérdida de Entrenamiento: 1.2021, Precisión de Entrenamiento: 0.5623\n",
      "Iteración 22281 - Lote 105/352 - Pérdida de Entrenamiento: 1.2100, Precisión de Entrenamiento: 0.5618\n",
      "Iteración 22316 - Lote 140/352 - Pérdida de Entrenamiento: 1.2081, Precisión de Entrenamiento: 0.5632\n",
      "Iteración 22351 - Lote 175/352 - Pérdida de Entrenamiento: 1.2148, Precisión de Entrenamiento: 0.5615\n",
      "Iteración 22386 - Lote 210/352 - Pérdida de Entrenamiento: 1.2154, Precisión de Entrenamiento: 0.5609\n",
      "Iteración 22421 - Lote 245/352 - Pérdida de Entrenamiento: 1.2158, Precisión de Entrenamiento: 0.5606\n",
      "Iteración 22456 - Lote 280/352 - Pérdida de Entrenamiento: 1.2186, Precisión de Entrenamiento: 0.5603\n",
      "Iteración 22491 - Lote 315/352 - Pérdida de Entrenamiento: 1.2171, Precisión de Entrenamiento: 0.5609\n",
      "Iteración 22526 - Lote 350/352 - Pérdida de Entrenamiento: 1.2139, Precisión de Entrenamiento: 0.5626\n",
      "Val loss: 1.1582, Val acc: 0.5752\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_64.pth\n",
      "Checkpoint del mejor modelo guardado en la época 64\n",
      "0.1\n",
      "Epoch 65/400\n",
      "Iteración 22563 - Lote 35/352 - Pérdida de Entrenamiento: 1.2035, Precisión de Entrenamiento: 0.5652\n",
      "Iteración 22598 - Lote 70/352 - Pérdida de Entrenamiento: 1.2002, Precisión de Entrenamiento: 0.5676\n",
      "Iteración 22633 - Lote 105/352 - Pérdida de Entrenamiento: 1.2063, Precisión de Entrenamiento: 0.5635\n",
      "Iteración 22668 - Lote 140/352 - Pérdida de Entrenamiento: 1.2046, Precisión de Entrenamiento: 0.5666\n",
      "Iteración 22703 - Lote 175/352 - Pérdida de Entrenamiento: 1.1978, Precisión de Entrenamiento: 0.5694\n",
      "Iteración 22738 - Lote 210/352 - Pérdida de Entrenamiento: 1.1959, Precisión de Entrenamiento: 0.5702\n",
      "Iteración 22773 - Lote 245/352 - Pérdida de Entrenamiento: 1.1918, Precisión de Entrenamiento: 0.5719\n",
      "Iteración 22808 - Lote 280/352 - Pérdida de Entrenamiento: 1.2029, Precisión de Entrenamiento: 0.5679\n",
      "Iteración 22843 - Lote 315/352 - Pérdida de Entrenamiento: 1.2054, Precisión de Entrenamiento: 0.5663\n",
      "Iteración 22878 - Lote 350/352 - Pérdida de Entrenamiento: 1.2053, Precisión de Entrenamiento: 0.5671\n",
      "Val loss: 1.1437, Val acc: 0.5846\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_65.pth\n",
      "Checkpoint del mejor modelo guardado en la época 65\n",
      "Gradientes para features.0.0.weight: min=-0.006923938635736704, max=0.02236512489616871, mean=0.0003467868664301932, std=0.0029024966061115265\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.010158121585845947, max=0.009978460147976875, mean=-6.070543804526096e-06, std=0.0008282106136903167\n",
      "Gradientes para classifier.1.weight: min=-0.011872641742229462, max=0.009268446825444698, mean=-4.656612768993984e-12, std=0.0014634077670052648\n",
      "0.1\n",
      "Epoch 66/400\n",
      "Iteración 22915 - Lote 35/352 - Pérdida de Entrenamiento: 1.1713, Precisión de Entrenamiento: 0.5797\n",
      "Iteración 22950 - Lote 70/352 - Pérdida de Entrenamiento: 1.1764, Precisión de Entrenamiento: 0.5778\n",
      "Iteración 22985 - Lote 105/352 - Pérdida de Entrenamiento: 1.1852, Precisión de Entrenamiento: 0.5750\n",
      "Iteración 23020 - Lote 140/352 - Pérdida de Entrenamiento: 1.1751, Precisión de Entrenamiento: 0.5765\n",
      "Iteración 23055 - Lote 175/352 - Pérdida de Entrenamiento: 1.1854, Precisión de Entrenamiento: 0.5723\n",
      "Iteración 23090 - Lote 210/352 - Pérdida de Entrenamiento: 1.1974, Precisión de Entrenamiento: 0.5689\n",
      "Iteración 23125 - Lote 245/352 - Pérdida de Entrenamiento: 1.1972, Precisión de Entrenamiento: 0.5702\n",
      "Iteración 23160 - Lote 280/352 - Pérdida de Entrenamiento: 1.1996, Precisión de Entrenamiento: 0.5689\n",
      "Iteración 23195 - Lote 315/352 - Pérdida de Entrenamiento: 1.1981, Precisión de Entrenamiento: 0.5699\n",
      "Iteración 23230 - Lote 350/352 - Pérdida de Entrenamiento: 1.1971, Precisión de Entrenamiento: 0.5707\n",
      "Val loss: 1.1397, Val acc: 0.5886\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_66.pth\n",
      "Checkpoint del mejor modelo guardado en la época 66\n",
      "0.1\n",
      "Epoch 67/400\n",
      "Iteración 23267 - Lote 35/352 - Pérdida de Entrenamiento: 1.1881, Precisión de Entrenamiento: 0.5734\n",
      "Iteración 23302 - Lote 70/352 - Pérdida de Entrenamiento: 1.1829, Precisión de Entrenamiento: 0.5780\n",
      "Iteración 23337 - Lote 105/352 - Pérdida de Entrenamiento: 1.1848, Precisión de Entrenamiento: 0.5754\n",
      "Iteración 23372 - Lote 140/352 - Pérdida de Entrenamiento: 1.1868, Precisión de Entrenamiento: 0.5739\n",
      "Iteración 23407 - Lote 175/352 - Pérdida de Entrenamiento: 1.1903, Precisión de Entrenamiento: 0.5725\n",
      "Iteración 23442 - Lote 210/352 - Pérdida de Entrenamiento: 1.1837, Precisión de Entrenamiento: 0.5765\n",
      "Iteración 23477 - Lote 245/352 - Pérdida de Entrenamiento: 1.1825, Precisión de Entrenamiento: 0.5768\n",
      "Iteración 23512 - Lote 280/352 - Pérdida de Entrenamiento: 1.1793, Precisión de Entrenamiento: 0.5785\n",
      "Iteración 23547 - Lote 315/352 - Pérdida de Entrenamiento: 1.1812, Precisión de Entrenamiento: 0.5776\n",
      "Iteración 23582 - Lote 350/352 - Pérdida de Entrenamiento: 1.1811, Precisión de Entrenamiento: 0.5777\n",
      "Val loss: 1.1430, Val acc: 0.5886\n",
      "0.1\n",
      "Epoch 68/400\n",
      "Iteración 23619 - Lote 35/352 - Pérdida de Entrenamiento: 1.1653, Precisión de Entrenamiento: 0.5741\n",
      "Iteración 23654 - Lote 70/352 - Pérdida de Entrenamiento: 1.1576, Precisión de Entrenamiento: 0.5826\n",
      "Iteración 23689 - Lote 105/352 - Pérdida de Entrenamiento: 1.1666, Precisión de Entrenamiento: 0.5787\n",
      "Iteración 23724 - Lote 140/352 - Pérdida de Entrenamiento: 1.1711, Precisión de Entrenamiento: 0.5781\n",
      "Iteración 23759 - Lote 175/352 - Pérdida de Entrenamiento: 1.1707, Precisión de Entrenamiento: 0.5796\n",
      "Iteración 23794 - Lote 210/352 - Pérdida de Entrenamiento: 1.1716, Precisión de Entrenamiento: 0.5789\n",
      "Iteración 23829 - Lote 245/352 - Pérdida de Entrenamiento: 1.1752, Precisión de Entrenamiento: 0.5780\n",
      "Iteración 23864 - Lote 280/352 - Pérdida de Entrenamiento: 1.1773, Precisión de Entrenamiento: 0.5768\n",
      "Iteración 23899 - Lote 315/352 - Pérdida de Entrenamiento: 1.1757, Precisión de Entrenamiento: 0.5781\n",
      "Iteración 23934 - Lote 350/352 - Pérdida de Entrenamiento: 1.1816, Precisión de Entrenamiento: 0.5762\n",
      "Val loss: 1.2058, Val acc: 0.5634\n",
      "0.1\n",
      "Epoch 69/400\n",
      "Iteración 23971 - Lote 35/352 - Pérdida de Entrenamiento: 1.2123, Precisión de Entrenamiento: 0.5627\n",
      "Iteración 24006 - Lote 70/352 - Pérdida de Entrenamiento: 1.2128, Precisión de Entrenamiento: 0.5608\n",
      "Iteración 24041 - Lote 105/352 - Pérdida de Entrenamiento: 1.1898, Precisión de Entrenamiento: 0.5699\n",
      "Iteración 24076 - Lote 140/352 - Pérdida de Entrenamiento: 1.1829, Precisión de Entrenamiento: 0.5727\n",
      "Iteración 24111 - Lote 175/352 - Pérdida de Entrenamiento: 1.1785, Precisión de Entrenamiento: 0.5725\n",
      "Iteración 24146 - Lote 210/352 - Pérdida de Entrenamiento: 1.1771, Precisión de Entrenamiento: 0.5753\n",
      "Iteración 24181 - Lote 245/352 - Pérdida de Entrenamiento: 1.1785, Precisión de Entrenamiento: 0.5747\n",
      "Iteración 24216 - Lote 280/352 - Pérdida de Entrenamiento: 1.1766, Precisión de Entrenamiento: 0.5763\n",
      "Iteración 24251 - Lote 315/352 - Pérdida de Entrenamiento: 1.1781, Precisión de Entrenamiento: 0.5757\n",
      "Iteración 24286 - Lote 350/352 - Pérdida de Entrenamiento: 1.1804, Precisión de Entrenamiento: 0.5740\n",
      "Val loss: 1.1559, Val acc: 0.5892\n",
      "0.1\n",
      "Epoch 70/400\n",
      "Iteración 24323 - Lote 35/352 - Pérdida de Entrenamiento: 1.1675, Precisión de Entrenamiento: 0.5815\n",
      "Iteración 24358 - Lote 70/352 - Pérdida de Entrenamiento: 1.1652, Precisión de Entrenamiento: 0.5780\n",
      "Iteración 24393 - Lote 105/352 - Pérdida de Entrenamiento: 1.1706, Precisión de Entrenamiento: 0.5789\n",
      "Iteración 24428 - Lote 140/352 - Pérdida de Entrenamiento: 1.1753, Precisión de Entrenamiento: 0.5775\n",
      "Iteración 24463 - Lote 175/352 - Pérdida de Entrenamiento: 1.1918, Precisión de Entrenamiento: 0.5692\n",
      "Iteración 24498 - Lote 210/352 - Pérdida de Entrenamiento: 1.1985, Precisión de Entrenamiento: 0.5677\n",
      "Iteración 24533 - Lote 245/352 - Pérdida de Entrenamiento: 1.2060, Precisión de Entrenamiento: 0.5647\n",
      "Iteración 24568 - Lote 280/352 - Pérdida de Entrenamiento: 1.2135, Precisión de Entrenamiento: 0.5607\n",
      "Iteración 24603 - Lote 315/352 - Pérdida de Entrenamiento: 1.2163, Precisión de Entrenamiento: 0.5605\n",
      "Iteración 24638 - Lote 350/352 - Pérdida de Entrenamiento: 1.2146, Precisión de Entrenamiento: 0.5614\n",
      "Val loss: 1.1823, Val acc: 0.5790\n",
      "Gradientes para features.0.0.weight: min=-0.01792989671230316, max=0.0029439914505928755, mean=-0.0004232242645230144, std=0.002151827560737729\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.01512121595442295, max=0.014769013039767742, mean=3.97208759750356e-06, std=0.0011506103910505772\n",
      "Gradientes para classifier.1.weight: min=-0.0124515937641263, max=0.01072901114821434, mean=2.3283064712331658e-11, std=0.0016573227476328611\n",
      "0.1\n",
      "Epoch 71/400\n",
      "Iteración 24675 - Lote 35/352 - Pérdida de Entrenamiento: 1.2419, Precisión de Entrenamiento: 0.5540\n",
      "Iteración 24710 - Lote 70/352 - Pérdida de Entrenamiento: 1.2119, Precisión de Entrenamiento: 0.5695\n",
      "Iteración 24745 - Lote 105/352 - Pérdida de Entrenamiento: 1.1991, Precisión de Entrenamiento: 0.5712\n",
      "Iteración 24780 - Lote 140/352 - Pérdida de Entrenamiento: 1.1904, Precisión de Entrenamiento: 0.5738\n",
      "Iteración 24815 - Lote 175/352 - Pérdida de Entrenamiento: 1.1844, Precisión de Entrenamiento: 0.5753\n",
      "Iteración 24850 - Lote 210/352 - Pérdida de Entrenamiento: 1.1806, Precisión de Entrenamiento: 0.5749\n",
      "Iteración 24885 - Lote 245/352 - Pérdida de Entrenamiento: 1.1738, Precisión de Entrenamiento: 0.5780\n",
      "Iteración 24920 - Lote 280/352 - Pérdida de Entrenamiento: 1.1718, Precisión de Entrenamiento: 0.5789\n",
      "Iteración 24955 - Lote 315/352 - Pérdida de Entrenamiento: 1.1720, Precisión de Entrenamiento: 0.5785\n",
      "Iteración 24990 - Lote 350/352 - Pérdida de Entrenamiento: 1.1684, Precisión de Entrenamiento: 0.5797\n",
      "Val loss: 1.1105, Val acc: 0.5986\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_71.pth\n",
      "Checkpoint del mejor modelo guardado en la época 71\n",
      "0.1\n",
      "Epoch 72/400\n",
      "Iteración 25027 - Lote 35/352 - Pérdida de Entrenamiento: 1.1212, Precisión de Entrenamiento: 0.6016\n",
      "Iteración 25062 - Lote 70/352 - Pérdida de Entrenamiento: 1.1347, Precisión de Entrenamiento: 0.5934\n",
      "Iteración 25097 - Lote 105/352 - Pérdida de Entrenamiento: 1.1449, Precisión de Entrenamiento: 0.5892\n",
      "Iteración 25132 - Lote 140/352 - Pérdida de Entrenamiento: 1.1347, Precisión de Entrenamiento: 0.5918\n",
      "Iteración 25167 - Lote 175/352 - Pérdida de Entrenamiento: 1.1333, Precisión de Entrenamiento: 0.5932\n",
      "Iteración 25202 - Lote 210/352 - Pérdida de Entrenamiento: 1.1315, Precisión de Entrenamiento: 0.5952\n",
      "Iteración 25237 - Lote 245/352 - Pérdida de Entrenamiento: 1.1441, Precisión de Entrenamiento: 0.5901\n",
      "Iteración 25272 - Lote 280/352 - Pérdida de Entrenamiento: 1.1481, Precisión de Entrenamiento: 0.5890\n",
      "Iteración 25307 - Lote 315/352 - Pérdida de Entrenamiento: 1.1482, Precisión de Entrenamiento: 0.5886\n",
      "Iteración 25342 - Lote 350/352 - Pérdida de Entrenamiento: 1.1508, Precisión de Entrenamiento: 0.5871\n",
      "Val loss: 1.0851, Val acc: 0.6104\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_72.pth\n",
      "Checkpoint del mejor modelo guardado en la época 72\n",
      "0.1\n",
      "Epoch 73/400\n",
      "Iteración 25379 - Lote 35/352 - Pérdida de Entrenamiento: 1.1262, Precisión de Entrenamiento: 0.5975\n",
      "Iteración 25414 - Lote 70/352 - Pérdida de Entrenamiento: 1.1289, Precisión de Entrenamiento: 0.5960\n",
      "Iteración 25449 - Lote 105/352 - Pérdida de Entrenamiento: 1.1340, Precisión de Entrenamiento: 0.5931\n",
      "Iteración 25484 - Lote 140/352 - Pérdida de Entrenamiento: 1.1380, Precisión de Entrenamiento: 0.5915\n",
      "Iteración 25519 - Lote 175/352 - Pérdida de Entrenamiento: 1.1427, Precisión de Entrenamiento: 0.5890\n",
      "Iteración 25554 - Lote 210/352 - Pérdida de Entrenamiento: 1.1423, Precisión de Entrenamiento: 0.5898\n",
      "Iteración 25589 - Lote 245/352 - Pérdida de Entrenamiento: 1.1412, Precisión de Entrenamiento: 0.5906\n",
      "Iteración 25624 - Lote 280/352 - Pérdida de Entrenamiento: 1.1411, Precisión de Entrenamiento: 0.5899\n",
      "Iteración 25659 - Lote 315/352 - Pérdida de Entrenamiento: 1.1398, Precisión de Entrenamiento: 0.5902\n",
      "Iteración 25694 - Lote 350/352 - Pérdida de Entrenamiento: 1.1370, Precisión de Entrenamiento: 0.5919\n",
      "Val loss: 1.1008, Val acc: 0.6008\n",
      "0.1\n",
      "Epoch 74/400\n",
      "Iteración 25731 - Lote 35/352 - Pérdida de Entrenamiento: 1.1091, Precisión de Entrenamiento: 0.6056\n",
      "Iteración 25766 - Lote 70/352 - Pérdida de Entrenamiento: 1.1122, Precisión de Entrenamiento: 0.6031\n",
      "Iteración 25801 - Lote 105/352 - Pérdida de Entrenamiento: 1.1155, Precisión de Entrenamiento: 0.6001\n",
      "Iteración 25836 - Lote 140/352 - Pérdida de Entrenamiento: 1.1179, Precisión de Entrenamiento: 0.6006\n",
      "Iteración 25871 - Lote 175/352 - Pérdida de Entrenamiento: 1.1193, Precisión de Entrenamiento: 0.5992\n",
      "Iteración 25906 - Lote 210/352 - Pérdida de Entrenamiento: 1.1398, Precisión de Entrenamiento: 0.5920\n",
      "Iteración 25941 - Lote 245/352 - Pérdida de Entrenamiento: 1.1588, Precisión de Entrenamiento: 0.5851\n",
      "Iteración 25976 - Lote 280/352 - Pérdida de Entrenamiento: 1.1616, Precisión de Entrenamiento: 0.5837\n",
      "Iteración 26011 - Lote 315/352 - Pérdida de Entrenamiento: 1.1627, Precisión de Entrenamiento: 0.5822\n",
      "Iteración 26046 - Lote 350/352 - Pérdida de Entrenamiento: 1.1624, Precisión de Entrenamiento: 0.5816\n",
      "Val loss: 1.1523, Val acc: 0.5882\n",
      "0.1\n",
      "Epoch 75/400\n",
      "Iteración 26083 - Lote 35/352 - Pérdida de Entrenamiento: 1.1429, Precisión de Entrenamiento: 0.5924\n",
      "Iteración 26118 - Lote 70/352 - Pérdida de Entrenamiento: 1.1613, Precisión de Entrenamiento: 0.5811\n",
      "Iteración 26153 - Lote 105/352 - Pérdida de Entrenamiento: 1.1465, Precisión de Entrenamiento: 0.5860\n",
      "Iteración 26188 - Lote 140/352 - Pérdida de Entrenamiento: 1.1441, Precisión de Entrenamiento: 0.5855\n",
      "Iteración 26223 - Lote 175/352 - Pérdida de Entrenamiento: 1.1470, Precisión de Entrenamiento: 0.5862\n",
      "Iteración 26258 - Lote 210/352 - Pérdida de Entrenamiento: 1.1430, Precisión de Entrenamiento: 0.5885\n",
      "Iteración 26293 - Lote 245/352 - Pérdida de Entrenamiento: 1.1396, Precisión de Entrenamiento: 0.5909\n",
      "Iteración 26328 - Lote 280/352 - Pérdida de Entrenamiento: 1.1413, Precisión de Entrenamiento: 0.5914\n",
      "Iteración 26363 - Lote 315/352 - Pérdida de Entrenamiento: 1.1414, Precisión de Entrenamiento: 0.5910\n",
      "Iteración 26398 - Lote 350/352 - Pérdida de Entrenamiento: 1.1395, Precisión de Entrenamiento: 0.5914\n",
      "Val loss: 1.0957, Val acc: 0.5988\n",
      "Gradientes para features.0.0.weight: min=-0.006799176335334778, max=0.02667050249874592, mean=0.0008316222229041159, std=0.0036712309811264277\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.016930941492319107, max=0.01410097349435091, mean=3.8773529809077445e-07, std=0.0011703565251082182\n",
      "Gradientes para classifier.1.weight: min=-0.012711646035313606, max=0.011848561465740204, mean=-9.313225537987968e-12, std=0.0017095396760851145\n",
      "0.1\n",
      "Epoch 76/400\n",
      "Iteración 26435 - Lote 35/352 - Pérdida de Entrenamiento: 1.1361, Precisión de Entrenamiento: 0.5935\n",
      "Iteración 26470 - Lote 70/352 - Pérdida de Entrenamiento: 1.1297, Precisión de Entrenamiento: 0.5911\n",
      "Iteración 26505 - Lote 105/352 - Pérdida de Entrenamiento: 1.1246, Precisión de Entrenamiento: 0.5966\n",
      "Iteración 26540 - Lote 140/352 - Pérdida de Entrenamiento: 1.1247, Precisión de Entrenamiento: 0.5944\n",
      "Iteración 26575 - Lote 175/352 - Pérdida de Entrenamiento: 1.1181, Precisión de Entrenamiento: 0.5952\n",
      "Iteración 26610 - Lote 210/352 - Pérdida de Entrenamiento: 1.1228, Precisión de Entrenamiento: 0.5953\n",
      "Iteración 26645 - Lote 245/352 - Pérdida de Entrenamiento: 1.1254, Precisión de Entrenamiento: 0.5941\n",
      "Iteración 26680 - Lote 280/352 - Pérdida de Entrenamiento: 1.1259, Precisión de Entrenamiento: 0.5932\n",
      "Iteración 26715 - Lote 315/352 - Pérdida de Entrenamiento: 1.1239, Precisión de Entrenamiento: 0.5946\n",
      "Iteración 26750 - Lote 350/352 - Pérdida de Entrenamiento: 1.1247, Precisión de Entrenamiento: 0.5937\n",
      "Val loss: 1.1027, Val acc: 0.6038\n",
      "0.1\n",
      "Epoch 77/400\n",
      "Iteración 26787 - Lote 35/352 - Pérdida de Entrenamiento: 1.1006, Precisión de Entrenamiento: 0.6156\n",
      "Iteración 26822 - Lote 70/352 - Pérdida de Entrenamiento: 1.1074, Precisión de Entrenamiento: 0.6060\n",
      "Iteración 26857 - Lote 105/352 - Pérdida de Entrenamiento: 1.1079, Precisión de Entrenamiento: 0.6046\n",
      "Iteración 26892 - Lote 140/352 - Pérdida de Entrenamiento: 1.1091, Precisión de Entrenamiento: 0.6043\n",
      "Iteración 26927 - Lote 175/352 - Pérdida de Entrenamiento: 1.1179, Precisión de Entrenamiento: 0.6011\n",
      "Iteración 26962 - Lote 210/352 - Pérdida de Entrenamiento: 1.1176, Precisión de Entrenamiento: 0.6011\n",
      "Iteración 26997 - Lote 245/352 - Pérdida de Entrenamiento: 1.1148, Precisión de Entrenamiento: 0.6017\n",
      "Iteración 27032 - Lote 280/352 - Pérdida de Entrenamiento: 1.1122, Precisión de Entrenamiento: 0.6023\n",
      "Iteración 27067 - Lote 315/352 - Pérdida de Entrenamiento: 1.1125, Precisión de Entrenamiento: 0.6007\n",
      "Iteración 27102 - Lote 350/352 - Pérdida de Entrenamiento: 1.1138, Precisión de Entrenamiento: 0.5998\n",
      "Val loss: 1.0847, Val acc: 0.6110\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_77.pth\n",
      "Checkpoint del mejor modelo guardado en la época 77\n",
      "0.1\n",
      "Epoch 78/400\n",
      "Iteración 27139 - Lote 35/352 - Pérdida de Entrenamiento: 1.0882, Precisión de Entrenamiento: 0.6049\n",
      "Iteración 27174 - Lote 70/352 - Pérdida de Entrenamiento: 1.0997, Precisión de Entrenamiento: 0.6009\n",
      "Iteración 27209 - Lote 105/352 - Pérdida de Entrenamiento: 1.1283, Precisión de Entrenamiento: 0.5932\n",
      "Iteración 27244 - Lote 140/352 - Pérdida de Entrenamiento: 1.1338, Precisión de Entrenamiento: 0.5925\n",
      "Iteración 27279 - Lote 175/352 - Pérdida de Entrenamiento: 1.1324, Precisión de Entrenamiento: 0.5923\n",
      "Iteración 27314 - Lote 210/352 - Pérdida de Entrenamiento: 1.1296, Precisión de Entrenamiento: 0.5935\n",
      "Iteración 27349 - Lote 245/352 - Pérdida de Entrenamiento: 1.1304, Precisión de Entrenamiento: 0.5925\n",
      "Iteración 27384 - Lote 280/352 - Pérdida de Entrenamiento: 1.1361, Precisión de Entrenamiento: 0.5903\n",
      "Iteración 27419 - Lote 315/352 - Pérdida de Entrenamiento: 1.1552, Precisión de Entrenamiento: 0.5829\n",
      "Iteración 27454 - Lote 350/352 - Pérdida de Entrenamiento: 1.1627, Precisión de Entrenamiento: 0.5803\n",
      "Val loss: 1.1366, Val acc: 0.5828\n",
      "0.1\n",
      "Epoch 79/400\n",
      "Iteración 27491 - Lote 35/352 - Pérdida de Entrenamiento: 1.1430, Precisión de Entrenamiento: 0.5835\n",
      "Iteración 27526 - Lote 70/352 - Pérdida de Entrenamiento: 1.1438, Precisión de Entrenamiento: 0.5862\n",
      "Iteración 27561 - Lote 105/352 - Pérdida de Entrenamiento: 1.1411, Precisión de Entrenamiento: 0.5876\n",
      "Iteración 27596 - Lote 140/352 - Pérdida de Entrenamiento: 1.1418, Precisión de Entrenamiento: 0.5891\n",
      "Iteración 27631 - Lote 175/352 - Pérdida de Entrenamiento: 1.1419, Precisión de Entrenamiento: 0.5886\n",
      "Iteración 27666 - Lote 210/352 - Pérdida de Entrenamiento: 1.1462, Precisión de Entrenamiento: 0.5868\n",
      "Iteración 27701 - Lote 245/352 - Pérdida de Entrenamiento: 1.1447, Precisión de Entrenamiento: 0.5881\n",
      "Iteración 27736 - Lote 280/352 - Pérdida de Entrenamiento: 1.1511, Precisión de Entrenamiento: 0.5860\n",
      "Iteración 27771 - Lote 315/352 - Pérdida de Entrenamiento: 1.1486, Precisión de Entrenamiento: 0.5875\n",
      "Iteración 27806 - Lote 350/352 - Pérdida de Entrenamiento: 1.1460, Precisión de Entrenamiento: 0.5886\n",
      "Val loss: 1.0890, Val acc: 0.6126\n",
      "0.1\n",
      "Epoch 80/400\n",
      "Iteración 27843 - Lote 35/352 - Pérdida de Entrenamiento: 1.0897, Precisión de Entrenamiento: 0.6170\n",
      "Iteración 27878 - Lote 70/352 - Pérdida de Entrenamiento: 1.0984, Precisión de Entrenamiento: 0.6110\n",
      "Iteración 27913 - Lote 105/352 - Pérdida de Entrenamiento: 1.1031, Precisión de Entrenamiento: 0.6085\n",
      "Iteración 27948 - Lote 140/352 - Pérdida de Entrenamiento: 1.0971, Precisión de Entrenamiento: 0.6085\n",
      "Iteración 27983 - Lote 175/352 - Pérdida de Entrenamiento: 1.1067, Precisión de Entrenamiento: 0.6061\n",
      "Iteración 28018 - Lote 210/352 - Pérdida de Entrenamiento: 1.1077, Precisión de Entrenamiento: 0.6049\n",
      "Iteración 28053 - Lote 245/352 - Pérdida de Entrenamiento: 1.1115, Precisión de Entrenamiento: 0.6025\n",
      "Iteración 28088 - Lote 280/352 - Pérdida de Entrenamiento: 1.1069, Precisión de Entrenamiento: 0.6041\n",
      "Iteración 28123 - Lote 315/352 - Pérdida de Entrenamiento: 1.1097, Precisión de Entrenamiento: 0.6023\n",
      "Iteración 28158 - Lote 350/352 - Pérdida de Entrenamiento: 1.1091, Precisión de Entrenamiento: 0.6016\n",
      "Val loss: 1.0631, Val acc: 0.6208\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_80.pth\n",
      "Checkpoint del mejor modelo guardado en la época 80\n",
      "Gradientes para features.0.0.weight: min=-0.009622836485505104, max=0.00571500975638628, mean=-9.103772754315287e-05, std=0.001085013383999467\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.014564082026481628, max=0.012152038514614105, mean=-2.4248909085144987e-06, std=0.0009974525310099125\n",
      "Gradientes para classifier.1.weight: min=-0.011542598716914654, max=0.009195217862725258, mean=9.313225537987968e-12, std=0.0017041866667568684\n",
      "0.1\n",
      "Epoch 81/400\n",
      "Iteración 28195 - Lote 35/352 - Pérdida de Entrenamiento: 1.0932, Precisión de Entrenamiento: 0.6100\n",
      "Iteración 28230 - Lote 70/352 - Pérdida de Entrenamiento: 1.0941, Precisión de Entrenamiento: 0.6100\n",
      "Iteración 28265 - Lote 105/352 - Pérdida de Entrenamiento: 1.0978, Precisión de Entrenamiento: 0.6067\n",
      "Iteración 28300 - Lote 140/352 - Pérdida de Entrenamiento: 1.0985, Precisión de Entrenamiento: 0.6065\n",
      "Iteración 28335 - Lote 175/352 - Pérdida de Entrenamiento: 1.0998, Precisión de Entrenamiento: 0.6074\n",
      "Iteración 28370 - Lote 210/352 - Pérdida de Entrenamiento: 1.0948, Precisión de Entrenamiento: 0.6086\n",
      "Iteración 28405 - Lote 245/352 - Pérdida de Entrenamiento: 1.0945, Precisión de Entrenamiento: 0.6085\n",
      "Iteración 28440 - Lote 280/352 - Pérdida de Entrenamiento: 1.0937, Precisión de Entrenamiento: 0.6078\n",
      "Iteración 28475 - Lote 315/352 - Pérdida de Entrenamiento: 1.0941, Precisión de Entrenamiento: 0.6084\n",
      "Iteración 28510 - Lote 350/352 - Pérdida de Entrenamiento: 1.0937, Precisión de Entrenamiento: 0.6081\n",
      "Val loss: 1.0748, Val acc: 0.6140\n",
      "0.1\n",
      "Epoch 82/400\n",
      "Iteración 28547 - Lote 35/352 - Pérdida de Entrenamiento: 1.0723, Precisión de Entrenamiento: 0.6132\n",
      "Iteración 28582 - Lote 70/352 - Pérdida de Entrenamiento: 1.0881, Precisión de Entrenamiento: 0.6108\n",
      "Iteración 28617 - Lote 105/352 - Pérdida de Entrenamiento: 1.0801, Precisión de Entrenamiento: 0.6110\n",
      "Iteración 28652 - Lote 140/352 - Pérdida de Entrenamiento: 1.0781, Precisión de Entrenamiento: 0.6112\n",
      "Iteración 28687 - Lote 175/352 - Pérdida de Entrenamiento: 1.0769, Precisión de Entrenamiento: 0.6125\n",
      "Iteración 28722 - Lote 210/352 - Pérdida de Entrenamiento: 1.0941, Precisión de Entrenamiento: 0.6060\n",
      "Iteración 28757 - Lote 245/352 - Pérdida de Entrenamiento: 1.1042, Precisión de Entrenamiento: 0.6026\n",
      "Iteración 28792 - Lote 280/352 - Pérdida de Entrenamiento: 1.1065, Precisión de Entrenamiento: 0.6019\n",
      "Iteración 28827 - Lote 315/352 - Pérdida de Entrenamiento: 1.1054, Precisión de Entrenamiento: 0.6026\n",
      "Iteración 28862 - Lote 350/352 - Pérdida de Entrenamiento: 1.1064, Precisión de Entrenamiento: 0.6031\n",
      "Val loss: 1.0664, Val acc: 0.6152\n",
      "0.1\n",
      "Epoch 83/400\n",
      "Iteración 28899 - Lote 35/352 - Pérdida de Entrenamiento: 1.0527, Precisión de Entrenamiento: 0.6212\n",
      "Iteración 28934 - Lote 70/352 - Pérdida de Entrenamiento: 1.0723, Precisión de Entrenamiento: 0.6191\n",
      "Iteración 28969 - Lote 105/352 - Pérdida de Entrenamiento: 1.0821, Precisión de Entrenamiento: 0.6133\n",
      "Iteración 29004 - Lote 140/352 - Pérdida de Entrenamiento: 1.0875, Precisión de Entrenamiento: 0.6093\n",
      "Iteración 29039 - Lote 175/352 - Pérdida de Entrenamiento: 1.0838, Precisión de Entrenamiento: 0.6122\n",
      "Iteración 29074 - Lote 210/352 - Pérdida de Entrenamiento: 1.0868, Precisión de Entrenamiento: 0.6107\n",
      "Iteración 29109 - Lote 245/352 - Pérdida de Entrenamiento: 1.0888, Precisión de Entrenamiento: 0.6100\n",
      "Iteración 29144 - Lote 280/352 - Pérdida de Entrenamiento: 1.0872, Precisión de Entrenamiento: 0.6109\n",
      "Iteración 29179 - Lote 315/352 - Pérdida de Entrenamiento: 1.0924, Precisión de Entrenamiento: 0.6091\n",
      "Iteración 29214 - Lote 350/352 - Pérdida de Entrenamiento: 1.0931, Precisión de Entrenamiento: 0.6091\n",
      "Val loss: 1.0745, Val acc: 0.6186\n",
      "0.1\n",
      "Epoch 84/400\n",
      "Iteración 29251 - Lote 35/352 - Pérdida de Entrenamiento: 1.0740, Precisión de Entrenamiento: 0.6199\n",
      "Iteración 29286 - Lote 70/352 - Pérdida de Entrenamiento: 1.0853, Precisión de Entrenamiento: 0.6161\n",
      "Iteración 29321 - Lote 105/352 - Pérdida de Entrenamiento: 1.0856, Precisión de Entrenamiento: 0.6170\n",
      "Iteración 29356 - Lote 140/352 - Pérdida de Entrenamiento: 1.0867, Precisión de Entrenamiento: 0.6170\n",
      "Iteración 29391 - Lote 175/352 - Pérdida de Entrenamiento: 1.0846, Precisión de Entrenamiento: 0.6168\n",
      "Iteración 29426 - Lote 210/352 - Pérdida de Entrenamiento: 1.0788, Precisión de Entrenamiento: 0.6177\n",
      "Iteración 29461 - Lote 245/352 - Pérdida de Entrenamiento: 1.0775, Precisión de Entrenamiento: 0.6170\n",
      "Iteración 29496 - Lote 280/352 - Pérdida de Entrenamiento: 1.0751, Precisión de Entrenamiento: 0.6177\n",
      "Iteración 29531 - Lote 315/352 - Pérdida de Entrenamiento: 1.0754, Precisión de Entrenamiento: 0.6182\n",
      "Iteración 29566 - Lote 350/352 - Pérdida de Entrenamiento: 1.0762, Precisión de Entrenamiento: 0.6169\n",
      "Val loss: 1.0496, Val acc: 0.6234\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_84.pth\n",
      "Checkpoint del mejor modelo guardado en la época 84\n",
      "0.1\n",
      "Epoch 85/400\n",
      "Iteración 29603 - Lote 35/352 - Pérdida de Entrenamiento: 1.0587, Precisión de Entrenamiento: 0.6181\n",
      "Iteración 29638 - Lote 70/352 - Pérdida de Entrenamiento: 1.0884, Precisión de Entrenamiento: 0.6092\n",
      "Iteración 29673 - Lote 105/352 - Pérdida de Entrenamiento: 1.0929, Precisión de Entrenamiento: 0.6106\n",
      "Iteración 29708 - Lote 140/352 - Pérdida de Entrenamiento: 1.0796, Precisión de Entrenamiento: 0.6147\n",
      "Iteración 29743 - Lote 175/352 - Pérdida de Entrenamiento: 1.0847, Precisión de Entrenamiento: 0.6102\n",
      "Iteración 29778 - Lote 210/352 - Pérdida de Entrenamiento: 1.0812, Precisión de Entrenamiento: 0.6115\n",
      "Iteración 29813 - Lote 245/352 - Pérdida de Entrenamiento: 1.0784, Precisión de Entrenamiento: 0.6134\n",
      "Iteración 29848 - Lote 280/352 - Pérdida de Entrenamiento: 1.0777, Precisión de Entrenamiento: 0.6141\n",
      "Iteración 29883 - Lote 315/352 - Pérdida de Entrenamiento: 1.0780, Precisión de Entrenamiento: 0.6133\n",
      "Iteración 29918 - Lote 350/352 - Pérdida de Entrenamiento: 1.0807, Precisión de Entrenamiento: 0.6126\n",
      "Val loss: 1.0909, Val acc: 0.6040\n",
      "Gradientes para features.0.0.weight: min=-0.022856639698147774, max=0.020459089428186417, mean=4.811167673324235e-05, std=0.0037890824023634195\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.014905266463756561, max=0.012925341725349426, mean=-1.3007762618144625e-06, std=0.0011666202917695045\n",
      "Gradientes para classifier.1.weight: min=-0.012682661414146423, max=0.018309466540813446, mean=-6.984919153490976e-12, std=0.002064635744318366\n",
      "0.1\n",
      "Epoch 86/400\n",
      "Iteración 29955 - Lote 35/352 - Pérdida de Entrenamiento: 1.0626, Precisión de Entrenamiento: 0.6167\n",
      "Iteración 29990 - Lote 70/352 - Pérdida de Entrenamiento: 1.0497, Precisión de Entrenamiento: 0.6254\n",
      "Iteración 30025 - Lote 105/352 - Pérdida de Entrenamiento: 1.0581, Precisión de Entrenamiento: 0.6220\n",
      "Iteración 30060 - Lote 140/352 - Pérdida de Entrenamiento: 1.0618, Precisión de Entrenamiento: 0.6205\n",
      "Iteración 30095 - Lote 175/352 - Pérdida de Entrenamiento: 1.0613, Precisión de Entrenamiento: 0.6203\n",
      "Iteración 30130 - Lote 210/352 - Pérdida de Entrenamiento: 1.0650, Precisión de Entrenamiento: 0.6193\n",
      "Iteración 30165 - Lote 245/352 - Pérdida de Entrenamiento: 1.0644, Precisión de Entrenamiento: 0.6190\n",
      "Iteración 30200 - Lote 280/352 - Pérdida de Entrenamiento: 1.0595, Precisión de Entrenamiento: 0.6209\n",
      "Iteración 30235 - Lote 315/352 - Pérdida de Entrenamiento: 1.0746, Precisión de Entrenamiento: 0.6152\n",
      "Iteración 30270 - Lote 350/352 - Pérdida de Entrenamiento: 1.0780, Precisión de Entrenamiento: 0.6147\n",
      "Val loss: 1.0644, Val acc: 0.6072\n",
      "0.1\n",
      "Epoch 87/400\n",
      "Iteración 30307 - Lote 35/352 - Pérdida de Entrenamiento: 1.0424, Precisión de Entrenamiento: 0.6194\n",
      "Iteración 30342 - Lote 70/352 - Pérdida de Entrenamiento: 1.0470, Precisión de Entrenamiento: 0.6230\n",
      "Iteración 30377 - Lote 105/352 - Pérdida de Entrenamiento: 1.0489, Precisión de Entrenamiento: 0.6225\n",
      "Iteración 30412 - Lote 140/352 - Pérdida de Entrenamiento: 1.0488, Precisión de Entrenamiento: 0.6233\n",
      "Iteración 30447 - Lote 175/352 - Pérdida de Entrenamiento: 1.0493, Precisión de Entrenamiento: 0.6244\n",
      "Iteración 30482 - Lote 210/352 - Pérdida de Entrenamiento: 1.0524, Precisión de Entrenamiento: 0.6229\n",
      "Iteración 30517 - Lote 245/352 - Pérdida de Entrenamiento: 1.0515, Precisión de Entrenamiento: 0.6231\n",
      "Iteración 30552 - Lote 280/352 - Pérdida de Entrenamiento: 1.0497, Precisión de Entrenamiento: 0.6228\n",
      "Iteración 30587 - Lote 315/352 - Pérdida de Entrenamiento: 1.0571, Precisión de Entrenamiento: 0.6197\n",
      "Iteración 30622 - Lote 350/352 - Pérdida de Entrenamiento: 1.0600, Precisión de Entrenamiento: 0.6195\n",
      "Val loss: 1.0252, Val acc: 0.6280\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_87.pth\n",
      "Checkpoint del mejor modelo guardado en la época 87\n",
      "0.1\n",
      "Epoch 88/400\n",
      "Iteración 30659 - Lote 35/352 - Pérdida de Entrenamiento: 1.0456, Precisión de Entrenamiento: 0.6266\n",
      "Iteración 30694 - Lote 70/352 - Pérdida de Entrenamiento: 1.0507, Precisión de Entrenamiento: 0.6227\n",
      "Iteración 30729 - Lote 105/352 - Pérdida de Entrenamiento: 1.0560, Precisión de Entrenamiento: 0.6193\n",
      "Iteración 30764 - Lote 140/352 - Pérdida de Entrenamiento: 1.0611, Precisión de Entrenamiento: 0.6184\n",
      "Iteración 30799 - Lote 175/352 - Pérdida de Entrenamiento: 1.0540, Precisión de Entrenamiento: 0.6216\n",
      "Iteración 30834 - Lote 210/352 - Pérdida de Entrenamiento: 1.0710, Precisión de Entrenamiento: 0.6160\n",
      "Iteración 30869 - Lote 245/352 - Pérdida de Entrenamiento: 1.0830, Precisión de Entrenamiento: 0.6122\n",
      "Iteración 30904 - Lote 280/352 - Pérdida de Entrenamiento: 1.0873, Precisión de Entrenamiento: 0.6105\n",
      "Iteración 30939 - Lote 315/352 - Pérdida de Entrenamiento: 1.0941, Precisión de Entrenamiento: 0.6080\n",
      "Iteración 30974 - Lote 350/352 - Pérdida de Entrenamiento: 1.0913, Precisión de Entrenamiento: 0.6096\n",
      "Val loss: 1.0356, Val acc: 0.6278\n",
      "0.1\n",
      "Epoch 89/400\n",
      "Iteración 31011 - Lote 35/352 - Pérdida de Entrenamiento: 1.0805, Precisión de Entrenamiento: 0.6154\n",
      "Iteración 31046 - Lote 70/352 - Pérdida de Entrenamiento: 1.0767, Precisión de Entrenamiento: 0.6114\n",
      "Iteración 31081 - Lote 105/352 - Pérdida de Entrenamiento: 1.0637, Precisión de Entrenamiento: 0.6167\n",
      "Iteración 31116 - Lote 140/352 - Pérdida de Entrenamiento: 1.0654, Precisión de Entrenamiento: 0.6184\n",
      "Iteración 31151 - Lote 175/352 - Pérdida de Entrenamiento: 1.0655, Precisión de Entrenamiento: 0.6192\n",
      "Iteración 31186 - Lote 210/352 - Pérdida de Entrenamiento: 1.0614, Precisión de Entrenamiento: 0.6191\n",
      "Iteración 31221 - Lote 245/352 - Pérdida de Entrenamiento: 1.0641, Precisión de Entrenamiento: 0.6188\n",
      "Iteración 31256 - Lote 280/352 - Pérdida de Entrenamiento: 1.0642, Precisión de Entrenamiento: 0.6191\n",
      "Iteración 31291 - Lote 315/352 - Pérdida de Entrenamiento: 1.0635, Precisión de Entrenamiento: 0.6195\n",
      "Iteración 31326 - Lote 350/352 - Pérdida de Entrenamiento: 1.0651, Precisión de Entrenamiento: 0.6186\n",
      "Val loss: 1.0188, Val acc: 0.6380\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_89.pth\n",
      "Checkpoint del mejor modelo guardado en la época 89\n",
      "0.1\n",
      "Epoch 90/400\n",
      "Iteración 31363 - Lote 35/352 - Pérdida de Entrenamiento: 1.0538, Precisión de Entrenamiento: 0.6194\n",
      "Iteración 31398 - Lote 70/352 - Pérdida de Entrenamiento: 1.0556, Precisión de Entrenamiento: 0.6214\n",
      "Iteración 31433 - Lote 105/352 - Pérdida de Entrenamiento: 1.0500, Precisión de Entrenamiento: 0.6254\n",
      "Iteración 31468 - Lote 140/352 - Pérdida de Entrenamiento: 1.0454, Precisión de Entrenamiento: 0.6278\n",
      "Iteración 31503 - Lote 175/352 - Pérdida de Entrenamiento: 1.0740, Precisión de Entrenamiento: 0.6175\n",
      "Iteración 31538 - Lote 210/352 - Pérdida de Entrenamiento: 1.1044, Precisión de Entrenamiento: 0.6062\n",
      "Iteración 31573 - Lote 245/352 - Pérdida de Entrenamiento: 1.1081, Precisión de Entrenamiento: 0.6048\n",
      "Iteración 31608 - Lote 280/352 - Pérdida de Entrenamiento: 1.1075, Precisión de Entrenamiento: 0.6059\n",
      "Iteración 31643 - Lote 315/352 - Pérdida de Entrenamiento: 1.1052, Precisión de Entrenamiento: 0.6073\n",
      "Iteración 31678 - Lote 350/352 - Pérdida de Entrenamiento: 1.1069, Precisión de Entrenamiento: 0.6056\n",
      "Val loss: 1.0412, Val acc: 0.6298\n",
      "Gradientes para features.0.0.weight: min=-0.003949083387851715, max=0.006266344338655472, mean=0.0001241123682120815, std=0.0008233112166635692\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.012066598050296307, max=0.012721960432827473, mean=-5.0618996283446904e-06, std=0.0009914907859638333\n",
      "Gradientes para classifier.1.weight: min=-0.010131090879440308, max=0.011917137540876865, mean=-7.566995641195007e-12, std=0.0016050563426688313\n",
      "0.1\n",
      "Epoch 91/400\n",
      "Iteración 31715 - Lote 35/352 - Pérdida de Entrenamiento: 1.0383, Precisión de Entrenamiento: 0.6317\n",
      "Iteración 31750 - Lote 70/352 - Pérdida de Entrenamiento: 1.0680, Precisión de Entrenamiento: 0.6193\n",
      "Iteración 31785 - Lote 105/352 - Pérdida de Entrenamiento: 1.0681, Precisión de Entrenamiento: 0.6207\n",
      "Iteración 31820 - Lote 140/352 - Pérdida de Entrenamiento: 1.0612, Precisión de Entrenamiento: 0.6207\n",
      "Iteración 31855 - Lote 175/352 - Pérdida de Entrenamiento: 1.0525, Precisión de Entrenamiento: 0.6234\n",
      "Iteración 31890 - Lote 210/352 - Pérdida de Entrenamiento: 1.0561, Precisión de Entrenamiento: 0.6211\n",
      "Iteración 31925 - Lote 245/352 - Pérdida de Entrenamiento: 1.0592, Precisión de Entrenamiento: 0.6199\n",
      "Iteración 31960 - Lote 280/352 - Pérdida de Entrenamiento: 1.0635, Precisión de Entrenamiento: 0.6188\n",
      "Iteración 31995 - Lote 315/352 - Pérdida de Entrenamiento: 1.0620, Precisión de Entrenamiento: 0.6193\n",
      "Iteración 32030 - Lote 350/352 - Pérdida de Entrenamiento: 1.0637, Precisión de Entrenamiento: 0.6194\n",
      "Val loss: 1.0968, Val acc: 0.6124\n",
      "0.1\n",
      "Epoch 92/400\n",
      "Iteración 32067 - Lote 35/352 - Pérdida de Entrenamiento: 1.0882, Precisión de Entrenamiento: 0.6114\n",
      "Iteración 32102 - Lote 70/352 - Pérdida de Entrenamiento: 1.0677, Precisión de Entrenamiento: 0.6165\n",
      "Iteración 32137 - Lote 105/352 - Pérdida de Entrenamiento: 1.0572, Precisión de Entrenamiento: 0.6207\n",
      "Iteración 32172 - Lote 140/352 - Pérdida de Entrenamiento: 1.0582, Precisión de Entrenamiento: 0.6203\n",
      "Iteración 32207 - Lote 175/352 - Pérdida de Entrenamiento: 1.0537, Precisión de Entrenamiento: 0.6222\n",
      "Iteración 32242 - Lote 210/352 - Pérdida de Entrenamiento: 1.0529, Precisión de Entrenamiento: 0.6230\n",
      "Iteración 32277 - Lote 245/352 - Pérdida de Entrenamiento: 1.0496, Precisión de Entrenamiento: 0.6245\n",
      "Iteración 32312 - Lote 280/352 - Pérdida de Entrenamiento: 1.0528, Precisión de Entrenamiento: 0.6230\n",
      "Iteración 32347 - Lote 315/352 - Pérdida de Entrenamiento: 1.0522, Precisión de Entrenamiento: 0.6240\n",
      "Iteración 32382 - Lote 350/352 - Pérdida de Entrenamiento: 1.0528, Precisión de Entrenamiento: 0.6242\n",
      "Val loss: 1.0441, Val acc: 0.6256\n",
      "0.1\n",
      "Epoch 93/400\n",
      "Iteración 32419 - Lote 35/352 - Pérdida de Entrenamiento: 1.0103, Precisión de Entrenamiento: 0.6353\n",
      "Iteración 32454 - Lote 70/352 - Pérdida de Entrenamiento: 1.0199, Precisión de Entrenamiento: 0.6323\n",
      "Iteración 32489 - Lote 105/352 - Pérdida de Entrenamiento: 1.0136, Precisión de Entrenamiento: 0.6333\n",
      "Iteración 32524 - Lote 140/352 - Pérdida de Entrenamiento: 1.0172, Precisión de Entrenamiento: 0.6331\n",
      "Iteración 32559 - Lote 175/352 - Pérdida de Entrenamiento: 1.0125, Precisión de Entrenamiento: 0.6348\n",
      "Iteración 32594 - Lote 210/352 - Pérdida de Entrenamiento: 1.0124, Precisión de Entrenamiento: 0.6358\n",
      "Iteración 32629 - Lote 245/352 - Pérdida de Entrenamiento: 1.0131, Precisión de Entrenamiento: 0.6352\n",
      "Iteración 32664 - Lote 280/352 - Pérdida de Entrenamiento: 1.0174, Precisión de Entrenamiento: 0.6347\n",
      "Iteración 32699 - Lote 315/352 - Pérdida de Entrenamiento: 1.0211, Precisión de Entrenamiento: 0.6338\n",
      "Iteración 32734 - Lote 350/352 - Pérdida de Entrenamiento: 1.0206, Precisión de Entrenamiento: 0.6346\n",
      "Val loss: 1.0031, Val acc: 0.6396\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_93.pth\n",
      "Checkpoint del mejor modelo guardado en la época 93\n",
      "0.1\n",
      "Epoch 94/400\n",
      "Iteración 32771 - Lote 35/352 - Pérdida de Entrenamiento: 0.9964, Precisión de Entrenamiento: 0.6404\n",
      "Iteración 32806 - Lote 70/352 - Pérdida de Entrenamiento: 1.0189, Precisión de Entrenamiento: 0.6347\n",
      "Iteración 32841 - Lote 105/352 - Pérdida de Entrenamiento: 1.0190, Precisión de Entrenamiento: 0.6326\n",
      "Iteración 32876 - Lote 140/352 - Pérdida de Entrenamiento: 1.0167, Precisión de Entrenamiento: 0.6332\n",
      "Iteración 32911 - Lote 175/352 - Pérdida de Entrenamiento: 1.0133, Precisión de Entrenamiento: 0.6360\n",
      "Iteración 32946 - Lote 210/352 - Pérdida de Entrenamiento: 1.0233, Precisión de Entrenamiento: 0.6324\n",
      "Iteración 32981 - Lote 245/352 - Pérdida de Entrenamiento: 1.0245, Precisión de Entrenamiento: 0.6323\n",
      "Iteración 33016 - Lote 280/352 - Pérdida de Entrenamiento: 1.0230, Precisión de Entrenamiento: 0.6331\n",
      "Iteración 33051 - Lote 315/352 - Pérdida de Entrenamiento: 1.0343, Precisión de Entrenamiento: 0.6297\n",
      "Iteración 33086 - Lote 350/352 - Pérdida de Entrenamiento: 1.0463, Precisión de Entrenamiento: 0.6255\n",
      "Val loss: 1.0643, Val acc: 0.6196\n",
      "0.1\n",
      "Epoch 95/400\n",
      "Iteración 33123 - Lote 35/352 - Pérdida de Entrenamiento: 1.0508, Precisión de Entrenamiento: 0.6297\n",
      "Iteración 33158 - Lote 70/352 - Pérdida de Entrenamiento: 1.0522, Precisión de Entrenamiento: 0.6253\n",
      "Iteración 33193 - Lote 105/352 - Pérdida de Entrenamiento: 1.0487, Precisión de Entrenamiento: 0.6257\n",
      "Iteración 33228 - Lote 140/352 - Pérdida de Entrenamiento: 1.0384, Precisión de Entrenamiento: 0.6285\n",
      "Iteración 33263 - Lote 175/352 - Pérdida de Entrenamiento: 1.0382, Precisión de Entrenamiento: 0.6280\n",
      "Iteración 33298 - Lote 210/352 - Pérdida de Entrenamiento: 1.0496, Precisión de Entrenamiento: 0.6239\n",
      "Iteración 33333 - Lote 245/352 - Pérdida de Entrenamiento: 1.0473, Precisión de Entrenamiento: 0.6252\n",
      "Iteración 33368 - Lote 280/352 - Pérdida de Entrenamiento: 1.0454, Precisión de Entrenamiento: 0.6263\n",
      "Iteración 33403 - Lote 315/352 - Pérdida de Entrenamiento: 1.0477, Precisión de Entrenamiento: 0.6252\n",
      "Iteración 33438 - Lote 350/352 - Pérdida de Entrenamiento: 1.0485, Precisión de Entrenamiento: 0.6246\n",
      "Val loss: 1.0570, Val acc: 0.6206\n",
      "Gradientes para features.0.0.weight: min=-0.02678988128900528, max=0.028445057570934296, mean=-0.00014795760216657072, std=0.004310334101319313\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.021530792117118835, max=0.02407378889620304, mean=-1.4464982086792588e-05, std=0.0015352941118180752\n",
      "Gradientes para classifier.1.weight: min=-0.0110509367659688, max=0.008131925016641617, mean=1.9790604918745736e-11, std=0.0017380223143845797\n",
      "0.1\n",
      "Epoch 96/400\n",
      "Iteración 33475 - Lote 35/352 - Pérdida de Entrenamiento: 1.1143, Precisión de Entrenamiento: 0.6080\n",
      "Iteración 33510 - Lote 70/352 - Pérdida de Entrenamiento: 1.1077, Precisión de Entrenamiento: 0.6104\n",
      "Iteración 33545 - Lote 105/352 - Pérdida de Entrenamiento: 1.0807, Precisión de Entrenamiento: 0.6175\n",
      "Iteración 33580 - Lote 140/352 - Pérdida de Entrenamiento: 1.0735, Precisión de Entrenamiento: 0.6172\n",
      "Iteración 33615 - Lote 175/352 - Pérdida de Entrenamiento: 1.0741, Precisión de Entrenamiento: 0.6166\n",
      "Iteración 33650 - Lote 210/352 - Pérdida de Entrenamiento: 1.0725, Precisión de Entrenamiento: 0.6173\n",
      "Iteración 33685 - Lote 245/352 - Pérdida de Entrenamiento: 1.0717, Precisión de Entrenamiento: 0.6181\n",
      "Iteración 33720 - Lote 280/352 - Pérdida de Entrenamiento: 1.0726, Precisión de Entrenamiento: 0.6193\n",
      "Iteración 33755 - Lote 315/352 - Pérdida de Entrenamiento: 1.0733, Precisión de Entrenamiento: 0.6187\n",
      "Iteración 33790 - Lote 350/352 - Pérdida de Entrenamiento: 1.0839, Precisión de Entrenamiento: 0.6153\n",
      "Val loss: 1.3533, Val acc: 0.5258\n",
      "0.1\n",
      "Epoch 97/400\n",
      "Iteración 33827 - Lote 35/352 - Pérdida de Entrenamiento: 1.1734, Precisión de Entrenamiento: 0.5781\n",
      "Iteración 33862 - Lote 70/352 - Pérdida de Entrenamiento: 1.1411, Precisión de Entrenamiento: 0.5871\n",
      "Iteración 33897 - Lote 105/352 - Pérdida de Entrenamiento: 1.1220, Precisión de Entrenamiento: 0.5943\n",
      "Iteración 33932 - Lote 140/352 - Pérdida de Entrenamiento: 1.1068, Precisión de Entrenamiento: 0.5996\n",
      "Iteración 33967 - Lote 175/352 - Pérdida de Entrenamiento: 1.0936, Precisión de Entrenamiento: 0.6034\n",
      "Iteración 34002 - Lote 210/352 - Pérdida de Entrenamiento: 1.0884, Precisión de Entrenamiento: 0.6065\n",
      "Iteración 34037 - Lote 245/352 - Pérdida de Entrenamiento: 1.0789, Precisión de Entrenamiento: 0.6102\n",
      "Iteración 34072 - Lote 280/352 - Pérdida de Entrenamiento: 1.0714, Precisión de Entrenamiento: 0.6128\n",
      "Iteración 34107 - Lote 315/352 - Pérdida de Entrenamiento: 1.0672, Precisión de Entrenamiento: 0.6141\n",
      "Iteración 34142 - Lote 350/352 - Pérdida de Entrenamiento: 1.0610, Precisión de Entrenamiento: 0.6165\n",
      "Val loss: 1.0753, Val acc: 0.6206\n",
      "0.1\n",
      "Epoch 98/400\n",
      "Iteración 34179 - Lote 35/352 - Pérdida de Entrenamiento: 1.0334, Precisión de Entrenamiento: 0.6295\n",
      "Iteración 34214 - Lote 70/352 - Pérdida de Entrenamiento: 1.0468, Precisión de Entrenamiento: 0.6262\n",
      "Iteración 34249 - Lote 105/352 - Pérdida de Entrenamiento: 1.0411, Precisión de Entrenamiento: 0.6268\n",
      "Iteración 34284 - Lote 140/352 - Pérdida de Entrenamiento: 1.0379, Precisión de Entrenamiento: 0.6285\n",
      "Iteración 34319 - Lote 175/352 - Pérdida de Entrenamiento: 1.0301, Precisión de Entrenamiento: 0.6303\n",
      "Iteración 34354 - Lote 210/352 - Pérdida de Entrenamiento: 1.0255, Precisión de Entrenamiento: 0.6317\n",
      "Iteración 34389 - Lote 245/352 - Pérdida de Entrenamiento: 1.0233, Precisión de Entrenamiento: 0.6338\n",
      "Iteración 34424 - Lote 280/352 - Pérdida de Entrenamiento: 1.0212, Precisión de Entrenamiento: 0.6347\n",
      "Iteración 34459 - Lote 315/352 - Pérdida de Entrenamiento: 1.0234, Precisión de Entrenamiento: 0.6335\n",
      "Iteración 34494 - Lote 350/352 - Pérdida de Entrenamiento: 1.0174, Precisión de Entrenamiento: 0.6357\n",
      "Val loss: 1.0220, Val acc: 0.6362\n",
      "0.1\n",
      "Epoch 99/400\n",
      "Iteración 34531 - Lote 35/352 - Pérdida de Entrenamiento: 1.0397, Precisión de Entrenamiento: 0.6261\n",
      "Iteración 34566 - Lote 70/352 - Pérdida de Entrenamiento: 1.0244, Precisión de Entrenamiento: 0.6319\n",
      "Iteración 34601 - Lote 105/352 - Pérdida de Entrenamiento: 1.0334, Precisión de Entrenamiento: 0.6262\n",
      "Iteración 34636 - Lote 140/352 - Pérdida de Entrenamiento: 1.0226, Precisión de Entrenamiento: 0.6324\n",
      "Iteración 34671 - Lote 175/352 - Pérdida de Entrenamiento: 1.0131, Precisión de Entrenamiento: 0.6367\n",
      "Iteración 34706 - Lote 210/352 - Pérdida de Entrenamiento: 1.0115, Precisión de Entrenamiento: 0.6364\n",
      "Iteración 34741 - Lote 245/352 - Pérdida de Entrenamiento: 1.0089, Precisión de Entrenamiento: 0.6379\n",
      "Iteración 34776 - Lote 280/352 - Pérdida de Entrenamiento: 1.0069, Precisión de Entrenamiento: 0.6384\n",
      "Iteración 34811 - Lote 315/352 - Pérdida de Entrenamiento: 1.0129, Precisión de Entrenamiento: 0.6366\n",
      "Iteración 34846 - Lote 350/352 - Pérdida de Entrenamiento: 1.0192, Precisión de Entrenamiento: 0.6343\n",
      "Val loss: 1.0233, Val acc: 0.6346\n",
      "0.1\n",
      "Epoch 100/400\n",
      "Iteración 34883 - Lote 35/352 - Pérdida de Entrenamiento: 0.9889, Precisión de Entrenamiento: 0.6440\n",
      "Iteración 34918 - Lote 70/352 - Pérdida de Entrenamiento: 0.9810, Precisión de Entrenamiento: 0.6474\n",
      "Iteración 34953 - Lote 105/352 - Pérdida de Entrenamiento: 0.9935, Precisión de Entrenamiento: 0.6461\n",
      "Iteración 34988 - Lote 140/352 - Pérdida de Entrenamiento: 1.0088, Precisión de Entrenamiento: 0.6398\n",
      "Iteración 35023 - Lote 175/352 - Pérdida de Entrenamiento: 1.0104, Precisión de Entrenamiento: 0.6392\n",
      "Iteración 35058 - Lote 210/352 - Pérdida de Entrenamiento: 1.0123, Precisión de Entrenamiento: 0.6384\n",
      "Iteración 35093 - Lote 245/352 - Pérdida de Entrenamiento: 1.0092, Precisión de Entrenamiento: 0.6392\n",
      "Iteración 35128 - Lote 280/352 - Pérdida de Entrenamiento: 1.0103, Precisión de Entrenamiento: 0.6384\n",
      "Iteración 35163 - Lote 315/352 - Pérdida de Entrenamiento: 1.0075, Precisión de Entrenamiento: 0.6399\n",
      "Iteración 35198 - Lote 350/352 - Pérdida de Entrenamiento: 1.0066, Precisión de Entrenamiento: 0.6406\n",
      "Val loss: 1.0088, Val acc: 0.6412\n",
      "Gradientes para features.0.0.weight: min=-0.03166365623474121, max=0.0026478515937924385, mean=-0.000908637885004282, std=0.004225156735628843\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.01673915423452854, max=0.011346252635121346, mean=-2.9125628770998446e-06, std=0.0010508905397728086\n",
      "Gradientes para classifier.1.weight: min=-0.011234890669584274, max=0.007159785367548466, mean=0.0, std=0.0014985759044066072\n",
      "0.1\n",
      "Epoch 101/400\n",
      "Iteración 35235 - Lote 35/352 - Pérdida de Entrenamiento: 0.9603, Precisión de Entrenamiento: 0.6471\n",
      "Iteración 35270 - Lote 70/352 - Pérdida de Entrenamiento: 0.9670, Precisión de Entrenamiento: 0.6475\n",
      "Iteración 35305 - Lote 105/352 - Pérdida de Entrenamiento: 0.9732, Precisión de Entrenamiento: 0.6464\n",
      "Iteración 35340 - Lote 140/352 - Pérdida de Entrenamiento: 0.9731, Precisión de Entrenamiento: 0.6477\n",
      "Iteración 35375 - Lote 175/352 - Pérdida de Entrenamiento: 0.9747, Precisión de Entrenamiento: 0.6484\n",
      "Iteración 35410 - Lote 210/352 - Pérdida de Entrenamiento: 0.9783, Precisión de Entrenamiento: 0.6471\n",
      "Iteración 35445 - Lote 245/352 - Pérdida de Entrenamiento: 0.9786, Precisión de Entrenamiento: 0.6466\n",
      "Iteración 35480 - Lote 280/352 - Pérdida de Entrenamiento: 0.9863, Precisión de Entrenamiento: 0.6439\n",
      "Iteración 35515 - Lote 315/352 - Pérdida de Entrenamiento: 1.0207, Precisión de Entrenamiento: 0.6331\n",
      "Iteración 35550 - Lote 350/352 - Pérdida de Entrenamiento: 1.0360, Precisión de Entrenamiento: 0.6271\n",
      "Val loss: 1.0696, Val acc: 0.6144\n",
      "0.1\n",
      "Epoch 102/400\n",
      "Iteración 35587 - Lote 35/352 - Pérdida de Entrenamiento: 1.0955, Precisión de Entrenamiento: 0.6051\n",
      "Iteración 35622 - Lote 70/352 - Pérdida de Entrenamiento: 1.0722, Precisión de Entrenamiento: 0.6156\n",
      "Iteración 35657 - Lote 105/352 - Pérdida de Entrenamiento: 1.0449, Precisión de Entrenamiento: 0.6255\n",
      "Iteración 35692 - Lote 140/352 - Pérdida de Entrenamiento: 1.0312, Precisión de Entrenamiento: 0.6295\n",
      "Iteración 35727 - Lote 175/352 - Pérdida de Entrenamiento: 1.0215, Precisión de Entrenamiento: 0.6343\n",
      "Iteración 35762 - Lote 210/352 - Pérdida de Entrenamiento: 1.0171, Precisión de Entrenamiento: 0.6359\n",
      "Iteración 35797 - Lote 245/352 - Pérdida de Entrenamiento: 1.0113, Precisión de Entrenamiento: 0.6373\n",
      "Iteración 35832 - Lote 280/352 - Pérdida de Entrenamiento: 1.0092, Precisión de Entrenamiento: 0.6383\n",
      "Iteración 35867 - Lote 315/352 - Pérdida de Entrenamiento: 1.0054, Precisión de Entrenamiento: 0.6406\n",
      "Iteración 35902 - Lote 350/352 - Pérdida de Entrenamiento: 1.0037, Precisión de Entrenamiento: 0.6409\n",
      "Val loss: 0.9987, Val acc: 0.6444\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_102.pth\n",
      "Checkpoint del mejor modelo guardado en la época 102\n",
      "0.1\n",
      "Epoch 103/400\n",
      "Iteración 35939 - Lote 35/352 - Pérdida de Entrenamiento: 0.9370, Precisión de Entrenamiento: 0.6647\n",
      "Iteración 35974 - Lote 70/352 - Pérdida de Entrenamiento: 0.9580, Precisión de Entrenamiento: 0.6593\n",
      "Iteración 36009 - Lote 105/352 - Pérdida de Entrenamiento: 0.9631, Precisión de Entrenamiento: 0.6545\n",
      "Iteración 36044 - Lote 140/352 - Pérdida de Entrenamiento: 0.9649, Precisión de Entrenamiento: 0.6550\n",
      "Iteración 36079 - Lote 175/352 - Pérdida de Entrenamiento: 0.9643, Precisión de Entrenamiento: 0.6538\n",
      "Iteración 36114 - Lote 210/352 - Pérdida de Entrenamiento: 0.9648, Precisión de Entrenamiento: 0.6527\n",
      "Iteración 36149 - Lote 245/352 - Pérdida de Entrenamiento: 0.9668, Precisión de Entrenamiento: 0.6528\n",
      "Iteración 36184 - Lote 280/352 - Pérdida de Entrenamiento: 0.9695, Precisión de Entrenamiento: 0.6520\n",
      "Iteración 36219 - Lote 315/352 - Pérdida de Entrenamiento: 0.9709, Precisión de Entrenamiento: 0.6520\n",
      "Iteración 36254 - Lote 350/352 - Pérdida de Entrenamiento: 0.9719, Precisión de Entrenamiento: 0.6520\n",
      "Val loss: 0.9922, Val acc: 0.6462\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_103.pth\n",
      "Checkpoint del mejor modelo guardado en la época 103\n",
      "0.1\n",
      "Epoch 104/400\n",
      "Iteración 36291 - Lote 35/352 - Pérdida de Entrenamiento: 0.9475, Precisión de Entrenamiento: 0.6529\n",
      "Iteración 36326 - Lote 70/352 - Pérdida de Entrenamiento: 0.9445, Precisión de Entrenamiento: 0.6609\n",
      "Iteración 36361 - Lote 105/352 - Pérdida de Entrenamiento: 0.9485, Precisión de Entrenamiento: 0.6590\n",
      "Iteración 36396 - Lote 140/352 - Pérdida de Entrenamiento: 0.9518, Precisión de Entrenamiento: 0.6561\n",
      "Iteración 36431 - Lote 175/352 - Pérdida de Entrenamiento: 0.9626, Precisión de Entrenamiento: 0.6526\n",
      "Iteración 36466 - Lote 210/352 - Pérdida de Entrenamiento: 0.9717, Precisión de Entrenamiento: 0.6508\n",
      "Iteración 36501 - Lote 245/352 - Pérdida de Entrenamiento: 0.9743, Precisión de Entrenamiento: 0.6503\n",
      "Iteración 36536 - Lote 280/352 - Pérdida de Entrenamiento: 0.9756, Precisión de Entrenamiento: 0.6494\n",
      "Iteración 36571 - Lote 315/352 - Pérdida de Entrenamiento: 0.9750, Precisión de Entrenamiento: 0.6481\n",
      "Iteración 36606 - Lote 350/352 - Pérdida de Entrenamiento: 0.9754, Precisión de Entrenamiento: 0.6482\n",
      "Val loss: 1.0116, Val acc: 0.6396\n",
      "0.1\n",
      "Epoch 105/400\n",
      "Iteración 36643 - Lote 35/352 - Pérdida de Entrenamiento: 0.9894, Precisión de Entrenamiento: 0.6402\n",
      "Iteración 36678 - Lote 70/352 - Pérdida de Entrenamiento: 0.9823, Precisión de Entrenamiento: 0.6471\n",
      "Iteración 36713 - Lote 105/352 - Pérdida de Entrenamiento: 0.9922, Precisión de Entrenamiento: 0.6423\n",
      "Iteración 36748 - Lote 140/352 - Pérdida de Entrenamiento: 0.9984, Precisión de Entrenamiento: 0.6411\n",
      "Iteración 36783 - Lote 175/352 - Pérdida de Entrenamiento: 0.9963, Precisión de Entrenamiento: 0.6407\n",
      "Iteración 36818 - Lote 210/352 - Pérdida de Entrenamiento: 0.9945, Precisión de Entrenamiento: 0.6408\n",
      "Iteración 36853 - Lote 245/352 - Pérdida de Entrenamiento: 0.9897, Precisión de Entrenamiento: 0.6425\n",
      "Iteración 36888 - Lote 280/352 - Pérdida de Entrenamiento: 0.9857, Precisión de Entrenamiento: 0.6442\n",
      "Iteración 36923 - Lote 315/352 - Pérdida de Entrenamiento: 0.9799, Precisión de Entrenamiento: 0.6460\n",
      "Iteración 36958 - Lote 350/352 - Pérdida de Entrenamiento: 0.9798, Precisión de Entrenamiento: 0.6471\n",
      "Val loss: 0.9875, Val acc: 0.6528\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_105.pth\n",
      "Checkpoint del mejor modelo guardado en la época 105\n",
      "Gradientes para features.0.0.weight: min=-0.016727864742279053, max=0.02323581837117672, mean=0.00025899853790178895, std=0.0027374981436878443\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.01775747537612915, max=0.016440344974398613, mean=1.137346316681942e-05, std=0.0011593397939577699\n",
      "Gradientes para classifier.1.weight: min=-0.011347835883498192, max=0.011415086686611176, mean=-1.8626451075975936e-11, std=0.001877294504083693\n",
      "0.1\n",
      "Epoch 106/400\n",
      "Iteración 36995 - Lote 35/352 - Pérdida de Entrenamiento: 0.9461, Precisión de Entrenamiento: 0.6598\n",
      "Iteración 37030 - Lote 70/352 - Pérdida de Entrenamiento: 0.9570, Precisión de Entrenamiento: 0.6504\n",
      "Iteración 37065 - Lote 105/352 - Pérdida de Entrenamiento: 0.9601, Precisión de Entrenamiento: 0.6501\n",
      "Iteración 37100 - Lote 140/352 - Pérdida de Entrenamiento: 0.9559, Precisión de Entrenamiento: 0.6541\n",
      "Iteración 37135 - Lote 175/352 - Pérdida de Entrenamiento: 0.9584, Precisión de Entrenamiento: 0.6540\n",
      "Iteración 37170 - Lote 210/352 - Pérdida de Entrenamiento: 0.9566, Precisión de Entrenamiento: 0.6551\n",
      "Iteración 37205 - Lote 245/352 - Pérdida de Entrenamiento: 0.9611, Precisión de Entrenamiento: 0.6528\n",
      "Iteración 37240 - Lote 280/352 - Pérdida de Entrenamiento: 0.9639, Precisión de Entrenamiento: 0.6526\n",
      "Iteración 37275 - Lote 315/352 - Pérdida de Entrenamiento: 0.9605, Precisión de Entrenamiento: 0.6546\n",
      "Iteración 37310 - Lote 350/352 - Pérdida de Entrenamiento: 0.9622, Precisión de Entrenamiento: 0.6545\n",
      "Val loss: 0.9762, Val acc: 0.6524\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_106.pth\n",
      "Checkpoint del mejor modelo guardado en la época 106\n",
      "0.1\n",
      "Epoch 107/400\n",
      "Iteración 37347 - Lote 35/352 - Pérdida de Entrenamiento: 0.9448, Precisión de Entrenamiento: 0.6578\n",
      "Iteración 37382 - Lote 70/352 - Pérdida de Entrenamiento: 0.9354, Precisión de Entrenamiento: 0.6597\n",
      "Iteración 37417 - Lote 105/352 - Pérdida de Entrenamiento: 0.9527, Precisión de Entrenamiento: 0.6542\n",
      "Iteración 37452 - Lote 140/352 - Pérdida de Entrenamiento: 0.9588, Precisión de Entrenamiento: 0.6521\n",
      "Iteración 37487 - Lote 175/352 - Pérdida de Entrenamiento: 0.9593, Precisión de Entrenamiento: 0.6528\n",
      "Iteración 37522 - Lote 210/352 - Pérdida de Entrenamiento: 0.9515, Precisión de Entrenamiento: 0.6573\n",
      "Iteración 37557 - Lote 245/352 - Pérdida de Entrenamiento: 0.9588, Precisión de Entrenamiento: 0.6549\n",
      "Iteración 37592 - Lote 280/352 - Pérdida de Entrenamiento: 0.9669, Precisión de Entrenamiento: 0.6508\n",
      "Iteración 37627 - Lote 315/352 - Pérdida de Entrenamiento: 0.9687, Precisión de Entrenamiento: 0.6504\n",
      "Iteración 37662 - Lote 350/352 - Pérdida de Entrenamiento: 0.9732, Precisión de Entrenamiento: 0.6489\n",
      "Val loss: 1.0263, Val acc: 0.6322\n",
      "0.1\n",
      "Epoch 108/400\n",
      "Iteración 37699 - Lote 35/352 - Pérdida de Entrenamiento: 1.0024, Precisión de Entrenamiento: 0.6404\n",
      "Iteración 37734 - Lote 70/352 - Pérdida de Entrenamiento: 0.9793, Precisión de Entrenamiento: 0.6509\n",
      "Iteración 37769 - Lote 105/352 - Pérdida de Entrenamiento: 0.9649, Precisión de Entrenamiento: 0.6546\n",
      "Iteración 37804 - Lote 140/352 - Pérdida de Entrenamiento: 0.9608, Precisión de Entrenamiento: 0.6574\n",
      "Iteración 37839 - Lote 175/352 - Pérdida de Entrenamiento: 0.9598, Precisión de Entrenamiento: 0.6579\n",
      "Iteración 37874 - Lote 210/352 - Pérdida de Entrenamiento: 0.9600, Precisión de Entrenamiento: 0.6565\n",
      "Iteración 37909 - Lote 245/352 - Pérdida de Entrenamiento: 0.9574, Precisión de Entrenamiento: 0.6577\n",
      "Iteración 37944 - Lote 280/352 - Pérdida de Entrenamiento: 0.9557, Precisión de Entrenamiento: 0.6578\n",
      "Iteración 37979 - Lote 315/352 - Pérdida de Entrenamiento: 0.9561, Precisión de Entrenamiento: 0.6573\n",
      "Iteración 38014 - Lote 350/352 - Pérdida de Entrenamiento: 0.9577, Precisión de Entrenamiento: 0.6561\n",
      "Val loss: 0.9537, Val acc: 0.6620\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_108.pth\n",
      "Checkpoint del mejor modelo guardado en la época 108\n",
      "0.1\n",
      "Epoch 109/400\n",
      "Iteración 38051 - Lote 35/352 - Pérdida de Entrenamiento: 0.9493, Precisión de Entrenamiento: 0.6580\n",
      "Iteración 38086 - Lote 70/352 - Pérdida de Entrenamiento: 0.9428, Precisión de Entrenamiento: 0.6616\n",
      "Iteración 38121 - Lote 105/352 - Pérdida de Entrenamiento: 0.9510, Precisión de Entrenamiento: 0.6619\n",
      "Iteración 38156 - Lote 140/352 - Pérdida de Entrenamiento: 0.9516, Precisión de Entrenamiento: 0.6605\n",
      "Iteración 38191 - Lote 175/352 - Pérdida de Entrenamiento: 0.9541, Precisión de Entrenamiento: 0.6580\n",
      "Iteración 38226 - Lote 210/352 - Pérdida de Entrenamiento: 0.9508, Precisión de Entrenamiento: 0.6605\n",
      "Iteración 38261 - Lote 245/352 - Pérdida de Entrenamiento: 0.9542, Precisión de Entrenamiento: 0.6589\n",
      "Iteración 38296 - Lote 280/352 - Pérdida de Entrenamiento: 0.9556, Precisión de Entrenamiento: 0.6583\n",
      "Iteración 38331 - Lote 315/352 - Pérdida de Entrenamiento: 0.9558, Precisión de Entrenamiento: 0.6579\n",
      "Iteración 38366 - Lote 350/352 - Pérdida de Entrenamiento: 0.9580, Precisión de Entrenamiento: 0.6567\n",
      "Val loss: 0.9762, Val acc: 0.6512\n",
      "0.1\n",
      "Epoch 110/400\n",
      "Iteración 38403 - Lote 35/352 - Pérdida de Entrenamiento: 0.9225, Precisión de Entrenamiento: 0.6672\n",
      "Iteración 38438 - Lote 70/352 - Pérdida de Entrenamiento: 0.9080, Precisión de Entrenamiento: 0.6724\n",
      "Iteración 38473 - Lote 105/352 - Pérdida de Entrenamiento: 0.9147, Precisión de Entrenamiento: 0.6708\n",
      "Iteración 38508 - Lote 140/352 - Pérdida de Entrenamiento: 0.9214, Precisión de Entrenamiento: 0.6695\n",
      "Iteración 38543 - Lote 175/352 - Pérdida de Entrenamiento: 0.9243, Precisión de Entrenamiento: 0.6685\n",
      "Iteración 38578 - Lote 210/352 - Pérdida de Entrenamiento: 0.9275, Precisión de Entrenamiento: 0.6680\n",
      "Iteración 38613 - Lote 245/352 - Pérdida de Entrenamiento: 0.9327, Precisión de Entrenamiento: 0.6655\n",
      "Iteración 38648 - Lote 280/352 - Pérdida de Entrenamiento: 0.9404, Precisión de Entrenamiento: 0.6628\n",
      "Iteración 38683 - Lote 315/352 - Pérdida de Entrenamiento: 0.9529, Precisión de Entrenamiento: 0.6582\n",
      "Iteración 38718 - Lote 350/352 - Pérdida de Entrenamiento: 0.9570, Precisión de Entrenamiento: 0.6569\n",
      "Val loss: 0.9699, Val acc: 0.6552\n",
      "Gradientes para features.0.0.weight: min=-0.01084577664732933, max=0.003117552725598216, mean=-0.0003216825134586543, std=0.0015541833126917481\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.00843204278498888, max=0.007751070894300938, mean=9.507079425929987e-07, std=0.0007928406121209264\n",
      "Gradientes para classifier.1.weight: min=-0.00761896837502718, max=0.007821989245712757, mean=-2.328306384496992e-12, std=0.0012651091674342752\n",
      "0.1\n",
      "Epoch 111/400\n",
      "Iteración 38755 - Lote 35/352 - Pérdida de Entrenamiento: 0.9637, Precisión de Entrenamiento: 0.6598\n",
      "Iteración 38790 - Lote 70/352 - Pérdida de Entrenamiento: 0.9579, Precisión de Entrenamiento: 0.6594\n",
      "Iteración 38825 - Lote 105/352 - Pérdida de Entrenamiento: 0.9658, Precisión de Entrenamiento: 0.6571\n",
      "Iteración 38860 - Lote 140/352 - Pérdida de Entrenamiento: 0.9692, Precisión de Entrenamiento: 0.6533\n",
      "Iteración 38895 - Lote 175/352 - Pérdida de Entrenamiento: 0.9728, Precisión de Entrenamiento: 0.6514\n",
      "Iteración 38930 - Lote 210/352 - Pérdida de Entrenamiento: 0.9696, Precisión de Entrenamiento: 0.6528\n",
      "Iteración 38965 - Lote 245/352 - Pérdida de Entrenamiento: 0.9680, Precisión de Entrenamiento: 0.6525\n",
      "Iteración 39000 - Lote 280/352 - Pérdida de Entrenamiento: 0.9610, Precisión de Entrenamiento: 0.6556\n",
      "Iteración 39035 - Lote 315/352 - Pérdida de Entrenamiento: 0.9618, Precisión de Entrenamiento: 0.6551\n",
      "Iteración 39070 - Lote 350/352 - Pérdida de Entrenamiento: 0.9608, Precisión de Entrenamiento: 0.6551\n",
      "Val loss: 0.9432, Val acc: 0.6578\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_111.pth\n",
      "Checkpoint del mejor modelo guardado en la época 111\n",
      "0.1\n",
      "Epoch 112/400\n",
      "Iteración 39107 - Lote 35/352 - Pérdida de Entrenamiento: 0.9003, Precisión de Entrenamiento: 0.6768\n",
      "Iteración 39142 - Lote 70/352 - Pérdida de Entrenamiento: 0.9304, Precisión de Entrenamiento: 0.6664\n",
      "Iteración 39177 - Lote 105/352 - Pérdida de Entrenamiento: 0.9373, Precisión de Entrenamiento: 0.6632\n",
      "Iteración 39212 - Lote 140/352 - Pérdida de Entrenamiento: 0.9374, Precisión de Entrenamiento: 0.6642\n",
      "Iteración 39247 - Lote 175/352 - Pérdida de Entrenamiento: 0.9401, Precisión de Entrenamiento: 0.6640\n",
      "Iteración 39282 - Lote 210/352 - Pérdida de Entrenamiento: 0.9370, Precisión de Entrenamiento: 0.6640\n",
      "Iteración 39317 - Lote 245/352 - Pérdida de Entrenamiento: 0.9369, Precisión de Entrenamiento: 0.6635\n",
      "Iteración 39352 - Lote 280/352 - Pérdida de Entrenamiento: 0.9345, Precisión de Entrenamiento: 0.6644\n",
      "Iteración 39387 - Lote 315/352 - Pérdida de Entrenamiento: 0.9340, Precisión de Entrenamiento: 0.6644\n",
      "Iteración 39422 - Lote 350/352 - Pérdida de Entrenamiento: 0.9375, Precisión de Entrenamiento: 0.6634\n",
      "Val loss: 0.9497, Val acc: 0.6606\n",
      "0.1\n",
      "Epoch 113/400\n",
      "Iteración 39459 - Lote 35/352 - Pérdida de Entrenamiento: 0.9157, Precisión de Entrenamiento: 0.6717\n",
      "Iteración 39494 - Lote 70/352 - Pérdida de Entrenamiento: 0.9198, Precisión de Entrenamiento: 0.6695\n",
      "Iteración 39529 - Lote 105/352 - Pérdida de Entrenamiento: 0.9215, Precisión de Entrenamiento: 0.6689\n",
      "Iteración 39564 - Lote 140/352 - Pérdida de Entrenamiento: 0.9295, Precisión de Entrenamiento: 0.6671\n",
      "Iteración 39599 - Lote 175/352 - Pérdida de Entrenamiento: 0.9228, Precisión de Entrenamiento: 0.6688\n",
      "Iteración 39634 - Lote 210/352 - Pérdida de Entrenamiento: 0.9229, Precisión de Entrenamiento: 0.6682\n",
      "Iteración 39669 - Lote 245/352 - Pérdida de Entrenamiento: 0.9237, Precisión de Entrenamiento: 0.6681\n",
      "Iteración 39704 - Lote 280/352 - Pérdida de Entrenamiento: 0.9222, Precisión de Entrenamiento: 0.6681\n",
      "Iteración 39739 - Lote 315/352 - Pérdida de Entrenamiento: 0.9248, Precisión de Entrenamiento: 0.6670\n",
      "Iteración 39774 - Lote 350/352 - Pérdida de Entrenamiento: 0.9240, Precisión de Entrenamiento: 0.6676\n",
      "Val loss: 0.9676, Val acc: 0.6600\n",
      "0.1\n",
      "Epoch 114/400\n",
      "Iteración 39811 - Lote 35/352 - Pérdida de Entrenamiento: 0.9135, Precisión de Entrenamiento: 0.6725\n",
      "Iteración 39846 - Lote 70/352 - Pérdida de Entrenamiento: 0.9086, Precisión de Entrenamiento: 0.6713\n",
      "Iteración 39881 - Lote 105/352 - Pérdida de Entrenamiento: 0.9096, Precisión de Entrenamiento: 0.6687\n",
      "Iteración 39916 - Lote 140/352 - Pérdida de Entrenamiento: 0.9046, Precisión de Entrenamiento: 0.6733\n",
      "Iteración 39951 - Lote 175/352 - Pérdida de Entrenamiento: 0.9235, Precisión de Entrenamiento: 0.6671\n",
      "Iteración 39986 - Lote 210/352 - Pérdida de Entrenamiento: 0.9294, Precisión de Entrenamiento: 0.6646\n",
      "Iteración 40021 - Lote 245/352 - Pérdida de Entrenamiento: 0.9265, Precisión de Entrenamiento: 0.6659\n",
      "Iteración 40056 - Lote 280/352 - Pérdida de Entrenamiento: 0.9256, Precisión de Entrenamiento: 0.6666\n",
      "Iteración 40091 - Lote 315/352 - Pérdida de Entrenamiento: 0.9204, Precisión de Entrenamiento: 0.6684\n",
      "Iteración 40126 - Lote 350/352 - Pérdida de Entrenamiento: 0.9194, Precisión de Entrenamiento: 0.6688\n",
      "Val loss: 0.9335, Val acc: 0.6686\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_114.pth\n",
      "Checkpoint del mejor modelo guardado en la época 114\n",
      "0.1\n",
      "Epoch 115/400\n",
      "Iteración 40163 - Lote 35/352 - Pérdida de Entrenamiento: 0.8970, Precisión de Entrenamiento: 0.6768\n",
      "Iteración 40198 - Lote 70/352 - Pérdida de Entrenamiento: 0.8998, Precisión de Entrenamiento: 0.6740\n",
      "Iteración 40233 - Lote 105/352 - Pérdida de Entrenamiento: 0.9023, Precisión de Entrenamiento: 0.6743\n",
      "Iteración 40268 - Lote 140/352 - Pérdida de Entrenamiento: 0.9039, Precisión de Entrenamiento: 0.6728\n",
      "Iteración 40303 - Lote 175/352 - Pérdida de Entrenamiento: 0.9076, Precisión de Entrenamiento: 0.6734\n",
      "Iteración 40338 - Lote 210/352 - Pérdida de Entrenamiento: 0.9031, Precisión de Entrenamiento: 0.6736\n",
      "Iteración 40373 - Lote 245/352 - Pérdida de Entrenamiento: 0.9029, Precisión de Entrenamiento: 0.6739\n",
      "Iteración 40408 - Lote 280/352 - Pérdida de Entrenamiento: 0.9023, Precisión de Entrenamiento: 0.6748\n",
      "Iteración 40443 - Lote 315/352 - Pérdida de Entrenamiento: 0.9089, Precisión de Entrenamiento: 0.6729\n",
      "Iteración 40478 - Lote 350/352 - Pérdida de Entrenamiento: 0.9129, Precisión de Entrenamiento: 0.6711\n",
      "Val loss: 0.9510, Val acc: 0.6618\n",
      "Gradientes para features.0.0.weight: min=-0.035141512751579285, max=0.0027619618922472, mean=-0.0008183716563507915, std=0.0043406737968325615\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.010183409787714481, max=0.011554433032870293, mean=6.146452051325468e-06, std=0.0009852717630565166\n",
      "Gradientes para classifier.1.weight: min=-0.008404266089200974, max=0.010934866033494473, mean=4.656612768993984e-12, std=0.0013735981192439795\n",
      "0.1\n",
      "Epoch 116/400\n",
      "Iteración 40515 - Lote 35/352 - Pérdida de Entrenamiento: 0.8929, Precisión de Entrenamiento: 0.6833\n",
      "Iteración 40550 - Lote 70/352 - Pérdida de Entrenamiento: 0.8981, Precisión de Entrenamiento: 0.6787\n",
      "Iteración 40585 - Lote 105/352 - Pérdida de Entrenamiento: 0.8972, Precisión de Entrenamiento: 0.6796\n",
      "Iteración 40620 - Lote 140/352 - Pérdida de Entrenamiento: 0.8934, Precisión de Entrenamiento: 0.6807\n",
      "Iteración 40655 - Lote 175/352 - Pérdida de Entrenamiento: 0.8987, Precisión de Entrenamiento: 0.6799\n",
      "Iteración 40690 - Lote 210/352 - Pérdida de Entrenamiento: 0.9106, Precisión de Entrenamiento: 0.6756\n",
      "Iteración 40725 - Lote 245/352 - Pérdida de Entrenamiento: 0.9159, Precisión de Entrenamiento: 0.6734\n",
      "Iteración 40760 - Lote 280/352 - Pérdida de Entrenamiento: 0.9184, Precisión de Entrenamiento: 0.6714\n",
      "Iteración 40795 - Lote 315/352 - Pérdida de Entrenamiento: 0.9205, Precisión de Entrenamiento: 0.6702\n",
      "Iteración 40830 - Lote 350/352 - Pérdida de Entrenamiento: 0.9219, Precisión de Entrenamiento: 0.6695\n",
      "Val loss: 0.9493, Val acc: 0.6672\n",
      "0.1\n",
      "Epoch 117/400\n",
      "Iteración 40867 - Lote 35/352 - Pérdida de Entrenamiento: 0.8774, Precisión de Entrenamiento: 0.6879\n",
      "Iteración 40902 - Lote 70/352 - Pérdida de Entrenamiento: 0.8930, Precisión de Entrenamiento: 0.6777\n",
      "Iteración 40937 - Lote 105/352 - Pérdida de Entrenamiento: 0.8893, Precisión de Entrenamiento: 0.6790\n",
      "Iteración 40972 - Lote 140/352 - Pérdida de Entrenamiento: 0.8944, Precisión de Entrenamiento: 0.6761\n",
      "Iteración 41007 - Lote 175/352 - Pérdida de Entrenamiento: 0.9035, Precisión de Entrenamiento: 0.6720\n",
      "Iteración 41042 - Lote 210/352 - Pérdida de Entrenamiento: 0.9064, Precisión de Entrenamiento: 0.6727\n",
      "Iteración 41077 - Lote 245/352 - Pérdida de Entrenamiento: 0.9085, Precisión de Entrenamiento: 0.6721\n",
      "Iteración 41112 - Lote 280/352 - Pérdida de Entrenamiento: 0.9113, Precisión de Entrenamiento: 0.6714\n",
      "Iteración 41147 - Lote 315/352 - Pérdida de Entrenamiento: 0.9120, Precisión de Entrenamiento: 0.6710\n",
      "Iteración 41182 - Lote 350/352 - Pérdida de Entrenamiento: 0.9147, Precisión de Entrenamiento: 0.6706\n",
      "Val loss: 0.9653, Val acc: 0.6626\n",
      "0.1\n",
      "Epoch 118/400\n",
      "Iteración 41219 - Lote 35/352 - Pérdida de Entrenamiento: 0.9059, Precisión de Entrenamiento: 0.6779\n",
      "Iteración 41254 - Lote 70/352 - Pérdida de Entrenamiento: 0.9013, Precisión de Entrenamiento: 0.6789\n",
      "Iteración 41289 - Lote 105/352 - Pérdida de Entrenamiento: 0.8987, Precisión de Entrenamiento: 0.6780\n",
      "Iteración 41324 - Lote 140/352 - Pérdida de Entrenamiento: 0.8904, Precisión de Entrenamiento: 0.6799\n",
      "Iteración 41359 - Lote 175/352 - Pérdida de Entrenamiento: 0.8969, Precisión de Entrenamiento: 0.6778\n",
      "Iteración 41394 - Lote 210/352 - Pérdida de Entrenamiento: 0.8978, Precisión de Entrenamiento: 0.6774\n",
      "Iteración 41429 - Lote 245/352 - Pérdida de Entrenamiento: 0.8974, Precisión de Entrenamiento: 0.6771\n",
      "Iteración 41464 - Lote 280/352 - Pérdida de Entrenamiento: 0.8986, Precisión de Entrenamiento: 0.6771\n",
      "Iteración 41499 - Lote 315/352 - Pérdida de Entrenamiento: 0.8991, Precisión de Entrenamiento: 0.6768\n",
      "Iteración 41534 - Lote 350/352 - Pérdida de Entrenamiento: 0.8982, Precisión de Entrenamiento: 0.6766\n",
      "Val loss: 0.9437, Val acc: 0.6692\n",
      "0.1\n",
      "Epoch 119/400\n",
      "Iteración 41571 - Lote 35/352 - Pérdida de Entrenamiento: 0.9008, Precisión de Entrenamiento: 0.6837\n",
      "Iteración 41606 - Lote 70/352 - Pérdida de Entrenamiento: 0.8856, Precisión de Entrenamiento: 0.6883\n",
      "Iteración 41641 - Lote 105/352 - Pérdida de Entrenamiento: 0.8782, Precisión de Entrenamiento: 0.6925\n",
      "Iteración 41676 - Lote 140/352 - Pérdida de Entrenamiento: 0.8846, Precisión de Entrenamiento: 0.6874\n",
      "Iteración 41711 - Lote 175/352 - Pérdida de Entrenamiento: 0.8837, Precisión de Entrenamiento: 0.6872\n",
      "Iteración 41746 - Lote 210/352 - Pérdida de Entrenamiento: 0.8794, Precisión de Entrenamiento: 0.6871\n",
      "Iteración 41781 - Lote 245/352 - Pérdida de Entrenamiento: 0.8788, Precisión de Entrenamiento: 0.6871\n",
      "Iteración 41816 - Lote 280/352 - Pérdida de Entrenamiento: 0.8788, Precisión de Entrenamiento: 0.6880\n",
      "Iteración 41851 - Lote 315/352 - Pérdida de Entrenamiento: 0.8810, Precisión de Entrenamiento: 0.6860\n",
      "Iteración 41886 - Lote 350/352 - Pérdida de Entrenamiento: 0.8769, Precisión de Entrenamiento: 0.6872\n",
      "Val loss: 0.9692, Val acc: 0.6582\n",
      "0.1\n",
      "Epoch 120/400\n",
      "Iteración 41923 - Lote 35/352 - Pérdida de Entrenamiento: 0.8977, Precisión de Entrenamiento: 0.6790\n",
      "Iteración 41958 - Lote 70/352 - Pérdida de Entrenamiento: 0.8789, Precisión de Entrenamiento: 0.6875\n",
      "Iteración 41993 - Lote 105/352 - Pérdida de Entrenamiento: 0.8650, Precisión de Entrenamiento: 0.6906\n",
      "Iteración 42028 - Lote 140/352 - Pérdida de Entrenamiento: 0.8650, Precisión de Entrenamiento: 0.6907\n",
      "Iteración 42063 - Lote 175/352 - Pérdida de Entrenamiento: 0.8655, Precisión de Entrenamiento: 0.6903\n",
      "Iteración 42098 - Lote 210/352 - Pérdida de Entrenamiento: 0.8679, Precisión de Entrenamiento: 0.6893\n",
      "Iteración 42133 - Lote 245/352 - Pérdida de Entrenamiento: 0.8719, Precisión de Entrenamiento: 0.6873\n",
      "Iteración 42168 - Lote 280/352 - Pérdida de Entrenamiento: 0.8768, Precisión de Entrenamiento: 0.6859\n",
      "Iteración 42203 - Lote 315/352 - Pérdida de Entrenamiento: 0.8771, Precisión de Entrenamiento: 0.6851\n",
      "Iteración 42238 - Lote 350/352 - Pérdida de Entrenamiento: 0.8766, Precisión de Entrenamiento: 0.6857\n",
      "Val loss: 0.9385, Val acc: 0.6656\n",
      "Gradientes para features.0.0.weight: min=-0.0049196756444871426, max=0.015048549510538578, mean=0.0003185509121976793, std=0.001598755014128983\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.00917314924299717, max=0.014885292388498783, mean=5.688564215233782e-06, std=0.0010140478843823075\n",
      "Gradientes para classifier.1.weight: min=-0.010041273199021816, max=0.01156474556773901, mean=-4.656612768993984e-12, std=0.0016199833480641246\n",
      "0.1\n",
      "Epoch 121/400\n",
      "Iteración 42275 - Lote 35/352 - Pérdida de Entrenamiento: 0.8255, Precisión de Entrenamiento: 0.7085\n",
      "Iteración 42310 - Lote 70/352 - Pérdida de Entrenamiento: 0.8348, Precisión de Entrenamiento: 0.7056\n",
      "Iteración 42345 - Lote 105/352 - Pérdida de Entrenamiento: 0.8442, Precisión de Entrenamiento: 0.7003\n",
      "Iteración 42380 - Lote 140/352 - Pérdida de Entrenamiento: 0.8455, Precisión de Entrenamiento: 0.6994\n",
      "Iteración 42415 - Lote 175/352 - Pérdida de Entrenamiento: 0.8520, Precisión de Entrenamiento: 0.6961\n",
      "Iteración 42450 - Lote 210/352 - Pérdida de Entrenamiento: 0.8512, Precisión de Entrenamiento: 0.6966\n",
      "Iteración 42485 - Lote 245/352 - Pérdida de Entrenamiento: 0.8562, Precisión de Entrenamiento: 0.6941\n",
      "Iteración 42520 - Lote 280/352 - Pérdida de Entrenamiento: 0.8578, Precisión de Entrenamiento: 0.6927\n",
      "Iteración 42555 - Lote 315/352 - Pérdida de Entrenamiento: 0.8582, Precisión de Entrenamiento: 0.6930\n",
      "Iteración 42590 - Lote 350/352 - Pérdida de Entrenamiento: 0.8675, Precisión de Entrenamiento: 0.6897\n",
      "Val loss: 0.9779, Val acc: 0.6606\n",
      "0.1\n",
      "Epoch 122/400\n",
      "Iteración 42627 - Lote 35/352 - Pérdida de Entrenamiento: 0.8604, Precisión de Entrenamiento: 0.6897\n",
      "Iteración 42662 - Lote 70/352 - Pérdida de Entrenamiento: 0.8629, Precisión de Entrenamiento: 0.6905\n",
      "Iteración 42697 - Lote 105/352 - Pérdida de Entrenamiento: 0.9410, Precisión de Entrenamiento: 0.6653\n",
      "Iteración 42732 - Lote 140/352 - Pérdida de Entrenamiento: 0.9568, Precisión de Entrenamiento: 0.6584\n",
      "Iteración 42767 - Lote 175/352 - Pérdida de Entrenamiento: 0.9496, Precisión de Entrenamiento: 0.6612\n",
      "Iteración 42802 - Lote 210/352 - Pérdida de Entrenamiento: 0.9409, Precisión de Entrenamiento: 0.6632\n",
      "Iteración 42837 - Lote 245/352 - Pérdida de Entrenamiento: 0.9429, Precisión de Entrenamiento: 0.6628\n",
      "Iteración 42872 - Lote 280/352 - Pérdida de Entrenamiento: 0.9463, Precisión de Entrenamiento: 0.6617\n",
      "Iteración 42907 - Lote 315/352 - Pérdida de Entrenamiento: 0.9429, Precisión de Entrenamiento: 0.6629\n",
      "Iteración 42942 - Lote 350/352 - Pérdida de Entrenamiento: 0.9375, Precisión de Entrenamiento: 0.6651\n",
      "Val loss: 0.9710, Val acc: 0.6612\n",
      "0.1\n",
      "Epoch 123/400\n",
      "Iteración 42979 - Lote 35/352 - Pérdida de Entrenamiento: 0.8821, Precisión de Entrenamiento: 0.6846\n",
      "Iteración 43014 - Lote 70/352 - Pérdida de Entrenamiento: 0.8925, Precisión de Entrenamiento: 0.6820\n",
      "Iteración 43049 - Lote 105/352 - Pérdida de Entrenamiento: 0.8962, Precisión de Entrenamiento: 0.6802\n",
      "Iteración 43084 - Lote 140/352 - Pérdida de Entrenamiento: 0.9498, Precisión de Entrenamiento: 0.6631\n",
      "Iteración 43119 - Lote 175/352 - Pérdida de Entrenamiento: 0.9870, Precisión de Entrenamiento: 0.6492\n",
      "Iteración 43154 - Lote 210/352 - Pérdida de Entrenamiento: 0.9961, Precisión de Entrenamiento: 0.6457\n",
      "Iteración 43189 - Lote 245/352 - Pérdida de Entrenamiento: 0.9924, Precisión de Entrenamiento: 0.6458\n",
      "Iteración 43224 - Lote 280/352 - Pérdida de Entrenamiento: 0.9858, Precisión de Entrenamiento: 0.6474\n",
      "Iteración 43259 - Lote 315/352 - Pérdida de Entrenamiento: 0.9930, Precisión de Entrenamiento: 0.6444\n",
      "Iteración 43294 - Lote 350/352 - Pérdida de Entrenamiento: 0.9961, Precisión de Entrenamiento: 0.6436\n",
      "Val loss: 1.0442, Val acc: 0.6268\n",
      "0.05\n",
      "Epoch 124/400\n",
      "Iteración 43331 - Lote 35/352 - Pérdida de Entrenamiento: 0.9839, Precisión de Entrenamiento: 0.6509\n",
      "Iteración 43366 - Lote 70/352 - Pérdida de Entrenamiento: 0.9552, Precisión de Entrenamiento: 0.6619\n",
      "Iteración 43401 - Lote 105/352 - Pérdida de Entrenamiento: 0.9502, Precisión de Entrenamiento: 0.6611\n",
      "Iteración 43436 - Lote 140/352 - Pérdida de Entrenamiento: 0.9431, Precisión de Entrenamiento: 0.6624\n",
      "Iteración 43471 - Lote 175/352 - Pérdida de Entrenamiento: 0.9310, Precisión de Entrenamiento: 0.6659\n",
      "Iteración 43506 - Lote 210/352 - Pérdida de Entrenamiento: 0.9230, Precisión de Entrenamiento: 0.6683\n",
      "Iteración 43541 - Lote 245/352 - Pérdida de Entrenamiento: 0.9158, Precisión de Entrenamiento: 0.6704\n",
      "Iteración 43576 - Lote 280/352 - Pérdida de Entrenamiento: 0.9095, Precisión de Entrenamiento: 0.6729\n",
      "Iteración 43611 - Lote 315/352 - Pérdida de Entrenamiento: 0.9271, Precisión de Entrenamiento: 0.6671\n",
      "Iteración 43646 - Lote 350/352 - Pérdida de Entrenamiento: 0.9371, Precisión de Entrenamiento: 0.6635\n",
      "Val loss: 1.0624, Val acc: 0.6174\n",
      "0.05\n",
      "Epoch 125/400\n",
      "Iteración 43683 - Lote 35/352 - Pérdida de Entrenamiento: 0.9911, Precisión de Entrenamiento: 0.6429\n",
      "Iteración 43718 - Lote 70/352 - Pérdida de Entrenamiento: 0.9844, Precisión de Entrenamiento: 0.6445\n",
      "Iteración 43753 - Lote 105/352 - Pérdida de Entrenamiento: 0.9725, Precisión de Entrenamiento: 0.6508\n",
      "Iteración 43788 - Lote 140/352 - Pérdida de Entrenamiento: 0.9661, Precisión de Entrenamiento: 0.6527\n",
      "Iteración 43823 - Lote 175/352 - Pérdida de Entrenamiento: 0.9521, Precisión de Entrenamiento: 0.6579\n",
      "Iteración 43858 - Lote 210/352 - Pérdida de Entrenamiento: 0.9450, Precisión de Entrenamiento: 0.6610\n",
      "Iteración 43893 - Lote 245/352 - Pérdida de Entrenamiento: 0.9374, Precisión de Entrenamiento: 0.6637\n",
      "Iteración 43928 - Lote 280/352 - Pérdida de Entrenamiento: 0.9305, Precisión de Entrenamiento: 0.6659\n",
      "Iteración 43963 - Lote 315/352 - Pérdida de Entrenamiento: 0.9237, Precisión de Entrenamiento: 0.6684\n",
      "Iteración 43998 - Lote 350/352 - Pérdida de Entrenamiento: 0.9200, Precisión de Entrenamiento: 0.6698\n",
      "Val loss: 0.9197, Val acc: 0.6644\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_125.pth\n",
      "Checkpoint del mejor modelo guardado en la época 125\n",
      "Gradientes para features.0.0.weight: min=-0.006057840306311846, max=0.0353989414870739, mean=0.0013167568249627948, std=0.005617151968181133\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.012158621102571487, max=0.0138900987803936, mean=-1.4692230934088002e-06, std=0.0011706773657351732\n",
      "Gradientes para classifier.1.weight: min=-0.00770048750564456, max=0.008952808566391468, mean=0.0, std=0.0015056580305099487\n",
      "0.05\n",
      "Epoch 126/400\n",
      "Iteración 44035 - Lote 35/352 - Pérdida de Entrenamiento: 0.8555, Precisión de Entrenamiento: 0.6933\n",
      "Iteración 44070 - Lote 70/352 - Pérdida de Entrenamiento: 0.8551, Precisión de Entrenamiento: 0.6939\n",
      "Iteración 44105 - Lote 105/352 - Pérdida de Entrenamiento: 0.8424, Precisión de Entrenamiento: 0.6972\n",
      "Iteración 44140 - Lote 140/352 - Pérdida de Entrenamiento: 0.8498, Precisión de Entrenamiento: 0.6943\n",
      "Iteración 44175 - Lote 175/352 - Pérdida de Entrenamiento: 0.8560, Precisión de Entrenamiento: 0.6921\n",
      "Iteración 44210 - Lote 210/352 - Pérdida de Entrenamiento: 0.8601, Precisión de Entrenamiento: 0.6906\n",
      "Iteración 44245 - Lote 245/352 - Pérdida de Entrenamiento: 0.8544, Precisión de Entrenamiento: 0.6919\n",
      "Iteración 44280 - Lote 280/352 - Pérdida de Entrenamiento: 0.8553, Precisión de Entrenamiento: 0.6921\n",
      "Iteración 44315 - Lote 315/352 - Pérdida de Entrenamiento: 0.8549, Precisión de Entrenamiento: 0.6923\n",
      "Iteración 44350 - Lote 350/352 - Pérdida de Entrenamiento: 0.8509, Precisión de Entrenamiento: 0.6936\n",
      "Val loss: 0.9229, Val acc: 0.6724\n",
      "0.05\n",
      "Epoch 127/400\n",
      "Iteración 44387 - Lote 35/352 - Pérdida de Entrenamiento: 0.8138, Precisión de Entrenamiento: 0.7083\n",
      "Iteración 44422 - Lote 70/352 - Pérdida de Entrenamiento: 0.8154, Precisión de Entrenamiento: 0.7038\n",
      "Iteración 44457 - Lote 105/352 - Pérdida de Entrenamiento: 0.8095, Precisión de Entrenamiento: 0.7075\n",
      "Iteración 44492 - Lote 140/352 - Pérdida de Entrenamiento: 0.8225, Precisión de Entrenamiento: 0.7038\n",
      "Iteración 44527 - Lote 175/352 - Pérdida de Entrenamiento: 0.8383, Precisión de Entrenamiento: 0.6987\n",
      "Iteración 44562 - Lote 210/352 - Pérdida de Entrenamiento: 0.8533, Precisión de Entrenamiento: 0.6953\n",
      "Iteración 44597 - Lote 245/352 - Pérdida de Entrenamiento: 0.8954, Precisión de Entrenamiento: 0.6816\n",
      "Iteración 44632 - Lote 280/352 - Pérdida de Entrenamiento: 0.9380, Precisión de Entrenamiento: 0.6671\n",
      "Iteración 44667 - Lote 315/352 - Pérdida de Entrenamiento: 0.9641, Precisión de Entrenamiento: 0.6575\n",
      "Iteración 44702 - Lote 350/352 - Pérdida de Entrenamiento: 0.9753, Precisión de Entrenamiento: 0.6532\n",
      "Val loss: 1.0349, Val acc: 0.6348\n",
      "0.05\n",
      "Epoch 128/400\n",
      "Iteración 44739 - Lote 35/352 - Pérdida de Entrenamiento: 1.1222, Precisión de Entrenamiento: 0.6038\n",
      "Iteración 44774 - Lote 70/352 - Pérdida de Entrenamiento: 1.0675, Precisión de Entrenamiento: 0.6160\n",
      "Iteración 44809 - Lote 105/352 - Pérdida de Entrenamiento: 1.0416, Precisión de Entrenamiento: 0.6217\n",
      "Iteración 44844 - Lote 140/352 - Pérdida de Entrenamiento: 1.0175, Precisión de Entrenamiento: 0.6298\n",
      "Iteración 44879 - Lote 175/352 - Pérdida de Entrenamiento: 1.0030, Precisión de Entrenamiento: 0.6361\n",
      "Iteración 44914 - Lote 210/352 - Pérdida de Entrenamiento: 0.9930, Precisión de Entrenamiento: 0.6399\n",
      "Iteración 44949 - Lote 245/352 - Pérdida de Entrenamiento: 0.9787, Precisión de Entrenamiento: 0.6453\n",
      "Iteración 44984 - Lote 280/352 - Pérdida de Entrenamiento: 0.9677, Precisión de Entrenamiento: 0.6490\n",
      "Iteración 45019 - Lote 315/352 - Pérdida de Entrenamiento: 0.9617, Precisión de Entrenamiento: 0.6515\n",
      "Iteración 45054 - Lote 350/352 - Pérdida de Entrenamiento: 0.9585, Precisión de Entrenamiento: 0.6531\n",
      "Val loss: 0.9516, Val acc: 0.6574\n",
      "0.05\n",
      "Epoch 129/400\n",
      "Iteración 45091 - Lote 35/352 - Pérdida de Entrenamiento: 0.8759, Precisión de Entrenamiento: 0.6875\n",
      "Iteración 45126 - Lote 70/352 - Pérdida de Entrenamiento: 0.8710, Precisión de Entrenamiento: 0.6924\n",
      "Iteración 45161 - Lote 105/352 - Pérdida de Entrenamiento: 0.8778, Precisión de Entrenamiento: 0.6867\n",
      "Iteración 45196 - Lote 140/352 - Pérdida de Entrenamiento: 0.8767, Precisión de Entrenamiento: 0.6855\n",
      "Iteración 45231 - Lote 175/352 - Pérdida de Entrenamiento: 0.8683, Precisión de Entrenamiento: 0.6875\n",
      "Iteración 45266 - Lote 210/352 - Pérdida de Entrenamiento: 0.8648, Precisión de Entrenamiento: 0.6881\n",
      "Iteración 45301 - Lote 245/352 - Pérdida de Entrenamiento: 0.8614, Precisión de Entrenamiento: 0.6891\n",
      "Iteración 45336 - Lote 280/352 - Pérdida de Entrenamiento: 0.8624, Precisión de Entrenamiento: 0.6890\n",
      "Iteración 45371 - Lote 315/352 - Pérdida de Entrenamiento: 0.8620, Precisión de Entrenamiento: 0.6891\n",
      "Iteración 45406 - Lote 350/352 - Pérdida de Entrenamiento: 0.8636, Precisión de Entrenamiento: 0.6885\n",
      "Val loss: 0.9019, Val acc: 0.6810\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_129.pth\n",
      "Checkpoint del mejor modelo guardado en la época 129\n",
      "0.05\n",
      "Epoch 130/400\n",
      "Iteración 45443 - Lote 35/352 - Pérdida de Entrenamiento: 0.8430, Precisión de Entrenamiento: 0.7045\n",
      "Iteración 45478 - Lote 70/352 - Pérdida de Entrenamiento: 0.8441, Precisión de Entrenamiento: 0.6982\n",
      "Iteración 45513 - Lote 105/352 - Pérdida de Entrenamiento: 0.8407, Precisión de Entrenamiento: 0.6991\n",
      "Iteración 45548 - Lote 140/352 - Pérdida de Entrenamiento: 0.8361, Precisión de Entrenamiento: 0.7007\n",
      "Iteración 45583 - Lote 175/352 - Pérdida de Entrenamiento: 0.8272, Precisión de Entrenamiento: 0.7028\n",
      "Iteración 45618 - Lote 210/352 - Pérdida de Entrenamiento: 0.8270, Precisión de Entrenamiento: 0.7026\n",
      "Iteración 45653 - Lote 245/352 - Pérdida de Entrenamiento: 0.8269, Precisión de Entrenamiento: 0.7022\n",
      "Iteración 45688 - Lote 280/352 - Pérdida de Entrenamiento: 0.8284, Precisión de Entrenamiento: 0.7016\n",
      "Iteración 45723 - Lote 315/352 - Pérdida de Entrenamiento: 0.8299, Precisión de Entrenamiento: 0.7014\n",
      "Iteración 45758 - Lote 350/352 - Pérdida de Entrenamiento: 0.8301, Precisión de Entrenamiento: 0.7007\n",
      "Val loss: 0.9106, Val acc: 0.6828\n",
      "Gradientes para features.0.0.weight: min=-0.0029201398137956858, max=0.02080879546701908, mean=0.0004948821733705699, std=0.0025054547004401684\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.01312002632766962, max=0.01184280775487423, mean=-3.6150838695903076e-06, std=0.001170021714642644\n",
      "Gradientes para classifier.1.weight: min=-0.0136970030143857, max=0.00926082581281662, mean=4.656612768993984e-12, std=0.0015524878399446607\n",
      "0.05\n",
      "Epoch 131/400\n",
      "Iteración 45795 - Lote 35/352 - Pérdida de Entrenamiento: 0.8142, Precisión de Entrenamiento: 0.7042\n",
      "Iteración 45830 - Lote 70/352 - Pérdida de Entrenamiento: 0.8217, Precisión de Entrenamiento: 0.7013\n",
      "Iteración 45865 - Lote 105/352 - Pérdida de Entrenamiento: 0.8654, Precisión de Entrenamiento: 0.6884\n",
      "Iteración 45900 - Lote 140/352 - Pérdida de Entrenamiento: 0.8704, Precisión de Entrenamiento: 0.6868\n",
      "Iteración 45935 - Lote 175/352 - Pérdida de Entrenamiento: 0.8664, Precisión de Entrenamiento: 0.6881\n",
      "Iteración 45970 - Lote 210/352 - Pérdida de Entrenamiento: 0.8666, Precisión de Entrenamiento: 0.6882\n",
      "Iteración 46005 - Lote 245/352 - Pérdida de Entrenamiento: 0.8644, Precisión de Entrenamiento: 0.6885\n",
      "Iteración 46040 - Lote 280/352 - Pérdida de Entrenamiento: 0.8619, Precisión de Entrenamiento: 0.6895\n",
      "Iteración 46075 - Lote 315/352 - Pérdida de Entrenamiento: 0.8584, Precisión de Entrenamiento: 0.6911\n",
      "Iteración 46110 - Lote 350/352 - Pérdida de Entrenamiento: 0.8565, Precisión de Entrenamiento: 0.6920\n",
      "Val loss: 0.8984, Val acc: 0.6812\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_131.pth\n",
      "Checkpoint del mejor modelo guardado en la época 131\n",
      "0.05\n",
      "Epoch 132/400\n",
      "Iteración 46147 - Lote 35/352 - Pérdida de Entrenamiento: 0.8298, Precisión de Entrenamiento: 0.7016\n",
      "Iteración 46182 - Lote 70/352 - Pérdida de Entrenamiento: 0.8203, Precisión de Entrenamiento: 0.7013\n",
      "Iteración 46217 - Lote 105/352 - Pérdida de Entrenamiento: 0.8145, Precisión de Entrenamiento: 0.7039\n",
      "Iteración 46252 - Lote 140/352 - Pérdida de Entrenamiento: 0.8155, Precisión de Entrenamiento: 0.7046\n",
      "Iteración 46287 - Lote 175/352 - Pérdida de Entrenamiento: 0.8091, Precisión de Entrenamiento: 0.7076\n",
      "Iteración 46322 - Lote 210/352 - Pérdida de Entrenamiento: 0.8123, Precisión de Entrenamiento: 0.7064\n",
      "Iteración 46357 - Lote 245/352 - Pérdida de Entrenamiento: 0.8157, Precisión de Entrenamiento: 0.7047\n",
      "Iteración 46392 - Lote 280/352 - Pérdida de Entrenamiento: 0.8175, Precisión de Entrenamiento: 0.7040\n",
      "Iteración 46427 - Lote 315/352 - Pérdida de Entrenamiento: 0.8165, Precisión de Entrenamiento: 0.7051\n",
      "Iteración 46462 - Lote 350/352 - Pérdida de Entrenamiento: 0.8159, Precisión de Entrenamiento: 0.7057\n",
      "Val loss: 0.9068, Val acc: 0.6848\n",
      "0.05\n",
      "Epoch 133/400\n",
      "Iteración 46499 - Lote 35/352 - Pérdida de Entrenamiento: 0.7999, Precisión de Entrenamiento: 0.7116\n",
      "Iteración 46534 - Lote 70/352 - Pérdida de Entrenamiento: 0.8031, Precisión de Entrenamiento: 0.7124\n",
      "Iteración 46569 - Lote 105/352 - Pérdida de Entrenamiento: 0.7948, Precisión de Entrenamiento: 0.7162\n",
      "Iteración 46604 - Lote 140/352 - Pérdida de Entrenamiento: 0.8056, Precisión de Entrenamiento: 0.7100\n",
      "Iteración 46639 - Lote 175/352 - Pérdida de Entrenamiento: 0.8056, Precisión de Entrenamiento: 0.7100\n",
      "Iteración 46674 - Lote 210/352 - Pérdida de Entrenamiento: 0.8066, Precisión de Entrenamiento: 0.7094\n",
      "Iteración 46709 - Lote 245/352 - Pérdida de Entrenamiento: 0.8101, Precisión de Entrenamiento: 0.7073\n",
      "Iteración 46744 - Lote 280/352 - Pérdida de Entrenamiento: 0.8209, Precisión de Entrenamiento: 0.7050\n",
      "Iteración 46779 - Lote 315/352 - Pérdida de Entrenamiento: 0.8214, Precisión de Entrenamiento: 0.7052\n",
      "Iteración 46814 - Lote 350/352 - Pérdida de Entrenamiento: 0.8234, Precisión de Entrenamiento: 0.7046\n",
      "Val loss: 0.8952, Val acc: 0.6856\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_133.pth\n",
      "Checkpoint del mejor modelo guardado en la época 133\n",
      "0.05\n",
      "Epoch 134/400\n",
      "Iteración 46851 - Lote 35/352 - Pérdida de Entrenamiento: 0.8099, Precisión de Entrenamiento: 0.7080\n",
      "Iteración 46886 - Lote 70/352 - Pérdida de Entrenamiento: 0.8021, Precisión de Entrenamiento: 0.7127\n",
      "Iteración 46921 - Lote 105/352 - Pérdida de Entrenamiento: 0.8027, Precisión de Entrenamiento: 0.7100\n",
      "Iteración 46956 - Lote 140/352 - Pérdida de Entrenamiento: 0.8008, Precisión de Entrenamiento: 0.7121\n",
      "Iteración 46991 - Lote 175/352 - Pérdida de Entrenamiento: 0.8027, Precisión de Entrenamiento: 0.7113\n",
      "Iteración 47026 - Lote 210/352 - Pérdida de Entrenamiento: 0.8071, Precisión de Entrenamiento: 0.7103\n",
      "Iteración 47061 - Lote 245/352 - Pérdida de Entrenamiento: 0.8049, Precisión de Entrenamiento: 0.7109\n",
      "Iteración 47096 - Lote 280/352 - Pérdida de Entrenamiento: 0.8036, Precisión de Entrenamiento: 0.7115\n",
      "Iteración 47131 - Lote 315/352 - Pérdida de Entrenamiento: 0.8133, Precisión de Entrenamiento: 0.7086\n",
      "Iteración 47166 - Lote 350/352 - Pérdida de Entrenamiento: 0.8295, Precisión de Entrenamiento: 0.7029\n",
      "Val loss: 0.9688, Val acc: 0.6594\n",
      "0.05\n",
      "Epoch 135/400\n",
      "Iteración 47203 - Lote 35/352 - Pérdida de Entrenamiento: 0.8708, Precisión de Entrenamiento: 0.6888\n",
      "Iteración 47238 - Lote 70/352 - Pérdida de Entrenamiento: 0.8527, Precisión de Entrenamiento: 0.6910\n",
      "Iteración 47273 - Lote 105/352 - Pérdida de Entrenamiento: 0.8458, Precisión de Entrenamiento: 0.6933\n",
      "Iteración 47308 - Lote 140/352 - Pérdida de Entrenamiento: 0.8491, Precisión de Entrenamiento: 0.6895\n",
      "Iteración 47343 - Lote 175/352 - Pérdida de Entrenamiento: 0.8439, Precisión de Entrenamiento: 0.6929\n",
      "Iteración 47378 - Lote 210/352 - Pérdida de Entrenamiento: 0.8420, Precisión de Entrenamiento: 0.6942\n",
      "Iteración 47413 - Lote 245/352 - Pérdida de Entrenamiento: 0.8405, Precisión de Entrenamiento: 0.6959\n",
      "Iteración 47448 - Lote 280/352 - Pérdida de Entrenamiento: 0.8434, Precisión de Entrenamiento: 0.6949\n",
      "Iteración 47483 - Lote 315/352 - Pérdida de Entrenamiento: 0.8430, Precisión de Entrenamiento: 0.6952\n",
      "Iteración 47518 - Lote 350/352 - Pérdida de Entrenamiento: 0.8403, Precisión de Entrenamiento: 0.6961\n",
      "Val loss: 0.9234, Val acc: 0.6796\n",
      "Gradientes para features.0.0.weight: min=-0.01686340756714344, max=0.006095923017710447, mean=-0.0003268536238465458, std=0.0018940645968541503\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.017422126606106758, max=0.012492830865085125, mean=-3.185416289852583e-06, std=0.001255971030332148\n",
      "Gradientes para classifier.1.weight: min=-0.014325140044093132, max=0.008498704992234707, mean=-4.656612768993984e-12, std=0.0015047892229631543\n",
      "0.05\n",
      "Epoch 136/400\n",
      "Iteración 47555 - Lote 35/352 - Pérdida de Entrenamiento: 0.7760, Precisión de Entrenamiento: 0.7152\n",
      "Iteración 47590 - Lote 70/352 - Pérdida de Entrenamiento: 0.7827, Precisión de Entrenamiento: 0.7175\n",
      "Iteración 47625 - Lote 105/352 - Pérdida de Entrenamiento: 0.7906, Precisión de Entrenamiento: 0.7130\n",
      "Iteración 47660 - Lote 140/352 - Pérdida de Entrenamiento: 0.7989, Precisión de Entrenamiento: 0.7104\n",
      "Iteración 47695 - Lote 175/352 - Pérdida de Entrenamiento: 0.8023, Precisión de Entrenamiento: 0.7118\n",
      "Iteración 47730 - Lote 210/352 - Pérdida de Entrenamiento: 0.8032, Precisión de Entrenamiento: 0.7121\n",
      "Iteración 47765 - Lote 245/352 - Pérdida de Entrenamiento: 0.8049, Precisión de Entrenamiento: 0.7116\n",
      "Iteración 47800 - Lote 280/352 - Pérdida de Entrenamiento: 0.8083, Precisión de Entrenamiento: 0.7100\n",
      "Iteración 47835 - Lote 315/352 - Pérdida de Entrenamiento: 0.8059, Precisión de Entrenamiento: 0.7112\n",
      "Iteración 47870 - Lote 350/352 - Pérdida de Entrenamiento: 0.8048, Precisión de Entrenamiento: 0.7119\n",
      "Val loss: 0.9122, Val acc: 0.6820\n",
      "0.05\n",
      "Epoch 137/400\n",
      "Iteración 47907 - Lote 35/352 - Pérdida de Entrenamiento: 0.7993, Precisión de Entrenamiento: 0.7147\n",
      "Iteración 47942 - Lote 70/352 - Pérdida de Entrenamiento: 0.7862, Precisión de Entrenamiento: 0.7203\n",
      "Iteración 47977 - Lote 105/352 - Pérdida de Entrenamiento: 0.7977, Precisión de Entrenamiento: 0.7170\n",
      "Iteración 48012 - Lote 140/352 - Pérdida de Entrenamiento: 0.8003, Precisión de Entrenamiento: 0.7153\n",
      "Iteración 48047 - Lote 175/352 - Pérdida de Entrenamiento: 0.7996, Precisión de Entrenamiento: 0.7146\n",
      "Iteración 48082 - Lote 210/352 - Pérdida de Entrenamiento: 0.8029, Precisión de Entrenamiento: 0.7123\n",
      "Iteración 48117 - Lote 245/352 - Pérdida de Entrenamiento: 0.8038, Precisión de Entrenamiento: 0.7125\n",
      "Iteración 48152 - Lote 280/352 - Pérdida de Entrenamiento: 0.8058, Precisión de Entrenamiento: 0.7114\n",
      "Iteración 48187 - Lote 315/352 - Pérdida de Entrenamiento: 0.8076, Precisión de Entrenamiento: 0.7109\n",
      "Iteración 48222 - Lote 350/352 - Pérdida de Entrenamiento: 0.8113, Precisión de Entrenamiento: 0.7095\n",
      "Val loss: 0.9195, Val acc: 0.6774\n",
      "0.05\n",
      "Epoch 138/400\n",
      "Iteración 48259 - Lote 35/352 - Pérdida de Entrenamiento: 0.7901, Precisión de Entrenamiento: 0.7179\n",
      "Iteración 48294 - Lote 70/352 - Pérdida de Entrenamiento: 0.8007, Precisión de Entrenamiento: 0.7154\n",
      "Iteración 48329 - Lote 105/352 - Pérdida de Entrenamiento: 0.8138, Precisión de Entrenamiento: 0.7105\n",
      "Iteración 48364 - Lote 140/352 - Pérdida de Entrenamiento: 0.8159, Precisión de Entrenamiento: 0.7071\n",
      "Iteración 48399 - Lote 175/352 - Pérdida de Entrenamiento: 0.8188, Precisión de Entrenamiento: 0.7061\n",
      "Iteración 48434 - Lote 210/352 - Pérdida de Entrenamiento: 0.8194, Precisión de Entrenamiento: 0.7061\n",
      "Iteración 48469 - Lote 245/352 - Pérdida de Entrenamiento: 0.8121, Precisión de Entrenamiento: 0.7083\n",
      "Iteración 48504 - Lote 280/352 - Pérdida de Entrenamiento: 0.8103, Precisión de Entrenamiento: 0.7090\n",
      "Iteración 48539 - Lote 315/352 - Pérdida de Entrenamiento: 0.8073, Precisión de Entrenamiento: 0.7096\n",
      "Iteración 48574 - Lote 350/352 - Pérdida de Entrenamiento: 0.8052, Precisión de Entrenamiento: 0.7110\n",
      "Val loss: 0.9039, Val acc: 0.6868\n",
      "0.05\n",
      "Epoch 139/400\n",
      "Iteración 48611 - Lote 35/352 - Pérdida de Entrenamiento: 0.7608, Precisión de Entrenamiento: 0.7203\n",
      "Iteración 48646 - Lote 70/352 - Pérdida de Entrenamiento: 0.7602, Precisión de Entrenamiento: 0.7221\n",
      "Iteración 48681 - Lote 105/352 - Pérdida de Entrenamiento: 0.7560, Precisión de Entrenamiento: 0.7253\n",
      "Iteración 48716 - Lote 140/352 - Pérdida de Entrenamiento: 0.7623, Precisión de Entrenamiento: 0.7233\n",
      "Iteración 48751 - Lote 175/352 - Pérdida de Entrenamiento: 0.7650, Precisión de Entrenamiento: 0.7226\n",
      "Iteración 48786 - Lote 210/352 - Pérdida de Entrenamiento: 0.7713, Precisión de Entrenamiento: 0.7214\n",
      "Iteración 48821 - Lote 245/352 - Pérdida de Entrenamiento: 0.7814, Precisión de Entrenamiento: 0.7183\n",
      "Iteración 48856 - Lote 280/352 - Pérdida de Entrenamiento: 0.7826, Precisión de Entrenamiento: 0.7176\n",
      "Iteración 48891 - Lote 315/352 - Pérdida de Entrenamiento: 0.7851, Precisión de Entrenamiento: 0.7164\n",
      "Iteración 48926 - Lote 350/352 - Pérdida de Entrenamiento: 0.7887, Precisión de Entrenamiento: 0.7150\n",
      "Val loss: 0.9033, Val acc: 0.6868\n",
      "0.05\n",
      "Epoch 140/400\n",
      "Iteración 48963 - Lote 35/352 - Pérdida de Entrenamiento: 0.7787, Precisión de Entrenamiento: 0.7179\n",
      "Iteración 48998 - Lote 70/352 - Pérdida de Entrenamiento: 0.8367, Precisión de Entrenamiento: 0.6979\n",
      "Iteración 49033 - Lote 105/352 - Pérdida de Entrenamiento: 0.8394, Precisión de Entrenamiento: 0.6964\n",
      "Iteración 49068 - Lote 140/352 - Pérdida de Entrenamiento: 0.8319, Precisión de Entrenamiento: 0.6988\n",
      "Iteración 49103 - Lote 175/352 - Pérdida de Entrenamiento: 0.8252, Precisión de Entrenamiento: 0.7018\n",
      "Iteración 49138 - Lote 210/352 - Pérdida de Entrenamiento: 0.8238, Precisión de Entrenamiento: 0.7012\n",
      "Iteración 49173 - Lote 245/352 - Pérdida de Entrenamiento: 0.8260, Precisión de Entrenamiento: 0.7007\n",
      "Iteración 49208 - Lote 280/352 - Pérdida de Entrenamiento: 0.8977, Precisión de Entrenamiento: 0.6780\n",
      "Iteración 49243 - Lote 315/352 - Pérdida de Entrenamiento: 0.9310, Precisión de Entrenamiento: 0.6652\n",
      "Iteración 49278 - Lote 350/352 - Pérdida de Entrenamiento: 0.9453, Precisión de Entrenamiento: 0.6596\n",
      "Val loss: 1.0288, Val acc: 0.6336\n",
      "Gradientes para features.0.0.weight: min=-0.007283953949809074, max=0.028716446831822395, mean=0.0008192386594600976, std=0.004262607078999281\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.011049673892557621, max=0.010931123048067093, mean=-4.642069143301342e-06, std=0.001059316797181964\n",
      "Gradientes para classifier.1.weight: min=-0.00866743829101324, max=0.011194608174264431, mean=-5.122274132629556e-11, std=0.0019593394827097654\n",
      "0.05\n",
      "Epoch 141/400\n",
      "Iteración 49315 - Lote 35/352 - Pérdida de Entrenamiento: 1.0241, Precisión de Entrenamiento: 0.6241\n",
      "Iteración 49350 - Lote 70/352 - Pérdida de Entrenamiento: 0.9794, Precisión de Entrenamiento: 0.6401\n",
      "Iteración 49385 - Lote 105/352 - Pérdida de Entrenamiento: 0.9628, Precisión de Entrenamiento: 0.6484\n",
      "Iteración 49420 - Lote 140/352 - Pérdida de Entrenamiento: 0.9513, Precisión de Entrenamiento: 0.6539\n",
      "Iteración 49455 - Lote 175/352 - Pérdida de Entrenamiento: 0.9411, Precisión de Entrenamiento: 0.6590\n",
      "Iteración 49490 - Lote 210/352 - Pérdida de Entrenamiento: 0.9261, Precisión de Entrenamiento: 0.6645\n",
      "Iteración 49525 - Lote 245/352 - Pérdida de Entrenamiento: 0.9154, Precisión de Entrenamiento: 0.6688\n",
      "Iteración 49560 - Lote 280/352 - Pérdida de Entrenamiento: 0.9072, Precisión de Entrenamiento: 0.6721\n",
      "Iteración 49595 - Lote 315/352 - Pérdida de Entrenamiento: 0.8988, Precisión de Entrenamiento: 0.6759\n",
      "Iteración 49630 - Lote 350/352 - Pérdida de Entrenamiento: 0.8922, Precisión de Entrenamiento: 0.6787\n",
      "Val loss: 0.9169, Val acc: 0.6802\n",
      "0.05\n",
      "Epoch 142/400\n",
      "Iteración 49667 - Lote 35/352 - Pérdida de Entrenamiento: 0.8306, Precisión de Entrenamiento: 0.6987\n",
      "Iteración 49702 - Lote 70/352 - Pérdida de Entrenamiento: 0.8133, Precisión de Entrenamiento: 0.7038\n",
      "Iteración 49737 - Lote 105/352 - Pérdida de Entrenamiento: 0.8175, Precisión de Entrenamiento: 0.7023\n",
      "Iteración 49772 - Lote 140/352 - Pérdida de Entrenamiento: 0.8096, Precisión de Entrenamiento: 0.7054\n",
      "Iteración 49807 - Lote 175/352 - Pérdida de Entrenamiento: 0.8111, Precisión de Entrenamiento: 0.7054\n",
      "Iteración 49842 - Lote 210/352 - Pérdida de Entrenamiento: 0.8135, Precisión de Entrenamiento: 0.7054\n",
      "Iteración 49877 - Lote 245/352 - Pérdida de Entrenamiento: 0.8135, Precisión de Entrenamiento: 0.7046\n",
      "Iteración 49912 - Lote 280/352 - Pérdida de Entrenamiento: 0.8110, Precisión de Entrenamiento: 0.7066\n",
      "Iteración 49947 - Lote 315/352 - Pérdida de Entrenamiento: 0.8137, Precisión de Entrenamiento: 0.7061\n",
      "Iteración 49982 - Lote 350/352 - Pérdida de Entrenamiento: 0.8120, Precisión de Entrenamiento: 0.7063\n",
      "Val loss: 0.8961, Val acc: 0.6840\n",
      "0.025\n",
      "Epoch 143/400\n",
      "Iteración 50019 - Lote 35/352 - Pérdida de Entrenamiento: 0.7867, Precisión de Entrenamiento: 0.7203\n",
      "Iteración 50054 - Lote 70/352 - Pérdida de Entrenamiento: 0.7850, Precisión de Entrenamiento: 0.7167\n",
      "Iteración 50089 - Lote 105/352 - Pérdida de Entrenamiento: 0.7741, Precisión de Entrenamiento: 0.7232\n",
      "Iteración 50124 - Lote 140/352 - Pérdida de Entrenamiento: 0.7682, Precisión de Entrenamiento: 0.7254\n",
      "Iteración 50159 - Lote 175/352 - Pérdida de Entrenamiento: 0.7632, Precisión de Entrenamiento: 0.7279\n",
      "Iteración 50194 - Lote 210/352 - Pérdida de Entrenamiento: 0.7603, Precisión de Entrenamiento: 0.7282\n",
      "Iteración 50229 - Lote 245/352 - Pérdida de Entrenamiento: 0.7615, Precisión de Entrenamiento: 0.7287\n",
      "Iteración 50264 - Lote 280/352 - Pérdida de Entrenamiento: 0.7593, Precisión de Entrenamiento: 0.7288\n",
      "Iteración 50299 - Lote 315/352 - Pérdida de Entrenamiento: 0.7601, Precisión de Entrenamiento: 0.7280\n",
      "Iteración 50334 - Lote 350/352 - Pérdida de Entrenamiento: 0.7602, Precisión de Entrenamiento: 0.7273\n",
      "Val loss: 0.8925, Val acc: 0.6862\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_143.pth\n",
      "Checkpoint del mejor modelo guardado en la época 143\n",
      "0.025\n",
      "Epoch 144/400\n",
      "Iteración 50371 - Lote 35/352 - Pérdida de Entrenamiento: 0.7574, Precisión de Entrenamiento: 0.7234\n",
      "Iteración 50406 - Lote 70/352 - Pérdida de Entrenamiento: 0.7617, Precisión de Entrenamiento: 0.7232\n",
      "Iteración 50441 - Lote 105/352 - Pérdida de Entrenamiento: 0.7600, Precisión de Entrenamiento: 0.7249\n",
      "Iteración 50476 - Lote 140/352 - Pérdida de Entrenamiento: 0.7524, Precisión de Entrenamiento: 0.7277\n",
      "Iteración 50511 - Lote 175/352 - Pérdida de Entrenamiento: 0.7491, Precisión de Entrenamiento: 0.7294\n",
      "Iteración 50546 - Lote 210/352 - Pérdida de Entrenamiento: 0.7483, Precisión de Entrenamiento: 0.7302\n",
      "Iteración 50581 - Lote 245/352 - Pérdida de Entrenamiento: 0.7465, Precisión de Entrenamiento: 0.7302\n",
      "Iteración 50616 - Lote 280/352 - Pérdida de Entrenamiento: 0.7456, Precisión de Entrenamiento: 0.7310\n",
      "Iteración 50651 - Lote 315/352 - Pérdida de Entrenamiento: 0.7427, Precisión de Entrenamiento: 0.7328\n",
      "Iteración 50686 - Lote 350/352 - Pérdida de Entrenamiento: 0.7428, Precisión de Entrenamiento: 0.7325\n",
      "Val loss: 0.8950, Val acc: 0.6906\n",
      "0.025\n",
      "Epoch 145/400\n",
      "Iteración 50723 - Lote 35/352 - Pérdida de Entrenamiento: 0.7125, Precisión de Entrenamiento: 0.7449\n",
      "Iteración 50758 - Lote 70/352 - Pérdida de Entrenamiento: 0.7410, Precisión de Entrenamiento: 0.7348\n",
      "Iteración 50793 - Lote 105/352 - Pérdida de Entrenamiento: 0.7448, Precisión de Entrenamiento: 0.7348\n",
      "Iteración 50828 - Lote 140/352 - Pérdida de Entrenamiento: 0.7466, Precisión de Entrenamiento: 0.7325\n",
      "Iteración 50863 - Lote 175/352 - Pérdida de Entrenamiento: 0.7430, Precisión de Entrenamiento: 0.7335\n",
      "Iteración 50898 - Lote 210/352 - Pérdida de Entrenamiento: 0.7425, Precisión de Entrenamiento: 0.7346\n",
      "Iteración 50933 - Lote 245/352 - Pérdida de Entrenamiento: 0.7421, Precisión de Entrenamiento: 0.7342\n",
      "Iteración 50968 - Lote 280/352 - Pérdida de Entrenamiento: 0.7397, Precisión de Entrenamiento: 0.7352\n",
      "Iteración 51003 - Lote 315/352 - Pérdida de Entrenamiento: 0.7394, Precisión de Entrenamiento: 0.7351\n",
      "Iteración 51038 - Lote 350/352 - Pérdida de Entrenamiento: 0.7393, Precisión de Entrenamiento: 0.7351\n",
      "Val loss: 0.8935, Val acc: 0.6874\n",
      "Gradientes para features.0.0.weight: min=-0.028319239616394043, max=0.008852383121848106, mean=-0.0006535182474181056, std=0.0035803441423922777\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.01027834601700306, max=0.014079268090426922, mean=9.62588001129916e-06, std=0.001058083144016564\n",
      "Gradientes para classifier.1.weight: min=-0.008371963165700436, max=0.01111653819680214, mean=0.0, std=0.0017031066818162799\n",
      "0.025\n",
      "Epoch 146/400\n",
      "Iteración 51075 - Lote 35/352 - Pérdida de Entrenamiento: 0.6971, Precisión de Entrenamiento: 0.7520\n",
      "Iteración 51110 - Lote 70/352 - Pérdida de Entrenamiento: 0.7014, Precisión de Entrenamiento: 0.7492\n",
      "Iteración 51145 - Lote 105/352 - Pérdida de Entrenamiento: 0.7079, Precisión de Entrenamiento: 0.7436\n",
      "Iteración 51180 - Lote 140/352 - Pérdida de Entrenamiento: 0.7092, Precisión de Entrenamiento: 0.7426\n",
      "Iteración 51215 - Lote 175/352 - Pérdida de Entrenamiento: 0.7119, Precisión de Entrenamiento: 0.7428\n",
      "Iteración 51250 - Lote 210/352 - Pérdida de Entrenamiento: 0.7121, Precisión de Entrenamiento: 0.7432\n",
      "Iteración 51285 - Lote 245/352 - Pérdida de Entrenamiento: 0.7134, Precisión de Entrenamiento: 0.7429\n",
      "Iteración 51320 - Lote 280/352 - Pérdida de Entrenamiento: 0.7190, Precisión de Entrenamiento: 0.7419\n",
      "Iteración 51355 - Lote 315/352 - Pérdida de Entrenamiento: 0.7185, Precisión de Entrenamiento: 0.7425\n",
      "Iteración 51390 - Lote 350/352 - Pérdida de Entrenamiento: 0.7235, Precisión de Entrenamiento: 0.7404\n",
      "Val loss: 0.8817, Val acc: 0.6946\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_146.pth\n",
      "Checkpoint del mejor modelo guardado en la época 146\n",
      "0.025\n",
      "Epoch 147/400\n",
      "Iteración 51427 - Lote 35/352 - Pérdida de Entrenamiento: 0.7462, Precisión de Entrenamiento: 0.7321\n",
      "Iteración 51462 - Lote 70/352 - Pérdida de Entrenamiento: 0.7207, Precisión de Entrenamiento: 0.7410\n",
      "Iteración 51497 - Lote 105/352 - Pérdida de Entrenamiento: 0.7097, Precisión de Entrenamiento: 0.7454\n",
      "Iteración 51532 - Lote 140/352 - Pérdida de Entrenamiento: 0.7106, Precisión de Entrenamiento: 0.7440\n",
      "Iteración 51567 - Lote 175/352 - Pérdida de Entrenamiento: 0.7114, Precisión de Entrenamiento: 0.7437\n",
      "Iteración 51602 - Lote 210/352 - Pérdida de Entrenamiento: 0.7121, Precisión de Entrenamiento: 0.7434\n",
      "Iteración 51637 - Lote 245/352 - Pérdida de Entrenamiento: 0.7138, Precisión de Entrenamiento: 0.7415\n",
      "Iteración 51672 - Lote 280/352 - Pérdida de Entrenamiento: 0.7160, Precisión de Entrenamiento: 0.7399\n",
      "Iteración 51707 - Lote 315/352 - Pérdida de Entrenamiento: 0.7177, Precisión de Entrenamiento: 0.7400\n",
      "Iteración 51742 - Lote 350/352 - Pérdida de Entrenamiento: 0.7156, Precisión de Entrenamiento: 0.7412\n",
      "Val loss: 0.8887, Val acc: 0.6902\n",
      "0.025\n",
      "Epoch 148/400\n",
      "Iteración 51779 - Lote 35/352 - Pérdida de Entrenamiento: 0.7033, Precisión de Entrenamiento: 0.7531\n",
      "Iteración 51814 - Lote 70/352 - Pérdida de Entrenamiento: 0.7092, Precisión de Entrenamiento: 0.7477\n",
      "Iteración 51849 - Lote 105/352 - Pérdida de Entrenamiento: 0.7107, Precisión de Entrenamiento: 0.7464\n",
      "Iteración 51884 - Lote 140/352 - Pérdida de Entrenamiento: 0.7136, Precisión de Entrenamiento: 0.7454\n",
      "Iteración 51919 - Lote 175/352 - Pérdida de Entrenamiento: 0.7177, Precisión de Entrenamiento: 0.7442\n",
      "Iteración 51954 - Lote 210/352 - Pérdida de Entrenamiento: 0.7169, Precisión de Entrenamiento: 0.7442\n",
      "Iteración 51989 - Lote 245/352 - Pérdida de Entrenamiento: 0.7236, Precisión de Entrenamiento: 0.7416\n",
      "Iteración 52024 - Lote 280/352 - Pérdida de Entrenamiento: 0.7253, Precisión de Entrenamiento: 0.7407\n",
      "Iteración 52059 - Lote 315/352 - Pérdida de Entrenamiento: 0.7248, Precisión de Entrenamiento: 0.7405\n",
      "Iteración 52094 - Lote 350/352 - Pérdida de Entrenamiento: 0.7253, Precisión de Entrenamiento: 0.7401\n",
      "Val loss: 0.8722, Val acc: 0.6904\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_148.pth\n",
      "Checkpoint del mejor modelo guardado en la época 148\n",
      "0.025\n",
      "Epoch 149/400\n",
      "Iteración 52131 - Lote 35/352 - Pérdida de Entrenamiento: 0.6807, Precisión de Entrenamiento: 0.7540\n",
      "Iteración 52166 - Lote 70/352 - Pérdida de Entrenamiento: 0.6922, Precisión de Entrenamiento: 0.7517\n",
      "Iteración 52201 - Lote 105/352 - Pérdida de Entrenamiento: 0.7000, Precisión de Entrenamiento: 0.7474\n",
      "Iteración 52236 - Lote 140/352 - Pérdida de Entrenamiento: 0.7019, Precisión de Entrenamiento: 0.7470\n",
      "Iteración 52271 - Lote 175/352 - Pérdida de Entrenamiento: 0.7135, Precisión de Entrenamiento: 0.7448\n",
      "Iteración 52306 - Lote 210/352 - Pérdida de Entrenamiento: 0.7139, Precisión de Entrenamiento: 0.7446\n",
      "Iteración 52341 - Lote 245/352 - Pérdida de Entrenamiento: 0.7130, Precisión de Entrenamiento: 0.7446\n",
      "Iteración 52376 - Lote 280/352 - Pérdida de Entrenamiento: 0.7126, Precisión de Entrenamiento: 0.7449\n",
      "Iteración 52411 - Lote 315/352 - Pérdida de Entrenamiento: 0.7106, Precisión de Entrenamiento: 0.7455\n",
      "Iteración 52446 - Lote 350/352 - Pérdida de Entrenamiento: 0.7133, Precisión de Entrenamiento: 0.7443\n",
      "Val loss: 0.9126, Val acc: 0.6886\n",
      "0.025\n",
      "Epoch 150/400\n",
      "Iteración 52483 - Lote 35/352 - Pérdida de Entrenamiento: 0.7211, Precisión de Entrenamiento: 0.7344\n",
      "Iteración 52518 - Lote 70/352 - Pérdida de Entrenamiento: 0.6974, Precisión de Entrenamiento: 0.7463\n",
      "Iteración 52553 - Lote 105/352 - Pérdida de Entrenamiento: 0.7030, Precisión de Entrenamiento: 0.7434\n",
      "Iteración 52588 - Lote 140/352 - Pérdida de Entrenamiento: 0.7044, Precisión de Entrenamiento: 0.7438\n",
      "Iteración 52623 - Lote 175/352 - Pérdida de Entrenamiento: 0.6985, Precisión de Entrenamiento: 0.7464\n",
      "Iteración 52658 - Lote 210/352 - Pérdida de Entrenamiento: 0.6999, Precisión de Entrenamiento: 0.7455\n",
      "Iteración 52693 - Lote 245/352 - Pérdida de Entrenamiento: 0.6949, Precisión de Entrenamiento: 0.7474\n",
      "Iteración 52728 - Lote 280/352 - Pérdida de Entrenamiento: 0.7000, Precisión de Entrenamiento: 0.7461\n",
      "Iteración 52763 - Lote 315/352 - Pérdida de Entrenamiento: 0.7011, Precisión de Entrenamiento: 0.7460\n",
      "Iteración 52798 - Lote 350/352 - Pérdida de Entrenamiento: 0.7016, Precisión de Entrenamiento: 0.7470\n",
      "Val loss: 0.9141, Val acc: 0.6840\n",
      "Gradientes para features.0.0.weight: min=-0.024302728474140167, max=0.006555336527526379, mean=-0.0004319075378589332, std=0.002816572319716215\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.013231954537332058, max=0.012299933470785618, mean=-5.631797648675274e-06, std=0.00124360341578722\n",
      "Gradientes para classifier.1.weight: min=-0.009609214030206203, max=0.011030116118490696, mean=1.1641532356165829e-11, std=0.0016547718551009893\n",
      "0.025\n",
      "Epoch 151/400\n",
      "Iteración 52835 - Lote 35/352 - Pérdida de Entrenamiento: 0.6890, Precisión de Entrenamiento: 0.7576\n",
      "Iteración 52870 - Lote 70/352 - Pérdida de Entrenamiento: 0.6746, Precisión de Entrenamiento: 0.7598\n",
      "Iteración 52905 - Lote 105/352 - Pérdida de Entrenamiento: 0.6815, Precisión de Entrenamiento: 0.7533\n",
      "Iteración 52940 - Lote 140/352 - Pérdida de Entrenamiento: 0.6924, Precisión de Entrenamiento: 0.7504\n",
      "Iteración 52975 - Lote 175/352 - Pérdida de Entrenamiento: 0.6974, Precisión de Entrenamiento: 0.7482\n",
      "Iteración 53010 - Lote 210/352 - Pérdida de Entrenamiento: 0.7030, Precisión de Entrenamiento: 0.7469\n",
      "Iteración 53045 - Lote 245/352 - Pérdida de Entrenamiento: 0.7113, Precisión de Entrenamiento: 0.7436\n",
      "Iteración 53080 - Lote 280/352 - Pérdida de Entrenamiento: 0.7140, Precisión de Entrenamiento: 0.7427\n",
      "Iteración 53115 - Lote 315/352 - Pérdida de Entrenamiento: 0.7125, Precisión de Entrenamiento: 0.7428\n",
      "Iteración 53150 - Lote 350/352 - Pérdida de Entrenamiento: 0.7118, Precisión de Entrenamiento: 0.7430\n",
      "Val loss: 0.8953, Val acc: 0.6868\n",
      "0.025\n",
      "Epoch 152/400\n",
      "Iteración 53187 - Lote 35/352 - Pérdida de Entrenamiento: 0.7034, Precisión de Entrenamiento: 0.7460\n",
      "Iteración 53222 - Lote 70/352 - Pérdida de Entrenamiento: 0.7051, Precisión de Entrenamiento: 0.7469\n",
      "Iteración 53257 - Lote 105/352 - Pérdida de Entrenamiento: 0.7085, Precisión de Entrenamiento: 0.7492\n",
      "Iteración 53292 - Lote 140/352 - Pérdida de Entrenamiento: 0.7069, Precisión de Entrenamiento: 0.7485\n",
      "Iteración 53327 - Lote 175/352 - Pérdida de Entrenamiento: 0.7049, Precisión de Entrenamiento: 0.7477\n",
      "Iteración 53362 - Lote 210/352 - Pérdida de Entrenamiento: 0.7073, Precisión de Entrenamiento: 0.7481\n",
      "Iteración 53397 - Lote 245/352 - Pérdida de Entrenamiento: 0.7048, Precisión de Entrenamiento: 0.7487\n",
      "Iteración 53432 - Lote 280/352 - Pérdida de Entrenamiento: 0.7118, Precisión de Entrenamiento: 0.7453\n",
      "Iteración 53467 - Lote 315/352 - Pérdida de Entrenamiento: 0.7144, Precisión de Entrenamiento: 0.7445\n",
      "Iteración 53502 - Lote 350/352 - Pérdida de Entrenamiento: 0.7149, Precisión de Entrenamiento: 0.7438\n",
      "Val loss: 0.9146, Val acc: 0.6842\n",
      "0.025\n",
      "Epoch 153/400\n",
      "Iteración 53539 - Lote 35/352 - Pérdida de Entrenamiento: 0.7055, Precisión de Entrenamiento: 0.7440\n",
      "Iteración 53574 - Lote 70/352 - Pérdida de Entrenamiento: 0.6832, Precisión de Entrenamiento: 0.7539\n",
      "Iteración 53609 - Lote 105/352 - Pérdida de Entrenamiento: 0.6850, Precisión de Entrenamiento: 0.7527\n",
      "Iteración 53644 - Lote 140/352 - Pérdida de Entrenamiento: 0.6878, Precisión de Entrenamiento: 0.7518\n",
      "Iteración 53679 - Lote 175/352 - Pérdida de Entrenamiento: 0.6893, Precisión de Entrenamiento: 0.7513\n",
      "Iteración 53714 - Lote 210/352 - Pérdida de Entrenamiento: 0.6878, Precisión de Entrenamiento: 0.7516\n",
      "Iteración 53749 - Lote 245/352 - Pérdida de Entrenamiento: 0.6945, Precisión de Entrenamiento: 0.7483\n",
      "Iteración 53784 - Lote 280/352 - Pérdida de Entrenamiento: 0.6924, Precisión de Entrenamiento: 0.7496\n",
      "Iteración 53819 - Lote 315/352 - Pérdida de Entrenamiento: 0.6921, Precisión de Entrenamiento: 0.7498\n",
      "Iteración 53854 - Lote 350/352 - Pérdida de Entrenamiento: 0.6941, Precisión de Entrenamiento: 0.7492\n",
      "Val loss: 0.8955, Val acc: 0.6902\n",
      "0.025\n",
      "Epoch 154/400\n",
      "Iteración 53891 - Lote 35/352 - Pérdida de Entrenamiento: 0.6889, Precisión de Entrenamiento: 0.7583\n",
      "Iteración 53926 - Lote 70/352 - Pérdida de Entrenamiento: 0.6831, Precisión de Entrenamiento: 0.7562\n",
      "Iteración 53961 - Lote 105/352 - Pérdida de Entrenamiento: 0.6857, Precisión de Entrenamiento: 0.7534\n",
      "Iteración 53996 - Lote 140/352 - Pérdida de Entrenamiento: 0.6822, Precisión de Entrenamiento: 0.7559\n",
      "Iteración 54031 - Lote 175/352 - Pérdida de Entrenamiento: 0.6817, Precisión de Entrenamiento: 0.7552\n",
      "Iteración 54066 - Lote 210/352 - Pérdida de Entrenamiento: 0.6836, Precisión de Entrenamiento: 0.7539\n",
      "Iteración 54101 - Lote 245/352 - Pérdida de Entrenamiento: 0.6896, Precisión de Entrenamiento: 0.7522\n",
      "Iteración 54136 - Lote 280/352 - Pérdida de Entrenamiento: 0.6927, Precisión de Entrenamiento: 0.7512\n",
      "Iteración 54171 - Lote 315/352 - Pérdida de Entrenamiento: 0.6912, Precisión de Entrenamiento: 0.7515\n",
      "Iteración 54206 - Lote 350/352 - Pérdida de Entrenamiento: 0.6931, Precisión de Entrenamiento: 0.7506\n",
      "Val loss: 0.8978, Val acc: 0.6928\n",
      "0.025\n",
      "Epoch 155/400\n",
      "Iteración 54243 - Lote 35/352 - Pérdida de Entrenamiento: 0.6706, Precisión de Entrenamiento: 0.7614\n",
      "Iteración 54278 - Lote 70/352 - Pérdida de Entrenamiento: 0.6703, Precisión de Entrenamiento: 0.7571\n",
      "Iteración 54313 - Lote 105/352 - Pérdida de Entrenamiento: 0.6778, Precisión de Entrenamiento: 0.7560\n",
      "Iteración 54348 - Lote 140/352 - Pérdida de Entrenamiento: 0.6836, Precisión de Entrenamiento: 0.7522\n",
      "Iteración 54383 - Lote 175/352 - Pérdida de Entrenamiento: 0.6898, Precisión de Entrenamiento: 0.7496\n",
      "Iteración 54418 - Lote 210/352 - Pérdida de Entrenamiento: 0.6940, Precisión de Entrenamiento: 0.7476\n",
      "Iteración 54453 - Lote 245/352 - Pérdida de Entrenamiento: 0.6928, Precisión de Entrenamiento: 0.7492\n",
      "Iteración 54488 - Lote 280/352 - Pérdida de Entrenamiento: 0.6926, Precisión de Entrenamiento: 0.7492\n",
      "Iteración 54523 - Lote 315/352 - Pérdida de Entrenamiento: 0.6902, Precisión de Entrenamiento: 0.7503\n",
      "Iteración 54558 - Lote 350/352 - Pérdida de Entrenamiento: 0.6886, Precisión de Entrenamiento: 0.7504\n",
      "Val loss: 0.9055, Val acc: 0.6952\n",
      "Gradientes para features.0.0.weight: min=-0.022797076031565666, max=0.03519243746995926, mean=0.00030633408459834754, std=0.005362367257475853\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.013374668546020985, max=0.012262246571481228, mean=1.9007032960871584e-06, std=0.0014438396319746971\n",
      "Gradientes para classifier.1.weight: min=-0.011032144539058208, max=0.01749575510621071, mean=9.313225537987968e-12, std=0.0018883179873228073\n",
      "0.025\n",
      "Epoch 156/400\n",
      "Iteración 54595 - Lote 35/352 - Pérdida de Entrenamiento: 0.6971, Precisión de Entrenamiento: 0.7431\n",
      "Iteración 54630 - Lote 70/352 - Pérdida de Entrenamiento: 0.7018, Precisión de Entrenamiento: 0.7458\n",
      "Iteración 54665 - Lote 105/352 - Pérdida de Entrenamiento: 0.7158, Precisión de Entrenamiento: 0.7420\n",
      "Iteración 54700 - Lote 140/352 - Pérdida de Entrenamiento: 0.7149, Precisión de Entrenamiento: 0.7422\n",
      "Iteración 54735 - Lote 175/352 - Pérdida de Entrenamiento: 0.7116, Precisión de Entrenamiento: 0.7425\n",
      "Iteración 54770 - Lote 210/352 - Pérdida de Entrenamiento: 0.7063, Precisión de Entrenamiento: 0.7445\n",
      "Iteración 54805 - Lote 245/352 - Pérdida de Entrenamiento: 0.7025, Precisión de Entrenamiento: 0.7458\n",
      "Iteración 54840 - Lote 280/352 - Pérdida de Entrenamiento: 0.7038, Precisión de Entrenamiento: 0.7451\n",
      "Iteración 54875 - Lote 315/352 - Pérdida de Entrenamiento: 0.7029, Precisión de Entrenamiento: 0.7460\n",
      "Iteración 54910 - Lote 350/352 - Pérdida de Entrenamiento: 0.6990, Precisión de Entrenamiento: 0.7475\n",
      "Val loss: 0.8955, Val acc: 0.6956\n",
      "0.025\n",
      "Epoch 157/400\n",
      "Iteración 54947 - Lote 35/352 - Pérdida de Entrenamiento: 0.6527, Precisión de Entrenamiento: 0.7621\n",
      "Iteración 54982 - Lote 70/352 - Pérdida de Entrenamiento: 0.6484, Precisión de Entrenamiento: 0.7644\n",
      "Iteración 55017 - Lote 105/352 - Pérdida de Entrenamiento: 0.6598, Precisión de Entrenamiento: 0.7615\n",
      "Iteración 55052 - Lote 140/352 - Pérdida de Entrenamiento: 0.6716, Precisión de Entrenamiento: 0.7575\n",
      "Iteración 55087 - Lote 175/352 - Pérdida de Entrenamiento: 0.6717, Precisión de Entrenamiento: 0.7565\n",
      "Iteración 55122 - Lote 210/352 - Pérdida de Entrenamiento: 0.6707, Precisión de Entrenamiento: 0.7566\n",
      "Iteración 55157 - Lote 245/352 - Pérdida de Entrenamiento: 0.6744, Precisión de Entrenamiento: 0.7549\n",
      "Iteración 55192 - Lote 280/352 - Pérdida de Entrenamiento: 0.6754, Precisión de Entrenamiento: 0.7552\n",
      "Iteración 55227 - Lote 315/352 - Pérdida de Entrenamiento: 0.6764, Precisión de Entrenamiento: 0.7545\n",
      "Iteración 55262 - Lote 350/352 - Pérdida de Entrenamiento: 0.6761, Precisión de Entrenamiento: 0.7548\n",
      "Val loss: 0.9080, Val acc: 0.6942\n",
      "0.0125\n",
      "Epoch 158/400\n",
      "Iteración 55299 - Lote 35/352 - Pérdida de Entrenamiento: 0.6466, Precisión de Entrenamiento: 0.7725\n",
      "Iteración 55334 - Lote 70/352 - Pérdida de Entrenamiento: 0.6496, Precisión de Entrenamiento: 0.7688\n",
      "Iteración 55369 - Lote 105/352 - Pérdida de Entrenamiento: 0.6512, Precisión de Entrenamiento: 0.7674\n",
      "Iteración 55404 - Lote 140/352 - Pérdida de Entrenamiento: 0.6469, Precisión de Entrenamiento: 0.7695\n",
      "Iteración 55439 - Lote 175/352 - Pérdida de Entrenamiento: 0.6502, Precisión de Entrenamiento: 0.7682\n",
      "Iteración 55474 - Lote 210/352 - Pérdida de Entrenamiento: 0.6523, Precisión de Entrenamiento: 0.7673\n",
      "Iteración 55509 - Lote 245/352 - Pérdida de Entrenamiento: 0.6503, Precisión de Entrenamiento: 0.7677\n",
      "Iteración 55544 - Lote 280/352 - Pérdida de Entrenamiento: 0.6530, Precisión de Entrenamiento: 0.7655\n",
      "Iteración 55579 - Lote 315/352 - Pérdida de Entrenamiento: 0.6510, Precisión de Entrenamiento: 0.7661\n",
      "Iteración 55614 - Lote 350/352 - Pérdida de Entrenamiento: 0.6542, Precisión de Entrenamiento: 0.7644\n",
      "Val loss: 0.8962, Val acc: 0.6950\n",
      "0.0125\n",
      "Epoch 159/400\n",
      "Iteración 55651 - Lote 35/352 - Pérdida de Entrenamiento: 0.6302, Precisión de Entrenamiento: 0.7741\n",
      "Iteración 55686 - Lote 70/352 - Pérdida de Entrenamiento: 0.6362, Precisión de Entrenamiento: 0.7720\n",
      "Iteración 55721 - Lote 105/352 - Pérdida de Entrenamiento: 0.6434, Precisión de Entrenamiento: 0.7694\n",
      "Iteración 55756 - Lote 140/352 - Pérdida de Entrenamiento: 0.6404, Precisión de Entrenamiento: 0.7715\n",
      "Iteración 55791 - Lote 175/352 - Pérdida de Entrenamiento: 0.6381, Precisión de Entrenamiento: 0.7712\n",
      "Iteración 55826 - Lote 210/352 - Pérdida de Entrenamiento: 0.6380, Precisión de Entrenamiento: 0.7707\n",
      "Iteración 55861 - Lote 245/352 - Pérdida de Entrenamiento: 0.6383, Precisión de Entrenamiento: 0.7695\n",
      "Iteración 55896 - Lote 280/352 - Pérdida de Entrenamiento: 0.6388, Precisión de Entrenamiento: 0.7697\n",
      "Iteración 55931 - Lote 315/352 - Pérdida de Entrenamiento: 0.6395, Precisión de Entrenamiento: 0.7702\n",
      "Iteración 55966 - Lote 350/352 - Pérdida de Entrenamiento: 0.6419, Precisión de Entrenamiento: 0.7688\n",
      "Val loss: 0.9147, Val acc: 0.6956\n",
      "0.0125\n",
      "Epoch 160/400\n",
      "Iteración 56003 - Lote 35/352 - Pérdida de Entrenamiento: 0.6294, Precisión de Entrenamiento: 0.7690\n",
      "Iteración 56038 - Lote 70/352 - Pérdida de Entrenamiento: 0.6366, Precisión de Entrenamiento: 0.7694\n",
      "Iteración 56073 - Lote 105/352 - Pérdida de Entrenamiento: 0.6473, Precisión de Entrenamiento: 0.7654\n",
      "Iteración 56108 - Lote 140/352 - Pérdida de Entrenamiento: 0.6434, Precisión de Entrenamiento: 0.7677\n",
      "Iteración 56143 - Lote 175/352 - Pérdida de Entrenamiento: 0.6452, Precisión de Entrenamiento: 0.7669\n",
      "Iteración 56178 - Lote 210/352 - Pérdida de Entrenamiento: 0.6413, Precisión de Entrenamiento: 0.7685\n",
      "Iteración 56213 - Lote 245/352 - Pérdida de Entrenamiento: 0.6429, Precisión de Entrenamiento: 0.7679\n",
      "Iteración 56248 - Lote 280/352 - Pérdida de Entrenamiento: 0.6417, Precisión de Entrenamiento: 0.7678\n",
      "Iteración 56283 - Lote 315/352 - Pérdida de Entrenamiento: 0.6434, Precisión de Entrenamiento: 0.7670\n",
      "Iteración 56318 - Lote 350/352 - Pérdida de Entrenamiento: 0.6444, Precisión de Entrenamiento: 0.7673\n",
      "Val loss: 0.9032, Val acc: 0.6952\n",
      "Gradientes para features.0.0.weight: min=-0.026144778355956078, max=0.003989801742136478, mean=-0.0006327004521153867, std=0.0028260766994208097\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.021616332232952118, max=0.014173953793942928, mean=4.519790763879428e-06, std=0.001417613821104169\n",
      "Gradientes para classifier.1.weight: min=-0.011656815186142921, max=0.009401231072843075, mean=9.313225537987968e-12, std=0.001834662863984704\n",
      "0.0125\n",
      "Epoch 161/400\n",
      "Iteración 56355 - Lote 35/352 - Pérdida de Entrenamiento: 0.6341, Precisión de Entrenamiento: 0.7719\n",
      "Iteración 56390 - Lote 70/352 - Pérdida de Entrenamiento: 0.6108, Precisión de Entrenamiento: 0.7811\n",
      "Iteración 56425 - Lote 105/352 - Pérdida de Entrenamiento: 0.6246, Precisión de Entrenamiento: 0.7765\n",
      "Iteración 56460 - Lote 140/352 - Pérdida de Entrenamiento: 0.6304, Precisión de Entrenamiento: 0.7717\n",
      "Iteración 56495 - Lote 175/352 - Pérdida de Entrenamiento: 0.6347, Precisión de Entrenamiento: 0.7693\n",
      "Iteración 56530 - Lote 210/352 - Pérdida de Entrenamiento: 0.6380, Precisión de Entrenamiento: 0.7687\n",
      "Iteración 56565 - Lote 245/352 - Pérdida de Entrenamiento: 0.6394, Precisión de Entrenamiento: 0.7690\n",
      "Iteración 56600 - Lote 280/352 - Pérdida de Entrenamiento: 0.6405, Precisión de Entrenamiento: 0.7694\n",
      "Iteración 56635 - Lote 315/352 - Pérdida de Entrenamiento: 0.6398, Precisión de Entrenamiento: 0.7705\n",
      "Iteración 56670 - Lote 350/352 - Pérdida de Entrenamiento: 0.6388, Precisión de Entrenamiento: 0.7705\n",
      "Val loss: 0.9223, Val acc: 0.6962\n",
      "0.0125\n",
      "Epoch 162/400\n",
      "Iteración 56707 - Lote 35/352 - Pérdida de Entrenamiento: 0.6186, Precisión de Entrenamiento: 0.7766\n",
      "Iteración 56742 - Lote 70/352 - Pérdida de Entrenamiento: 0.6236, Precisión de Entrenamiento: 0.7758\n",
      "Iteración 56777 - Lote 105/352 - Pérdida de Entrenamiento: 0.6247, Precisión de Entrenamiento: 0.7746\n",
      "Iteración 56812 - Lote 140/352 - Pérdida de Entrenamiento: 0.6237, Precisión de Entrenamiento: 0.7748\n",
      "Iteración 56847 - Lote 175/352 - Pérdida de Entrenamiento: 0.6279, Precisión de Entrenamiento: 0.7744\n",
      "Iteración 56882 - Lote 210/352 - Pérdida de Entrenamiento: 0.6336, Precisión de Entrenamiento: 0.7719\n",
      "Iteración 56917 - Lote 245/352 - Pérdida de Entrenamiento: 0.6314, Precisión de Entrenamiento: 0.7731\n",
      "Iteración 56952 - Lote 280/352 - Pérdida de Entrenamiento: 0.6308, Precisión de Entrenamiento: 0.7738\n",
      "Iteración 56987 - Lote 315/352 - Pérdida de Entrenamiento: 0.6303, Precisión de Entrenamiento: 0.7731\n",
      "Iteración 57022 - Lote 350/352 - Pérdida de Entrenamiento: 0.6316, Precisión de Entrenamiento: 0.7722\n",
      "Val loss: 0.9228, Val acc: 0.6958\n",
      "0.0125\n",
      "Epoch 163/400\n",
      "Iteración 57059 - Lote 35/352 - Pérdida de Entrenamiento: 0.6062, Precisión de Entrenamiento: 0.7775\n",
      "Iteración 57094 - Lote 70/352 - Pérdida de Entrenamiento: 0.6111, Precisión de Entrenamiento: 0.7749\n",
      "Iteración 57129 - Lote 105/352 - Pérdida de Entrenamiento: 0.6159, Precisión de Entrenamiento: 0.7738\n",
      "Iteración 57164 - Lote 140/352 - Pérdida de Entrenamiento: 0.6216, Precisión de Entrenamiento: 0.7739\n",
      "Iteración 57199 - Lote 175/352 - Pérdida de Entrenamiento: 0.6267, Precisión de Entrenamiento: 0.7727\n",
      "Iteración 57234 - Lote 210/352 - Pérdida de Entrenamiento: 0.6321, Precisión de Entrenamiento: 0.7718\n",
      "Iteración 57269 - Lote 245/352 - Pérdida de Entrenamiento: 0.6356, Precisión de Entrenamiento: 0.7711\n",
      "Iteración 57304 - Lote 280/352 - Pérdida de Entrenamiento: 0.6351, Precisión de Entrenamiento: 0.7709\n",
      "Iteración 57339 - Lote 315/352 - Pérdida de Entrenamiento: 0.6366, Precisión de Entrenamiento: 0.7699\n",
      "Iteración 57374 - Lote 350/352 - Pérdida de Entrenamiento: 0.6360, Precisión de Entrenamiento: 0.7702\n",
      "Val loss: 0.9204, Val acc: 0.6972\n",
      "0.0125\n",
      "Epoch 164/400\n",
      "Iteración 57411 - Lote 35/352 - Pérdida de Entrenamiento: 0.6290, Precisión de Entrenamiento: 0.7696\n",
      "Iteración 57446 - Lote 70/352 - Pérdida de Entrenamiento: 0.6376, Precisión de Entrenamiento: 0.7682\n",
      "Iteración 57481 - Lote 105/352 - Pérdida de Entrenamiento: 0.6399, Precisión de Entrenamiento: 0.7680\n",
      "Iteración 57516 - Lote 140/352 - Pérdida de Entrenamiento: 0.6346, Precisión de Entrenamiento: 0.7696\n",
      "Iteración 57551 - Lote 175/352 - Pérdida de Entrenamiento: 0.6308, Precisión de Entrenamiento: 0.7704\n",
      "Iteración 57586 - Lote 210/352 - Pérdida de Entrenamiento: 0.6334, Precisión de Entrenamiento: 0.7692\n",
      "Iteración 57621 - Lote 245/352 - Pérdida de Entrenamiento: 0.6338, Precisión de Entrenamiento: 0.7700\n",
      "Iteración 57656 - Lote 280/352 - Pérdida de Entrenamiento: 0.6339, Precisión de Entrenamiento: 0.7699\n",
      "Iteración 57691 - Lote 315/352 - Pérdida de Entrenamiento: 0.6324, Precisión de Entrenamiento: 0.7704\n",
      "Iteración 57726 - Lote 350/352 - Pérdida de Entrenamiento: 0.6366, Precisión de Entrenamiento: 0.7699\n",
      "Val loss: 0.9176, Val acc: 0.6952\n",
      "0.0125\n",
      "Epoch 165/400\n",
      "Iteración 57763 - Lote 35/352 - Pérdida de Entrenamiento: 0.6175, Precisión de Entrenamiento: 0.7768\n",
      "Iteración 57798 - Lote 70/352 - Pérdida de Entrenamiento: 0.6233, Precisión de Entrenamiento: 0.7756\n",
      "Iteración 57833 - Lote 105/352 - Pérdida de Entrenamiento: 0.6305, Precisión de Entrenamiento: 0.7737\n",
      "Iteración 57868 - Lote 140/352 - Pérdida de Entrenamiento: 0.6291, Precisión de Entrenamiento: 0.7734\n",
      "Iteración 57903 - Lote 175/352 - Pérdida de Entrenamiento: 0.6311, Precisión de Entrenamiento: 0.7722\n",
      "Iteración 57938 - Lote 210/352 - Pérdida de Entrenamiento: 0.6311, Precisión de Entrenamiento: 0.7712\n",
      "Iteración 57973 - Lote 245/352 - Pérdida de Entrenamiento: 0.6315, Precisión de Entrenamiento: 0.7711\n",
      "Iteración 58008 - Lote 280/352 - Pérdida de Entrenamiento: 0.6292, Precisión de Entrenamiento: 0.7729\n",
      "Iteración 58043 - Lote 315/352 - Pérdida de Entrenamiento: 0.6286, Precisión de Entrenamiento: 0.7731\n",
      "Iteración 58078 - Lote 350/352 - Pérdida de Entrenamiento: 0.6280, Precisión de Entrenamiento: 0.7731\n",
      "Val loss: 0.9166, Val acc: 0.6980\n",
      "Gradientes para features.0.0.weight: min=-0.029541438445448875, max=0.00607468094676733, mean=-0.0005715527222491801, std=0.003477835562080145\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.011829152703285217, max=0.013575206510722637, mean=6.638983336415549e-07, std=0.0012599825859069824\n",
      "Gradientes para classifier.1.weight: min=-0.00959955807775259, max=0.007406534627079964, mean=-3.492459576745488e-12, std=0.0014443902764469385\n",
      "0.0125\n",
      "Epoch 166/400\n",
      "Iteración 58115 - Lote 35/352 - Pérdida de Entrenamiento: 0.5848, Precisión de Entrenamiento: 0.7891\n",
      "Iteración 58150 - Lote 70/352 - Pérdida de Entrenamiento: 0.6174, Precisión de Entrenamiento: 0.7758\n",
      "Iteración 58185 - Lote 105/352 - Pérdida de Entrenamiento: 0.6242, Precisión de Entrenamiento: 0.7739\n",
      "Iteración 58220 - Lote 140/352 - Pérdida de Entrenamiento: 0.6284, Precisión de Entrenamiento: 0.7727\n",
      "Iteración 58255 - Lote 175/352 - Pérdida de Entrenamiento: 0.6335, Precisión de Entrenamiento: 0.7717\n",
      "Iteración 58290 - Lote 210/352 - Pérdida de Entrenamiento: 0.6328, Precisión de Entrenamiento: 0.7726\n",
      "Iteración 58325 - Lote 245/352 - Pérdida de Entrenamiento: 0.6301, Precisión de Entrenamiento: 0.7735\n",
      "Iteración 58360 - Lote 280/352 - Pérdida de Entrenamiento: 0.6333, Precisión de Entrenamiento: 0.7729\n",
      "Iteración 58395 - Lote 315/352 - Pérdida de Entrenamiento: 0.6515, Precisión de Entrenamiento: 0.7661\n",
      "Iteración 58430 - Lote 350/352 - Pérdida de Entrenamiento: 0.6594, Precisión de Entrenamiento: 0.7628\n",
      "Val loss: 0.9255, Val acc: 0.6830\n",
      "0.00625\n",
      "Epoch 167/400\n",
      "Iteración 58467 - Lote 35/352 - Pérdida de Entrenamiento: 0.6666, Precisión de Entrenamiento: 0.7554\n",
      "Iteración 58502 - Lote 70/352 - Pérdida de Entrenamiento: 0.6514, Precisión de Entrenamiento: 0.7653\n",
      "Iteración 58537 - Lote 105/352 - Pérdida de Entrenamiento: 0.6533, Precisión de Entrenamiento: 0.7652\n",
      "Iteración 58572 - Lote 140/352 - Pérdida de Entrenamiento: 0.6525, Precisión de Entrenamiento: 0.7648\n",
      "Iteración 58607 - Lote 175/352 - Pérdida de Entrenamiento: 0.6507, Precisión de Entrenamiento: 0.7654\n",
      "Iteración 58642 - Lote 210/352 - Pérdida de Entrenamiento: 0.6484, Precisión de Entrenamiento: 0.7656\n",
      "Iteración 58677 - Lote 245/352 - Pérdida de Entrenamiento: 0.6475, Precisión de Entrenamiento: 0.7654\n",
      "Iteración 58712 - Lote 280/352 - Pérdida de Entrenamiento: 0.6497, Precisión de Entrenamiento: 0.7647\n",
      "Iteración 58747 - Lote 315/352 - Pérdida de Entrenamiento: 0.6509, Precisión de Entrenamiento: 0.7644\n",
      "Iteración 58782 - Lote 350/352 - Pérdida de Entrenamiento: 0.6488, Precisión de Entrenamiento: 0.7653\n",
      "Val loss: 0.9093, Val acc: 0.6954\n",
      "0.00625\n",
      "Epoch 168/400\n",
      "Iteración 58819 - Lote 35/352 - Pérdida de Entrenamiento: 0.6222, Precisión de Entrenamiento: 0.7746\n",
      "Iteración 58854 - Lote 70/352 - Pérdida de Entrenamiento: 0.6173, Precisión de Entrenamiento: 0.7787\n",
      "Iteración 58889 - Lote 105/352 - Pérdida de Entrenamiento: 0.6258, Precisión de Entrenamiento: 0.7765\n",
      "Iteración 58924 - Lote 140/352 - Pérdida de Entrenamiento: 0.6229, Precisión de Entrenamiento: 0.7771\n",
      "Iteración 58959 - Lote 175/352 - Pérdida de Entrenamiento: 0.6227, Precisión de Entrenamiento: 0.7758\n",
      "Iteración 58994 - Lote 210/352 - Pérdida de Entrenamiento: 0.6187, Precisión de Entrenamiento: 0.7775\n",
      "Iteración 59029 - Lote 245/352 - Pérdida de Entrenamiento: 0.6166, Precisión de Entrenamiento: 0.7779\n",
      "Iteración 59064 - Lote 280/352 - Pérdida de Entrenamiento: 0.6185, Precisión de Entrenamiento: 0.7769\n",
      "Iteración 59099 - Lote 315/352 - Pérdida de Entrenamiento: 0.6180, Precisión de Entrenamiento: 0.7768\n",
      "Iteración 59134 - Lote 350/352 - Pérdida de Entrenamiento: 0.6209, Precisión de Entrenamiento: 0.7758\n",
      "Val loss: 0.9184, Val acc: 0.6950\n",
      "0.00625\n",
      "Epoch 169/400\n",
      "Iteración 59171 - Lote 35/352 - Pérdida de Entrenamiento: 0.6109, Precisión de Entrenamiento: 0.7757\n",
      "Iteración 59206 - Lote 70/352 - Pérdida de Entrenamiento: 0.6174, Precisión de Entrenamiento: 0.7748\n",
      "Iteración 59241 - Lote 105/352 - Pérdida de Entrenamiento: 0.6103, Precisión de Entrenamiento: 0.7766\n",
      "Iteración 59276 - Lote 140/352 - Pérdida de Entrenamiento: 0.6079, Precisión de Entrenamiento: 0.7776\n",
      "Iteración 59311 - Lote 175/352 - Pérdida de Entrenamiento: 0.6119, Precisión de Entrenamiento: 0.7774\n",
      "Iteración 59346 - Lote 210/352 - Pérdida de Entrenamiento: 0.6134, Precisión de Entrenamiento: 0.7776\n",
      "Iteración 59381 - Lote 245/352 - Pérdida de Entrenamiento: 0.6090, Precisión de Entrenamiento: 0.7790\n",
      "Iteración 59416 - Lote 280/352 - Pérdida de Entrenamiento: 0.6096, Precisión de Entrenamiento: 0.7797\n",
      "Iteración 59451 - Lote 315/352 - Pérdida de Entrenamiento: 0.6104, Precisión de Entrenamiento: 0.7800\n",
      "Iteración 59486 - Lote 350/352 - Pérdida de Entrenamiento: 0.6114, Precisión de Entrenamiento: 0.7798\n",
      "Val loss: 0.9199, Val acc: 0.6982\n",
      "0.00625\n",
      "Epoch 170/400\n",
      "Iteración 59523 - Lote 35/352 - Pérdida de Entrenamiento: 0.6101, Precisión de Entrenamiento: 0.7772\n",
      "Iteración 59558 - Lote 70/352 - Pérdida de Entrenamiento: 0.6113, Precisión de Entrenamiento: 0.7794\n",
      "Iteración 59593 - Lote 105/352 - Pérdida de Entrenamiento: 0.6069, Precisión de Entrenamiento: 0.7796\n",
      "Iteración 59628 - Lote 140/352 - Pérdida de Entrenamiento: 0.6068, Precisión de Entrenamiento: 0.7801\n",
      "Iteración 59663 - Lote 175/352 - Pérdida de Entrenamiento: 0.6045, Precisión de Entrenamiento: 0.7815\n",
      "Iteración 59698 - Lote 210/352 - Pérdida de Entrenamiento: 0.6050, Precisión de Entrenamiento: 0.7820\n",
      "Iteración 59733 - Lote 245/352 - Pérdida de Entrenamiento: 0.6076, Precisión de Entrenamiento: 0.7800\n",
      "Iteración 59768 - Lote 280/352 - Pérdida de Entrenamiento: 0.6109, Precisión de Entrenamiento: 0.7784\n",
      "Iteración 59803 - Lote 315/352 - Pérdida de Entrenamiento: 0.6108, Precisión de Entrenamiento: 0.7791\n",
      "Iteración 59838 - Lote 350/352 - Pérdida de Entrenamiento: 0.6104, Precisión de Entrenamiento: 0.7790\n",
      "Val loss: 0.9301, Val acc: 0.6924\n",
      "Gradientes para features.0.0.weight: min=-0.004058543127030134, max=0.020933695137500763, mean=0.0004955488839186728, std=0.0030002675484865904\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.014650896191596985, max=0.01578943431377411, mean=-1.2800430340575986e-05, std=0.0014326220843940973\n",
      "Gradientes para classifier.1.weight: min=-0.012508333660662174, max=0.011325797066092491, mean=1.280568533157389e-11, std=0.0019015102880075574\n",
      "0.00625\n",
      "Epoch 171/400\n",
      "Iteración 59875 - Lote 35/352 - Pérdida de Entrenamiento: 0.5847, Precisión de Entrenamiento: 0.7920\n",
      "Iteración 59910 - Lote 70/352 - Pérdida de Entrenamiento: 0.5986, Precisión de Entrenamiento: 0.7842\n",
      "Iteración 59945 - Lote 105/352 - Pérdida de Entrenamiento: 0.6073, Precisión de Entrenamiento: 0.7807\n",
      "Iteración 59980 - Lote 140/352 - Pérdida de Entrenamiento: 0.6061, Precisión de Entrenamiento: 0.7806\n",
      "Iteración 60015 - Lote 175/352 - Pérdida de Entrenamiento: 0.5999, Precisión de Entrenamiento: 0.7823\n",
      "Iteración 60050 - Lote 210/352 - Pérdida de Entrenamiento: 0.5984, Precisión de Entrenamiento: 0.7827\n",
      "Iteración 60085 - Lote 245/352 - Pérdida de Entrenamiento: 0.6010, Precisión de Entrenamiento: 0.7814\n",
      "Iteración 60120 - Lote 280/352 - Pérdida de Entrenamiento: 0.6051, Precisión de Entrenamiento: 0.7802\n",
      "Iteración 60155 - Lote 315/352 - Pérdida de Entrenamiento: 0.6042, Precisión de Entrenamiento: 0.7812\n",
      "Iteración 60190 - Lote 350/352 - Pérdida de Entrenamiento: 0.6030, Precisión de Entrenamiento: 0.7821\n",
      "Val loss: 0.9284, Val acc: 0.6968\n",
      "0.00625\n",
      "Epoch 172/400\n",
      "Iteración 60227 - Lote 35/352 - Pérdida de Entrenamiento: 0.5920, Precisión de Entrenamiento: 0.7806\n",
      "Iteración 60262 - Lote 70/352 - Pérdida de Entrenamiento: 0.5935, Precisión de Entrenamiento: 0.7833\n",
      "Iteración 60297 - Lote 105/352 - Pérdida de Entrenamiento: 0.5925, Precisión de Entrenamiento: 0.7829\n",
      "Iteración 60332 - Lote 140/352 - Pérdida de Entrenamiento: 0.5865, Precisión de Entrenamiento: 0.7852\n",
      "Iteración 60367 - Lote 175/352 - Pérdida de Entrenamiento: 0.5894, Precisión de Entrenamiento: 0.7851\n",
      "Iteración 60402 - Lote 210/352 - Pérdida de Entrenamiento: 0.5933, Precisión de Entrenamiento: 0.7850\n",
      "Iteración 60437 - Lote 245/352 - Pérdida de Entrenamiento: 0.5925, Precisión de Entrenamiento: 0.7846\n",
      "Iteración 60472 - Lote 280/352 - Pérdida de Entrenamiento: 0.5943, Precisión de Entrenamiento: 0.7844\n",
      "Iteración 60507 - Lote 315/352 - Pérdida de Entrenamiento: 0.5949, Precisión de Entrenamiento: 0.7841\n",
      "Iteración 60542 - Lote 350/352 - Pérdida de Entrenamiento: 0.5953, Precisión de Entrenamiento: 0.7834\n",
      "Val loss: 0.9338, Val acc: 0.6958\n",
      "0.00625\n",
      "Epoch 173/400\n",
      "Iteración 60579 - Lote 35/352 - Pérdida de Entrenamiento: 0.6035, Precisión de Entrenamiento: 0.7748\n",
      "Iteración 60614 - Lote 70/352 - Pérdida de Entrenamiento: 0.6164, Precisión de Entrenamiento: 0.7740\n",
      "Iteración 60649 - Lote 105/352 - Pérdida de Entrenamiento: 0.6080, Precisión de Entrenamiento: 0.7774\n",
      "Iteración 60684 - Lote 140/352 - Pérdida de Entrenamiento: 0.6067, Precisión de Entrenamiento: 0.7782\n",
      "Iteración 60719 - Lote 175/352 - Pérdida de Entrenamiento: 0.6011, Precisión de Entrenamiento: 0.7803\n",
      "Iteración 60754 - Lote 210/352 - Pérdida de Entrenamiento: 0.6004, Precisión de Entrenamiento: 0.7811\n",
      "Iteración 60789 - Lote 245/352 - Pérdida de Entrenamiento: 0.5988, Precisión de Entrenamiento: 0.7827\n",
      "Iteración 60824 - Lote 280/352 - Pérdida de Entrenamiento: 0.5996, Precisión de Entrenamiento: 0.7824\n",
      "Iteración 60859 - Lote 315/352 - Pérdida de Entrenamiento: 0.5997, Precisión de Entrenamiento: 0.7822\n",
      "Iteración 60894 - Lote 350/352 - Pérdida de Entrenamiento: 0.5999, Precisión de Entrenamiento: 0.7817\n",
      "Val loss: 0.9348, Val acc: 0.6934\n",
      "Gradientes para features.0.0.weight: min=-0.0165354385972023, max=0.04785269498825073, mean=0.0007763133035041392, std=0.005602425895631313\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.01220419630408287, max=0.014445441775023937, mean=-1.3694571862288285e-05, std=0.001576147391460836\n",
      "Gradientes para classifier.1.weight: min=-0.01471109688282013, max=0.011171006597578526, mean=1.8626451075975936e-11, std=0.001903138356283307\n",
      "Early stopping at epoch 173\n",
      "\n",
      "Tiempo total de entrenamiento: 6422.99 segundos\n",
      "\n",
      "Entrenamiento completado exitosamente\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento - 2024-12-02 00:15:55\n",
      "Configuración del entrenamiento:\n",
      "Batch size: 128\n",
      "Epochs: 400\n",
      "Learning rate: 0.1\n",
      "Weight decay: 0.001\n",
      "Data augmentation: True\n",
      "GPU disponible: True\n",
      "--------------------------------------------------\n",
      "\n",
      "Arquitectura del modelo:\n",
      "EfficientNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
      "      )\n",
      "      (1): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
      "      )\n",
      "      (2): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
      "      )\n",
      "      (3): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): MBConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2dNormActivation(\n",
      "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): SiLU(inplace=True)\n",
      "            (scale_activation): Sigmoid()\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
      "      )\n",
      "    )\n",
      "    (8): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=True)\n",
      "    (1): Linear(in_features=1280, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "--------------------------------------------------\n",
      "Using train transform: Compose(\n",
      "    RandomCrop(size=(32, 32), padding=4)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))\n",
      ")\n",
      "Using validation transform: Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))\n",
      ")\n",
      "0.1\n",
      "Epoch 1/400\n",
      "Iteración 35 - Lote 35/352 - Pérdida de Entrenamiento: 5.3808, Precisión de Entrenamiento: 0.1027\n",
      "Iteración 70 - Lote 70/352 - Pérdida de Entrenamiento: 4.7617, Precisión de Entrenamiento: 0.1061\n",
      "Iteración 105 - Lote 105/352 - Pérdida de Entrenamiento: 4.3424, Precisión de Entrenamiento: 0.1069\n",
      "Iteración 140 - Lote 140/352 - Pérdida de Entrenamiento: 4.0613, Precisión de Entrenamiento: 0.1055\n",
      "Iteración 175 - Lote 175/352 - Pérdida de Entrenamiento: 3.8238, Precisión de Entrenamiento: 0.1058\n",
      "Iteración 210 - Lote 210/352 - Pérdida de Entrenamiento: 3.6231, Precisión de Entrenamiento: 0.1059\n",
      "Iteración 245 - Lote 245/352 - Pérdida de Entrenamiento: 3.4667, Precisión de Entrenamiento: 0.1063\n",
      "Iteración 280 - Lote 280/352 - Pérdida de Entrenamiento: 3.3405, Precisión de Entrenamiento: 0.1053\n",
      "Iteración 315 - Lote 315/352 - Pérdida de Entrenamiento: 3.2377, Precisión de Entrenamiento: 0.1066\n",
      "Iteración 350 - Lote 350/352 - Pérdida de Entrenamiento: 3.1487, Precisión de Entrenamiento: 0.1089\n",
      "Val loss: 2.2817, Val acc: 0.1252\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_1.pth\n",
      "Checkpoint del mejor modelo guardado en la época 1\n",
      "Gradientes para features.0.0.weight: min=-0.0008195940754376352, max=0.005164196714758873, mean=0.0001034245069604367, std=0.0005440021050162613\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.002571833087131381, max=0.0022036631125956774, mean=1.4401992132206942e-07, std=7.614562491653487e-05\n",
      "Gradientes para classifier.1.weight: min=-0.04542435705661774, max=0.040624529123306274, mean=3.725290215195187e-11, std=0.0034084839280694723\n",
      "0.1\n",
      "Epoch 2/400\n",
      "Iteración 387 - Lote 35/352 - Pérdida de Entrenamiento: 2.3092, Precisión de Entrenamiento: 0.1402\n",
      "Iteración 422 - Lote 70/352 - Pérdida de Entrenamiento: 2.2937, Precisión de Entrenamiento: 0.1415\n",
      "Iteración 457 - Lote 105/352 - Pérdida de Entrenamiento: 2.2837, Precisión de Entrenamiento: 0.1443\n",
      "Iteración 492 - Lote 140/352 - Pérdida de Entrenamiento: 2.2721, Precisión de Entrenamiento: 0.1436\n",
      "Iteración 527 - Lote 175/352 - Pérdida de Entrenamiento: 2.2639, Precisión de Entrenamiento: 0.1452\n",
      "Iteración 562 - Lote 210/352 - Pérdida de Entrenamiento: 2.2572, Precisión de Entrenamiento: 0.1472\n",
      "Iteración 597 - Lote 245/352 - Pérdida de Entrenamiento: 2.2503, Precisión de Entrenamiento: 0.1482\n",
      "Iteración 632 - Lote 280/352 - Pérdida de Entrenamiento: 2.2437, Precisión de Entrenamiento: 0.1495\n",
      "Iteración 667 - Lote 315/352 - Pérdida de Entrenamiento: 2.2389, Precisión de Entrenamiento: 0.1512\n",
      "Iteración 702 - Lote 350/352 - Pérdida de Entrenamiento: 2.2352, Precisión de Entrenamiento: 0.1517\n",
      "Val loss: 2.2246, Val acc: 0.1678\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_2.pth\n",
      "Checkpoint del mejor modelo guardado en la época 2\n",
      "0.1\n",
      "Epoch 3/400\n",
      "Iteración 739 - Lote 35/352 - Pérdida de Entrenamiento: 2.1753, Precisión de Entrenamiento: 0.1670\n",
      "Iteración 774 - Lote 70/352 - Pérdida de Entrenamiento: 2.1747, Precisión de Entrenamiento: 0.1693\n",
      "Iteración 809 - Lote 105/352 - Pérdida de Entrenamiento: 2.1725, Precisión de Entrenamiento: 0.1705\n",
      "Iteración 844 - Lote 140/352 - Pérdida de Entrenamiento: 2.1683, Precisión de Entrenamiento: 0.1734\n",
      "Iteración 879 - Lote 175/352 - Pérdida de Entrenamiento: 2.1599, Precisión de Entrenamiento: 0.1743\n",
      "Iteración 914 - Lote 210/352 - Pérdida de Entrenamiento: 2.1528, Precisión de Entrenamiento: 0.1758\n",
      "Iteración 949 - Lote 245/352 - Pérdida de Entrenamiento: 2.1438, Precisión de Entrenamiento: 0.1791\n",
      "Iteración 984 - Lote 280/352 - Pérdida de Entrenamiento: 2.1332, Precisión de Entrenamiento: 0.1811\n",
      "Iteración 1019 - Lote 315/352 - Pérdida de Entrenamiento: 2.1229, Precisión de Entrenamiento: 0.1830\n",
      "Iteración 1054 - Lote 350/352 - Pérdida de Entrenamiento: 2.1164, Precisión de Entrenamiento: 0.1850\n",
      "Val loss: 2.0585, Val acc: 0.1930\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_3.pth\n",
      "Checkpoint del mejor modelo guardado en la época 3\n",
      "0.1\n",
      "Epoch 4/400\n",
      "Iteración 1091 - Lote 35/352 - Pérdida de Entrenamiento: 1.9951, Precisión de Entrenamiento: 0.2208\n",
      "Iteración 1126 - Lote 70/352 - Pérdida de Entrenamiento: 1.9961, Precisión de Entrenamiento: 0.2192\n",
      "Iteración 1161 - Lote 105/352 - Pérdida de Entrenamiento: 1.9914, Precisión de Entrenamiento: 0.2202\n",
      "Iteración 1196 - Lote 140/352 - Pérdida de Entrenamiento: 1.9871, Precisión de Entrenamiento: 0.2203\n",
      "Iteración 1231 - Lote 175/352 - Pérdida de Entrenamiento: 1.9813, Precisión de Entrenamiento: 0.2235\n",
      "Iteración 1266 - Lote 210/352 - Pérdida de Entrenamiento: 1.9759, Precisión de Entrenamiento: 0.2244\n",
      "Iteración 1301 - Lote 245/352 - Pérdida de Entrenamiento: 1.9693, Precisión de Entrenamiento: 0.2263\n",
      "Iteración 1336 - Lote 280/352 - Pérdida de Entrenamiento: 1.9636, Precisión de Entrenamiento: 0.2289\n",
      "Iteración 1371 - Lote 315/352 - Pérdida de Entrenamiento: 1.9600, Precisión de Entrenamiento: 0.2316\n",
      "Iteración 1406 - Lote 350/352 - Pérdida de Entrenamiento: 1.9560, Precisión de Entrenamiento: 0.2333\n",
      "Val loss: 2.0407, Val acc: 0.2284\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_4.pth\n",
      "Checkpoint del mejor modelo guardado en la época 4\n",
      "0.1\n",
      "Epoch 5/400\n",
      "Iteración 1443 - Lote 35/352 - Pérdida de Entrenamiento: 1.8735, Precisión de Entrenamiento: 0.2734\n",
      "Iteración 1478 - Lote 70/352 - Pérdida de Entrenamiento: 1.8733, Precisión de Entrenamiento: 0.2834\n",
      "Iteración 1513 - Lote 105/352 - Pérdida de Entrenamiento: 1.8668, Precisión de Entrenamiento: 0.2847\n",
      "Iteración 1548 - Lote 140/352 - Pérdida de Entrenamiento: 1.8569, Precisión de Entrenamiento: 0.2879\n",
      "Iteración 1583 - Lote 175/352 - Pérdida de Entrenamiento: 1.8520, Precisión de Entrenamiento: 0.2889\n",
      "Iteración 1618 - Lote 210/352 - Pérdida de Entrenamiento: 1.8439, Precisión de Entrenamiento: 0.2915\n",
      "Iteración 1653 - Lote 245/352 - Pérdida de Entrenamiento: 1.8401, Precisión de Entrenamiento: 0.2928\n",
      "Iteración 1688 - Lote 280/352 - Pérdida de Entrenamiento: 1.8339, Precisión de Entrenamiento: 0.2957\n",
      "Iteración 1723 - Lote 315/352 - Pérdida de Entrenamiento: 1.8274, Precisión de Entrenamiento: 0.2977\n",
      "Iteración 1758 - Lote 350/352 - Pérdida de Entrenamiento: 1.8202, Precisión de Entrenamiento: 0.2995\n",
      "Val loss: 1.7865, Val acc: 0.3394\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_5.pth\n",
      "Checkpoint del mejor modelo guardado en la época 5\n",
      "Gradientes para features.0.0.weight: min=-0.14815178513526917, max=0.05860837176442146, mean=-0.0016734986566007137, std=0.015229035168886185\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.036982759833335876, max=0.029682625085115433, mean=-6.36982440482825e-06, std=0.0007391786202788353\n",
      "Gradientes para classifier.1.weight: min=-0.021733062341809273, max=0.019907886162400246, mean=2.328306384496992e-12, std=0.002382443519309163\n",
      "0.1\n",
      "Epoch 6/400\n",
      "Iteración 1795 - Lote 35/352 - Pérdida de Entrenamiento: 1.7805, Precisión de Entrenamiento: 0.3248\n",
      "Iteración 1830 - Lote 70/352 - Pérdida de Entrenamiento: 1.7570, Precisión de Entrenamiento: 0.3350\n",
      "Iteración 1865 - Lote 105/352 - Pérdida de Entrenamiento: 1.7603, Precisión de Entrenamiento: 0.3347\n",
      "Iteración 1900 - Lote 140/352 - Pérdida de Entrenamiento: 1.7463, Precisión de Entrenamiento: 0.3387\n",
      "Iteración 1935 - Lote 175/352 - Pérdida de Entrenamiento: 1.7444, Precisión de Entrenamiento: 0.3408\n",
      "Iteración 1970 - Lote 210/352 - Pérdida de Entrenamiento: 1.7344, Precisión de Entrenamiento: 0.3445\n",
      "Iteración 2005 - Lote 245/352 - Pérdida de Entrenamiento: 1.7294, Precisión de Entrenamiento: 0.3473\n",
      "Iteración 2040 - Lote 280/352 - Pérdida de Entrenamiento: 1.7209, Precisión de Entrenamiento: 0.3519\n",
      "Iteración 2075 - Lote 315/352 - Pérdida de Entrenamiento: 1.7151, Precisión de Entrenamiento: 0.3529\n",
      "Iteración 2110 - Lote 350/352 - Pérdida de Entrenamiento: 1.7108, Precisión de Entrenamiento: 0.3537\n",
      "Val loss: 1.7080, Val acc: 0.3564\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_6.pth\n",
      "Checkpoint del mejor modelo guardado en la época 6\n",
      "0.1\n",
      "Epoch 7/400\n",
      "Iteración 2147 - Lote 35/352 - Pérdida de Entrenamiento: 1.6439, Precisión de Entrenamiento: 0.3790\n",
      "Iteración 2182 - Lote 70/352 - Pérdida de Entrenamiento: 1.6277, Precisión de Entrenamiento: 0.3866\n",
      "Iteración 2217 - Lote 105/352 - Pérdida de Entrenamiento: 1.6320, Precisión de Entrenamiento: 0.3835\n",
      "Iteración 2252 - Lote 140/352 - Pérdida de Entrenamiento: 1.6253, Precisión de Entrenamiento: 0.3871\n",
      "Iteración 2287 - Lote 175/352 - Pérdida de Entrenamiento: 1.6234, Precisión de Entrenamiento: 0.3879\n",
      "Iteración 2322 - Lote 210/352 - Pérdida de Entrenamiento: 1.6230, Precisión de Entrenamiento: 0.3883\n",
      "Iteración 2357 - Lote 245/352 - Pérdida de Entrenamiento: 1.6203, Precisión de Entrenamiento: 0.3882\n",
      "Iteración 2392 - Lote 280/352 - Pérdida de Entrenamiento: 1.6187, Precisión de Entrenamiento: 0.3891\n",
      "Iteración 2427 - Lote 315/352 - Pérdida de Entrenamiento: 1.6140, Precisión de Entrenamiento: 0.3921\n",
      "Iteración 2462 - Lote 350/352 - Pérdida de Entrenamiento: 1.6092, Precisión de Entrenamiento: 0.3943\n",
      "Val loss: 1.6343, Val acc: 0.3992\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_7.pth\n",
      "Checkpoint del mejor modelo guardado en la época 7\n",
      "0.1\n",
      "Epoch 8/400\n",
      "Iteración 2499 - Lote 35/352 - Pérdida de Entrenamiento: 1.5816, Precisión de Entrenamiento: 0.4067\n",
      "Iteración 2534 - Lote 70/352 - Pérdida de Entrenamiento: 1.5633, Precisión de Entrenamiento: 0.4141\n",
      "Iteración 2569 - Lote 105/352 - Pérdida de Entrenamiento: 1.5617, Precisión de Entrenamiento: 0.4138\n",
      "Iteración 2604 - Lote 140/352 - Pérdida de Entrenamiento: 1.5657, Precisión de Entrenamiento: 0.4124\n",
      "Iteración 2639 - Lote 175/352 - Pérdida de Entrenamiento: 1.5618, Precisión de Entrenamiento: 0.4174\n",
      "Iteración 2674 - Lote 210/352 - Pérdida de Entrenamiento: 1.5594, Precisión de Entrenamiento: 0.4172\n",
      "Iteración 2709 - Lote 245/352 - Pérdida de Entrenamiento: 1.5585, Precisión de Entrenamiento: 0.4177\n",
      "Iteración 2744 - Lote 280/352 - Pérdida de Entrenamiento: 1.5546, Precisión de Entrenamiento: 0.4194\n",
      "Iteración 2779 - Lote 315/352 - Pérdida de Entrenamiento: 1.5537, Precisión de Entrenamiento: 0.4212\n",
      "Iteración 2814 - Lote 350/352 - Pérdida de Entrenamiento: 1.5517, Precisión de Entrenamiento: 0.4213\n",
      "Val loss: 1.7282, Val acc: 0.3400\n",
      "0.1\n",
      "Epoch 9/400\n",
      "Iteración 2851 - Lote 35/352 - Pérdida de Entrenamiento: 1.5510, Precisión de Entrenamiento: 0.4299\n",
      "Iteración 2886 - Lote 70/352 - Pérdida de Entrenamiento: 1.5344, Precisión de Entrenamiento: 0.4330\n",
      "Iteración 2921 - Lote 105/352 - Pérdida de Entrenamiento: 1.5147, Precisión de Entrenamiento: 0.4414\n",
      "Iteración 2956 - Lote 140/352 - Pérdida de Entrenamiento: 1.5149, Precisión de Entrenamiento: 0.4411\n",
      "Iteración 2991 - Lote 175/352 - Pérdida de Entrenamiento: 1.5090, Precisión de Entrenamiento: 0.4430\n",
      "Iteración 3026 - Lote 210/352 - Pérdida de Entrenamiento: 1.5090, Precisión de Entrenamiento: 0.4426\n",
      "Iteración 3061 - Lote 245/352 - Pérdida de Entrenamiento: 1.5037, Precisión de Entrenamiento: 0.4444\n",
      "Iteración 3096 - Lote 280/352 - Pérdida de Entrenamiento: 1.5014, Precisión de Entrenamiento: 0.4469\n",
      "Iteración 3131 - Lote 315/352 - Pérdida de Entrenamiento: 1.4948, Precisión de Entrenamiento: 0.4494\n",
      "Iteración 3166 - Lote 350/352 - Pérdida de Entrenamiento: 1.4924, Precisión de Entrenamiento: 0.4504\n",
      "Val loss: 1.4965, Val acc: 0.4522\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_9.pth\n",
      "Checkpoint del mejor modelo guardado en la época 9\n",
      "0.1\n",
      "Epoch 10/400\n",
      "Iteración 3203 - Lote 35/352 - Pérdida de Entrenamiento: 1.4569, Precisión de Entrenamiento: 0.4728\n",
      "Iteración 3238 - Lote 70/352 - Pérdida de Entrenamiento: 1.4516, Precisión de Entrenamiento: 0.4695\n",
      "Iteración 3273 - Lote 105/352 - Pérdida de Entrenamiento: 1.4491, Precisión de Entrenamiento: 0.4711\n",
      "Iteración 3308 - Lote 140/352 - Pérdida de Entrenamiento: 1.4450, Precisión de Entrenamiento: 0.4734\n",
      "Iteración 3343 - Lote 175/352 - Pérdida de Entrenamiento: 1.4440, Precisión de Entrenamiento: 0.4729\n",
      "Iteración 3378 - Lote 210/352 - Pérdida de Entrenamiento: 1.4376, Precisión de Entrenamiento: 0.4744\n",
      "Iteración 3413 - Lote 245/352 - Pérdida de Entrenamiento: 1.4442, Precisión de Entrenamiento: 0.4726\n",
      "Iteración 3448 - Lote 280/352 - Pérdida de Entrenamiento: 1.4423, Precisión de Entrenamiento: 0.4717\n",
      "Iteración 3483 - Lote 315/352 - Pérdida de Entrenamiento: 1.4405, Precisión de Entrenamiento: 0.4716\n",
      "Iteración 3518 - Lote 350/352 - Pérdida de Entrenamiento: 1.4378, Precisión de Entrenamiento: 0.4738\n",
      "Val loss: 1.5786, Val acc: 0.4410\n",
      "Gradientes para features.0.0.weight: min=-0.04452809318900108, max=0.055577877908945084, mean=0.00022272217029239982, std=0.010044483467936516\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.019210344180464745, max=0.02578892558813095, mean=-1.069174595613731e-05, std=0.0011015919735655189\n",
      "Gradientes para classifier.1.weight: min=-0.011242480017244816, max=0.014838517643511295, mean=0.0, std=0.0019637332297861576\n",
      "0.1\n",
      "Epoch 11/400\n",
      "Iteración 3555 - Lote 35/352 - Pérdida de Entrenamiento: 1.4286, Precisión de Entrenamiento: 0.4719\n",
      "Iteración 3590 - Lote 70/352 - Pérdida de Entrenamiento: 1.4134, Precisión de Entrenamiento: 0.4817\n",
      "Iteración 3625 - Lote 105/352 - Pérdida de Entrenamiento: 1.4113, Precisión de Entrenamiento: 0.4817\n",
      "Iteración 3660 - Lote 140/352 - Pérdida de Entrenamiento: 1.4135, Precisión de Entrenamiento: 0.4800\n",
      "Iteración 3695 - Lote 175/352 - Pérdida de Entrenamiento: 1.4086, Precisión de Entrenamiento: 0.4829\n",
      "Iteración 3730 - Lote 210/352 - Pérdida de Entrenamiento: 1.4016, Precisión de Entrenamiento: 0.4861\n",
      "Iteración 3765 - Lote 245/352 - Pérdida de Entrenamiento: 1.4026, Precisión de Entrenamiento: 0.4858\n",
      "Iteración 3800 - Lote 280/352 - Pérdida de Entrenamiento: 1.3972, Precisión de Entrenamiento: 0.4879\n",
      "Iteración 3835 - Lote 315/352 - Pérdida de Entrenamiento: 1.3940, Precisión de Entrenamiento: 0.4883\n",
      "Iteración 3870 - Lote 350/352 - Pérdida de Entrenamiento: 1.3957, Precisión de Entrenamiento: 0.4873\n",
      "Val loss: 1.6795, Val acc: 0.3964\n",
      "0.1\n",
      "Epoch 12/400\n",
      "Iteración 3907 - Lote 35/352 - Pérdida de Entrenamiento: 1.3924, Precisión de Entrenamiento: 0.4891\n",
      "Iteración 3942 - Lote 70/352 - Pérdida de Entrenamiento: 1.3900, Precisión de Entrenamiento: 0.4876\n",
      "Iteración 3977 - Lote 105/352 - Pérdida de Entrenamiento: 1.3840, Precisión de Entrenamiento: 0.4927\n",
      "Iteración 4012 - Lote 140/352 - Pérdida de Entrenamiento: 1.3807, Precisión de Entrenamiento: 0.4968\n",
      "Iteración 4047 - Lote 175/352 - Pérdida de Entrenamiento: 1.3745, Precisión de Entrenamiento: 0.5002\n",
      "Iteración 4082 - Lote 210/352 - Pérdida de Entrenamiento: 1.3712, Precisión de Entrenamiento: 0.5012\n",
      "Iteración 4117 - Lote 245/352 - Pérdida de Entrenamiento: 1.3666, Precisión de Entrenamiento: 0.5031\n",
      "Iteración 4152 - Lote 280/352 - Pérdida de Entrenamiento: 1.3650, Precisión de Entrenamiento: 0.5041\n",
      "Iteración 4187 - Lote 315/352 - Pérdida de Entrenamiento: 1.3618, Precisión de Entrenamiento: 0.5053\n",
      "Iteración 4222 - Lote 350/352 - Pérdida de Entrenamiento: 1.3646, Precisión de Entrenamiento: 0.5045\n",
      "Val loss: 1.3744, Val acc: 0.5112\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_12.pth\n",
      "Checkpoint del mejor modelo guardado en la época 12\n",
      "0.1\n",
      "Epoch 13/400\n",
      "Iteración 4259 - Lote 35/352 - Pérdida de Entrenamiento: 1.3186, Precisión de Entrenamiento: 0.5185\n",
      "Iteración 4294 - Lote 70/352 - Pérdida de Entrenamiento: 1.3279, Precisión de Entrenamiento: 0.5184\n",
      "Iteración 4329 - Lote 105/352 - Pérdida de Entrenamiento: 1.3323, Precisión de Entrenamiento: 0.5174\n",
      "Iteración 4364 - Lote 140/352 - Pérdida de Entrenamiento: 1.3488, Precisión de Entrenamiento: 0.5104\n",
      "Iteración 4399 - Lote 175/352 - Pérdida de Entrenamiento: 1.3497, Precisión de Entrenamiento: 0.5105\n",
      "Iteración 4434 - Lote 210/352 - Pérdida de Entrenamiento: 1.3482, Precisión de Entrenamiento: 0.5105\n",
      "Iteración 4469 - Lote 245/352 - Pérdida de Entrenamiento: 1.3447, Precisión de Entrenamiento: 0.5119\n",
      "Iteración 4504 - Lote 280/352 - Pérdida de Entrenamiento: 1.3401, Precisión de Entrenamiento: 0.5130\n",
      "Iteración 4539 - Lote 315/352 - Pérdida de Entrenamiento: 1.3399, Precisión de Entrenamiento: 0.5126\n",
      "Iteración 4574 - Lote 350/352 - Pérdida de Entrenamiento: 1.3383, Precisión de Entrenamiento: 0.5136\n",
      "Val loss: 1.3437, Val acc: 0.5230\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_13.pth\n",
      "Checkpoint del mejor modelo guardado en la época 13\n",
      "0.1\n",
      "Epoch 14/400\n",
      "Iteración 4611 - Lote 35/352 - Pérdida de Entrenamiento: 1.3177, Precisión de Entrenamiento: 0.5292\n",
      "Iteración 4646 - Lote 70/352 - Pérdida de Entrenamiento: 1.2989, Precisión de Entrenamiento: 0.5302\n",
      "Iteración 4681 - Lote 105/352 - Pérdida de Entrenamiento: 1.3120, Precisión de Entrenamiento: 0.5289\n",
      "Iteración 4716 - Lote 140/352 - Pérdida de Entrenamiento: 1.3194, Precisión de Entrenamiento: 0.5235\n",
      "Iteración 4751 - Lote 175/352 - Pérdida de Entrenamiento: 1.3184, Precisión de Entrenamiento: 0.5228\n",
      "Iteración 4786 - Lote 210/352 - Pérdida de Entrenamiento: 1.3224, Precisión de Entrenamiento: 0.5221\n",
      "Iteración 4821 - Lote 245/352 - Pérdida de Entrenamiento: 1.3180, Precisión de Entrenamiento: 0.5245\n",
      "Iteración 4856 - Lote 280/352 - Pérdida de Entrenamiento: 1.3191, Precisión de Entrenamiento: 0.5239\n",
      "Iteración 4891 - Lote 315/352 - Pérdida de Entrenamiento: 1.3158, Precisión de Entrenamiento: 0.5251\n",
      "Iteración 4926 - Lote 350/352 - Pérdida de Entrenamiento: 1.3147, Precisión de Entrenamiento: 0.5242\n",
      "Val loss: 1.3193, Val acc: 0.5292\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_14.pth\n",
      "Checkpoint del mejor modelo guardado en la época 14\n",
      "0.1\n",
      "Epoch 15/400\n",
      "Iteración 4963 - Lote 35/352 - Pérdida de Entrenamiento: 1.3335, Precisión de Entrenamiento: 0.5172\n",
      "Iteración 4998 - Lote 70/352 - Pérdida de Entrenamiento: 1.3001, Precisión de Entrenamiento: 0.5262\n",
      "Iteración 5033 - Lote 105/352 - Pérdida de Entrenamiento: 1.3120, Precisión de Entrenamiento: 0.5221\n",
      "Iteración 5068 - Lote 140/352 - Pérdida de Entrenamiento: 1.3191, Precisión de Entrenamiento: 0.5193\n",
      "Iteración 5103 - Lote 175/352 - Pérdida de Entrenamiento: 1.3176, Precisión de Entrenamiento: 0.5204\n",
      "Iteración 5138 - Lote 210/352 - Pérdida de Entrenamiento: 1.3115, Precisión de Entrenamiento: 0.5249\n",
      "Iteración 5173 - Lote 245/352 - Pérdida de Entrenamiento: 1.3088, Precisión de Entrenamiento: 0.5257\n",
      "Iteración 5208 - Lote 280/352 - Pérdida de Entrenamiento: 1.3053, Precisión de Entrenamiento: 0.5264\n",
      "Iteración 5243 - Lote 315/352 - Pérdida de Entrenamiento: 1.3046, Precisión de Entrenamiento: 0.5267\n",
      "Iteración 5278 - Lote 350/352 - Pérdida de Entrenamiento: 1.3042, Precisión de Entrenamiento: 0.5270\n",
      "Val loss: 1.2284, Val acc: 0.5604\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_15.pth\n",
      "Checkpoint del mejor modelo guardado en la época 15\n",
      "Gradientes para features.0.0.weight: min=-0.07306237518787384, max=0.029732273891568184, mean=-0.0010967584094032645, std=0.00980256125330925\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.016864052042365074, max=0.024676764383912086, mean=1.0543483767833095e-05, std=0.0009937194408848882\n",
      "Gradientes para classifier.1.weight: min=-0.012545264326035976, max=0.011012938804924488, mean=-2.328306384496992e-12, std=0.0017420361982658505\n",
      "0.1\n",
      "Epoch 16/400\n",
      "Iteración 5315 - Lote 35/352 - Pérdida de Entrenamiento: 1.2682, Precisión de Entrenamiento: 0.5411\n",
      "Iteración 5350 - Lote 70/352 - Pérdida de Entrenamiento: 1.2601, Precisión de Entrenamiento: 0.5463\n",
      "Iteración 5385 - Lote 105/352 - Pérdida de Entrenamiento: 1.2676, Precisión de Entrenamiento: 0.5432\n",
      "Iteración 5420 - Lote 140/352 - Pérdida de Entrenamiento: 1.2788, Precisión de Entrenamiento: 0.5406\n",
      "Iteración 5455 - Lote 175/352 - Pérdida de Entrenamiento: 1.2836, Precisión de Entrenamiento: 0.5398\n",
      "Iteración 5490 - Lote 210/352 - Pérdida de Entrenamiento: 1.2832, Precisión de Entrenamiento: 0.5393\n",
      "Iteración 5525 - Lote 245/352 - Pérdida de Entrenamiento: 1.2808, Precisión de Entrenamiento: 0.5415\n",
      "Iteración 5560 - Lote 280/352 - Pérdida de Entrenamiento: 1.2797, Precisión de Entrenamiento: 0.5405\n",
      "Iteración 5595 - Lote 315/352 - Pérdida de Entrenamiento: 1.2798, Precisión de Entrenamiento: 0.5404\n",
      "Iteración 5630 - Lote 350/352 - Pérdida de Entrenamiento: 1.2786, Precisión de Entrenamiento: 0.5408\n",
      "Val loss: 1.3148, Val acc: 0.5310\n",
      "0.1\n",
      "Epoch 17/400\n",
      "Iteración 5667 - Lote 35/352 - Pérdida de Entrenamiento: 1.2553, Precisión de Entrenamiento: 0.5393\n",
      "Iteración 5702 - Lote 70/352 - Pérdida de Entrenamiento: 1.2602, Precisión de Entrenamiento: 0.5448\n",
      "Iteración 5737 - Lote 105/352 - Pérdida de Entrenamiento: 1.2562, Precisión de Entrenamiento: 0.5450\n",
      "Iteración 5772 - Lote 140/352 - Pérdida de Entrenamiento: 1.2584, Precisión de Entrenamiento: 0.5443\n",
      "Iteración 5807 - Lote 175/352 - Pérdida de Entrenamiento: 1.2509, Precisión de Entrenamiento: 0.5480\n",
      "Iteración 5842 - Lote 210/352 - Pérdida de Entrenamiento: 1.2526, Precisión de Entrenamiento: 0.5485\n",
      "Iteración 5877 - Lote 245/352 - Pérdida de Entrenamiento: 1.2488, Precisión de Entrenamiento: 0.5500\n",
      "Iteración 5912 - Lote 280/352 - Pérdida de Entrenamiento: 1.2486, Precisión de Entrenamiento: 0.5501\n",
      "Iteración 5947 - Lote 315/352 - Pérdida de Entrenamiento: 1.2480, Precisión de Entrenamiento: 0.5509\n",
      "Iteración 5982 - Lote 350/352 - Pérdida de Entrenamiento: 1.2481, Precisión de Entrenamiento: 0.5504\n",
      "Val loss: 1.3092, Val acc: 0.5344\n",
      "0.1\n",
      "Epoch 18/400\n",
      "Iteración 6019 - Lote 35/352 - Pérdida de Entrenamiento: 1.2457, Precisión de Entrenamiento: 0.5563\n",
      "Iteración 6054 - Lote 70/352 - Pérdida de Entrenamiento: 1.2567, Precisión de Entrenamiento: 0.5480\n",
      "Iteración 6089 - Lote 105/352 - Pérdida de Entrenamiento: 1.2489, Precisión de Entrenamiento: 0.5525\n",
      "Iteración 6124 - Lote 140/352 - Pérdida de Entrenamiento: 1.2410, Precisión de Entrenamiento: 0.5556\n",
      "Iteración 6159 - Lote 175/352 - Pérdida de Entrenamiento: 1.2360, Precisión de Entrenamiento: 0.5570\n",
      "Iteración 6194 - Lote 210/352 - Pérdida de Entrenamiento: 1.2377, Precisión de Entrenamiento: 0.5563\n",
      "Iteración 6229 - Lote 245/352 - Pérdida de Entrenamiento: 1.2348, Precisión de Entrenamiento: 0.5576\n",
      "Iteración 6264 - Lote 280/352 - Pérdida de Entrenamiento: 1.2355, Precisión de Entrenamiento: 0.5579\n",
      "Iteración 6299 - Lote 315/352 - Pérdida de Entrenamiento: 1.2347, Precisión de Entrenamiento: 0.5597\n",
      "Iteración 6334 - Lote 350/352 - Pérdida de Entrenamiento: 1.2345, Precisión de Entrenamiento: 0.5600\n",
      "Val loss: 1.3143, Val acc: 0.5356\n",
      "0.1\n",
      "Epoch 19/400\n",
      "Iteración 6371 - Lote 35/352 - Pérdida de Entrenamiento: 1.2515, Precisión de Entrenamiento: 0.5527\n",
      "Iteración 6406 - Lote 70/352 - Pérdida de Entrenamiento: 1.2221, Precisión de Entrenamiento: 0.5627\n",
      "Iteración 6441 - Lote 105/352 - Pérdida de Entrenamiento: 1.2149, Precisión de Entrenamiento: 0.5632\n",
      "Iteración 6476 - Lote 140/352 - Pérdida de Entrenamiento: 1.2140, Precisión de Entrenamiento: 0.5642\n",
      "Iteración 6511 - Lote 175/352 - Pérdida de Entrenamiento: 1.2113, Precisión de Entrenamiento: 0.5675\n",
      "Iteración 6546 - Lote 210/352 - Pérdida de Entrenamiento: 1.2100, Precisión de Entrenamiento: 0.5673\n",
      "Iteración 6581 - Lote 245/352 - Pérdida de Entrenamiento: 1.2059, Precisión de Entrenamiento: 0.5692\n",
      "Iteración 6616 - Lote 280/352 - Pérdida de Entrenamiento: 1.2058, Precisión de Entrenamiento: 0.5688\n",
      "Iteración 6651 - Lote 315/352 - Pérdida de Entrenamiento: 1.2031, Precisión de Entrenamiento: 0.5702\n",
      "Iteración 6686 - Lote 350/352 - Pérdida de Entrenamiento: 1.2056, Precisión de Entrenamiento: 0.5690\n",
      "Val loss: 1.2530, Val acc: 0.5464\n",
      "0.1\n",
      "Epoch 20/400\n",
      "Iteración 6723 - Lote 35/352 - Pérdida de Entrenamiento: 1.2104, Precisión de Entrenamiento: 0.5672\n",
      "Iteración 6758 - Lote 70/352 - Pérdida de Entrenamiento: 1.2108, Precisión de Entrenamiento: 0.5684\n",
      "Iteración 6793 - Lote 105/352 - Pérdida de Entrenamiento: 1.2009, Precisión de Entrenamiento: 0.5711\n",
      "Iteración 6828 - Lote 140/352 - Pérdida de Entrenamiento: 1.1949, Precisión de Entrenamiento: 0.5719\n",
      "Iteración 6863 - Lote 175/352 - Pérdida de Entrenamiento: 1.1984, Precisión de Entrenamiento: 0.5705\n",
      "Iteración 6898 - Lote 210/352 - Pérdida de Entrenamiento: 1.2023, Precisión de Entrenamiento: 0.5700\n",
      "Iteración 6933 - Lote 245/352 - Pérdida de Entrenamiento: 1.1981, Precisión de Entrenamiento: 0.5707\n",
      "Iteración 6968 - Lote 280/352 - Pérdida de Entrenamiento: 1.1926, Precisión de Entrenamiento: 0.5718\n",
      "Iteración 7003 - Lote 315/352 - Pérdida de Entrenamiento: 1.1918, Precisión de Entrenamiento: 0.5725\n",
      "Iteración 7038 - Lote 350/352 - Pérdida de Entrenamiento: 1.1902, Precisión de Entrenamiento: 0.5730\n",
      "Val loss: 1.2035, Val acc: 0.5708\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_20.pth\n",
      "Checkpoint del mejor modelo guardado en la época 20\n",
      "Gradientes para features.0.0.weight: min=-0.02579253539443016, max=0.0371389277279377, mean=0.0008950781193561852, std=0.006806342396885157\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.02786674164235592, max=0.036600932478904724, mean=3.2348751233257644e-07, std=0.0012870876817032695\n",
      "Gradientes para classifier.1.weight: min=-0.01925450749695301, max=0.016046348959207535, mean=-4.656612768993984e-12, std=0.002267921343445778\n",
      "0.1\n",
      "Epoch 21/400\n",
      "Iteración 7075 - Lote 35/352 - Pérdida de Entrenamiento: 1.1852, Precisión de Entrenamiento: 0.5850\n",
      "Iteración 7110 - Lote 70/352 - Pérdida de Entrenamiento: 1.1719, Precisión de Entrenamiento: 0.5876\n",
      "Iteración 7145 - Lote 105/352 - Pérdida de Entrenamiento: 1.1783, Precisión de Entrenamiento: 0.5815\n",
      "Iteración 7180 - Lote 140/352 - Pérdida de Entrenamiento: 1.1802, Precisión de Entrenamiento: 0.5816\n",
      "Iteración 7215 - Lote 175/352 - Pérdida de Entrenamiento: 1.1779, Precisión de Entrenamiento: 0.5822\n",
      "Iteración 7250 - Lote 210/352 - Pérdida de Entrenamiento: 1.1797, Precisión de Entrenamiento: 0.5814\n",
      "Iteración 7285 - Lote 245/352 - Pérdida de Entrenamiento: 1.1792, Precisión de Entrenamiento: 0.5804\n",
      "Iteración 7320 - Lote 280/352 - Pérdida de Entrenamiento: 1.1732, Precisión de Entrenamiento: 0.5819\n",
      "Iteración 7355 - Lote 315/352 - Pérdida de Entrenamiento: 1.1732, Precisión de Entrenamiento: 0.5828\n",
      "Iteración 7390 - Lote 350/352 - Pérdida de Entrenamiento: 1.1725, Precisión de Entrenamiento: 0.5833\n",
      "Val loss: 1.2196, Val acc: 0.5652\n",
      "0.1\n",
      "Epoch 22/400\n",
      "Iteración 7427 - Lote 35/352 - Pérdida de Entrenamiento: 1.1696, Precisión de Entrenamiento: 0.5839\n",
      "Iteración 7462 - Lote 70/352 - Pérdida de Entrenamiento: 1.1768, Precisión de Entrenamiento: 0.5818\n",
      "Iteración 7497 - Lote 105/352 - Pérdida de Entrenamiento: 1.1680, Precisión de Entrenamiento: 0.5829\n",
      "Iteración 7532 - Lote 140/352 - Pérdida de Entrenamiento: 1.1597, Precisión de Entrenamiento: 0.5869\n",
      "Iteración 7567 - Lote 175/352 - Pérdida de Entrenamiento: 1.1575, Precisión de Entrenamiento: 0.5877\n",
      "Iteración 7602 - Lote 210/352 - Pérdida de Entrenamiento: 1.1606, Precisión de Entrenamiento: 0.5871\n",
      "Iteración 7637 - Lote 245/352 - Pérdida de Entrenamiento: 1.1603, Precisión de Entrenamiento: 0.5867\n",
      "Iteración 7672 - Lote 280/352 - Pérdida de Entrenamiento: 1.1636, Precisión de Entrenamiento: 0.5868\n",
      "Iteración 7707 - Lote 315/352 - Pérdida de Entrenamiento: 1.1641, Precisión de Entrenamiento: 0.5876\n",
      "Iteración 7742 - Lote 350/352 - Pérdida de Entrenamiento: 1.1651, Precisión de Entrenamiento: 0.5871\n",
      "Val loss: 1.7303, Val acc: 0.4816\n",
      "0.1\n",
      "Epoch 23/400\n",
      "Iteración 7779 - Lote 35/352 - Pérdida de Entrenamiento: 1.1100, Precisión de Entrenamiento: 0.6080\n",
      "Iteración 7814 - Lote 70/352 - Pérdida de Entrenamiento: 1.1213, Precisión de Entrenamiento: 0.6021\n",
      "Iteración 7849 - Lote 105/352 - Pérdida de Entrenamiento: 1.1319, Precisión de Entrenamiento: 0.6007\n",
      "Iteración 7884 - Lote 140/352 - Pérdida de Entrenamiento: 1.1474, Precisión de Entrenamiento: 0.5940\n",
      "Iteración 7919 - Lote 175/352 - Pérdida de Entrenamiento: 1.1472, Precisión de Entrenamiento: 0.5938\n",
      "Iteración 7954 - Lote 210/352 - Pérdida de Entrenamiento: 1.1460, Precisión de Entrenamiento: 0.5930\n",
      "Iteración 7989 - Lote 245/352 - Pérdida de Entrenamiento: 1.1471, Precisión de Entrenamiento: 0.5929\n",
      "Iteración 8024 - Lote 280/352 - Pérdida de Entrenamiento: 1.1482, Precisión de Entrenamiento: 0.5922\n",
      "Iteración 8059 - Lote 315/352 - Pérdida de Entrenamiento: 1.1478, Precisión de Entrenamiento: 0.5924\n",
      "Iteración 8094 - Lote 350/352 - Pérdida de Entrenamiento: 1.1473, Precisión de Entrenamiento: 0.5923\n",
      "Val loss: 1.2052, Val acc: 0.5872\n",
      "0.1\n",
      "Epoch 24/400\n",
      "Iteración 8131 - Lote 35/352 - Pérdida de Entrenamiento: 1.1529, Precisión de Entrenamiento: 0.5868\n",
      "Iteración 8166 - Lote 70/352 - Pérdida de Entrenamiento: 1.1523, Precisión de Entrenamiento: 0.5910\n",
      "Iteración 8201 - Lote 105/352 - Pérdida de Entrenamiento: 1.1489, Precisión de Entrenamiento: 0.5922\n",
      "Iteración 8236 - Lote 140/352 - Pérdida de Entrenamiento: 1.1493, Precisión de Entrenamiento: 0.5900\n",
      "Iteración 8271 - Lote 175/352 - Pérdida de Entrenamiento: 1.1482, Precisión de Entrenamiento: 0.5903\n",
      "Iteración 8306 - Lote 210/352 - Pérdida de Entrenamiento: 1.1440, Precisión de Entrenamiento: 0.5919\n",
      "Iteración 8341 - Lote 245/352 - Pérdida de Entrenamiento: 1.1448, Precisión de Entrenamiento: 0.5923\n",
      "Iteración 8376 - Lote 280/352 - Pérdida de Entrenamiento: 1.1435, Precisión de Entrenamiento: 0.5916\n",
      "Iteración 8411 - Lote 315/352 - Pérdida de Entrenamiento: 1.1415, Precisión de Entrenamiento: 0.5932\n",
      "Iteración 8446 - Lote 350/352 - Pérdida de Entrenamiento: 1.1409, Precisión de Entrenamiento: 0.5936\n",
      "Val loss: 1.3042, Val acc: 0.5376\n",
      "0.1\n",
      "Epoch 25/400\n",
      "Iteración 8483 - Lote 35/352 - Pérdida de Entrenamiento: 1.1281, Precisión de Entrenamiento: 0.6038\n",
      "Iteración 8518 - Lote 70/352 - Pérdida de Entrenamiento: 1.1449, Precisión de Entrenamiento: 0.5991\n",
      "Iteración 8553 - Lote 105/352 - Pérdida de Entrenamiento: 1.1346, Precisión de Entrenamiento: 0.6028\n",
      "Iteración 8588 - Lote 140/352 - Pérdida de Entrenamiento: 1.1362, Precisión de Entrenamiento: 0.6017\n",
      "Iteración 8623 - Lote 175/352 - Pérdida de Entrenamiento: 1.1362, Precisión de Entrenamiento: 0.6012\n",
      "Iteración 8658 - Lote 210/352 - Pérdida de Entrenamiento: 1.1361, Precisión de Entrenamiento: 0.6021\n",
      "Iteración 8693 - Lote 245/352 - Pérdida de Entrenamiento: 1.1345, Precisión de Entrenamiento: 0.6024\n",
      "Iteración 8728 - Lote 280/352 - Pérdida de Entrenamiento: 1.1357, Precisión de Entrenamiento: 0.6019\n",
      "Iteración 8763 - Lote 315/352 - Pérdida de Entrenamiento: 1.1336, Precisión de Entrenamiento: 0.6032\n",
      "Iteración 8798 - Lote 350/352 - Pérdida de Entrenamiento: 1.1323, Precisión de Entrenamiento: 0.6035\n",
      "Val loss: 1.1115, Val acc: 0.6340\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_25.pth\n",
      "Checkpoint del mejor modelo guardado en la época 25\n",
      "Gradientes para features.0.0.weight: min=-0.04085296764969826, max=0.05046403035521507, mean=-0.001070228056050837, std=0.013401728123426437\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.0323546826839447, max=0.031069055199623108, mean=-3.3685560083540622e-06, std=0.0012431878130882978\n",
      "Gradientes para classifier.1.weight: min=-0.013904525898396969, max=0.011897198855876923, mean=-1.3969838306981952e-11, std=0.0019071699352934957\n",
      "0.1\n",
      "Epoch 26/400\n",
      "Iteración 8835 - Lote 35/352 - Pérdida de Entrenamiento: 1.1337, Precisión de Entrenamiento: 0.5955\n",
      "Iteración 8870 - Lote 70/352 - Pérdida de Entrenamiento: 1.1315, Precisión de Entrenamiento: 0.5985\n",
      "Iteración 8905 - Lote 105/352 - Pérdida de Entrenamiento: 1.1337, Precisión de Entrenamiento: 0.5950\n",
      "Iteración 8940 - Lote 140/352 - Pérdida de Entrenamiento: 1.1338, Precisión de Entrenamiento: 0.5967\n",
      "Iteración 8975 - Lote 175/352 - Pérdida de Entrenamiento: 1.1319, Precisión de Entrenamiento: 0.5993\n",
      "Iteración 9010 - Lote 210/352 - Pérdida de Entrenamiento: 1.1268, Precisión de Entrenamiento: 0.6010\n",
      "Iteración 9045 - Lote 245/352 - Pérdida de Entrenamiento: 1.1262, Precisión de Entrenamiento: 0.5999\n",
      "Iteración 9080 - Lote 280/352 - Pérdida de Entrenamiento: 1.1278, Precisión de Entrenamiento: 0.5998\n",
      "Iteración 9115 - Lote 315/352 - Pérdida de Entrenamiento: 1.1279, Precisión de Entrenamiento: 0.6005\n",
      "Iteración 9150 - Lote 350/352 - Pérdida de Entrenamiento: 1.1312, Precisión de Entrenamiento: 0.6002\n",
      "Val loss: 1.3115, Val acc: 0.5478\n",
      "0.1\n",
      "Epoch 27/400\n",
      "Iteración 9187 - Lote 35/352 - Pérdida de Entrenamiento: 1.1419, Precisión de Entrenamiento: 0.5980\n",
      "Iteración 9222 - Lote 70/352 - Pérdida de Entrenamiento: 1.1300, Precisión de Entrenamiento: 0.6009\n",
      "Iteración 9257 - Lote 105/352 - Pérdida de Entrenamiento: 1.1310, Precisión de Entrenamiento: 0.5984\n",
      "Iteración 9292 - Lote 140/352 - Pérdida de Entrenamiento: 1.1255, Precisión de Entrenamiento: 0.5990\n",
      "Iteración 9327 - Lote 175/352 - Pérdida de Entrenamiento: 1.1218, Precisión de Entrenamiento: 0.5987\n",
      "Iteración 9362 - Lote 210/352 - Pérdida de Entrenamiento: 1.1278, Precisión de Entrenamiento: 0.5989\n",
      "Iteración 9397 - Lote 245/352 - Pérdida de Entrenamiento: 1.1286, Precisión de Entrenamiento: 0.5982\n",
      "Iteración 9432 - Lote 280/352 - Pérdida de Entrenamiento: 1.1272, Precisión de Entrenamiento: 0.5982\n",
      "Iteración 9467 - Lote 315/352 - Pérdida de Entrenamiento: 1.1294, Precisión de Entrenamiento: 0.5975\n",
      "Iteración 9502 - Lote 350/352 - Pérdida de Entrenamiento: 1.1270, Precisión de Entrenamiento: 0.5991\n",
      "Val loss: 1.1286, Val acc: 0.6016\n",
      "0.1\n",
      "Epoch 28/400\n",
      "Iteración 9539 - Lote 35/352 - Pérdida de Entrenamiento: 1.0972, Precisión de Entrenamiento: 0.6192\n",
      "Iteración 9574 - Lote 70/352 - Pérdida de Entrenamiento: 1.1021, Precisión de Entrenamiento: 0.6141\n",
      "Iteración 9609 - Lote 105/352 - Pérdida de Entrenamiento: 1.1044, Precisión de Entrenamiento: 0.6138\n",
      "Iteración 9644 - Lote 140/352 - Pérdida de Entrenamiento: 1.1055, Precisión de Entrenamiento: 0.6129\n",
      "Iteración 9679 - Lote 175/352 - Pérdida de Entrenamiento: 1.1046, Precisión de Entrenamiento: 0.6132\n",
      "Iteración 9714 - Lote 210/352 - Pérdida de Entrenamiento: 1.1056, Precisión de Entrenamiento: 0.6122\n",
      "Iteración 9749 - Lote 245/352 - Pérdida de Entrenamiento: 1.1136, Precisión de Entrenamiento: 0.6085\n",
      "Iteración 9784 - Lote 280/352 - Pérdida de Entrenamiento: 1.1126, Precisión de Entrenamiento: 0.6091\n",
      "Iteración 9819 - Lote 315/352 - Pérdida de Entrenamiento: 1.1141, Precisión de Entrenamiento: 0.6100\n",
      "Iteración 9854 - Lote 350/352 - Pérdida de Entrenamiento: 1.1102, Precisión de Entrenamiento: 0.6112\n",
      "Val loss: 1.0960, Val acc: 0.6122\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_28.pth\n",
      "Checkpoint del mejor modelo guardado en la época 28\n",
      "0.1\n",
      "Epoch 29/400\n",
      "Iteración 9891 - Lote 35/352 - Pérdida de Entrenamiento: 1.0775, Precisión de Entrenamiento: 0.6214\n",
      "Iteración 9926 - Lote 70/352 - Pérdida de Entrenamiento: 1.0997, Precisión de Entrenamiento: 0.6123\n",
      "Iteración 9961 - Lote 105/352 - Pérdida de Entrenamiento: 1.1167, Precisión de Entrenamiento: 0.6070\n",
      "Iteración 9996 - Lote 140/352 - Pérdida de Entrenamiento: 1.1118, Precisión de Entrenamiento: 0.6089\n",
      "Iteración 10031 - Lote 175/352 - Pérdida de Entrenamiento: 1.1119, Precisión de Entrenamiento: 0.6083\n",
      "Iteración 10066 - Lote 210/352 - Pérdida de Entrenamiento: 1.1099, Precisión de Entrenamiento: 0.6085\n",
      "Iteración 10101 - Lote 245/352 - Pérdida de Entrenamiento: 1.1094, Precisión de Entrenamiento: 0.6079\n",
      "Iteración 10136 - Lote 280/352 - Pérdida de Entrenamiento: 1.1072, Precisión de Entrenamiento: 0.6088\n",
      "Iteración 10171 - Lote 315/352 - Pérdida de Entrenamiento: 1.1097, Precisión de Entrenamiento: 0.6077\n",
      "Iteración 10206 - Lote 350/352 - Pérdida de Entrenamiento: 1.1114, Precisión de Entrenamiento: 0.6069\n",
      "Val loss: 1.2464, Val acc: 0.5610\n",
      "0.1\n",
      "Epoch 30/400\n",
      "Iteración 10243 - Lote 35/352 - Pérdida de Entrenamiento: 1.1184, Precisión de Entrenamiento: 0.6027\n",
      "Iteración 10278 - Lote 70/352 - Pérdida de Entrenamiento: 1.1014, Precisión de Entrenamiento: 0.6096\n",
      "Iteración 10313 - Lote 105/352 - Pérdida de Entrenamiento: 1.1075, Precisión de Entrenamiento: 0.6065\n",
      "Iteración 10348 - Lote 140/352 - Pérdida de Entrenamiento: 1.1100, Precisión de Entrenamiento: 0.6039\n",
      "Iteración 10383 - Lote 175/352 - Pérdida de Entrenamiento: 1.1108, Precisión de Entrenamiento: 0.6060\n",
      "Iteración 10418 - Lote 210/352 - Pérdida de Entrenamiento: 1.1094, Precisión de Entrenamiento: 0.6060\n",
      "Iteración 10453 - Lote 245/352 - Pérdida de Entrenamiento: 1.1133, Precisión de Entrenamiento: 0.6049\n",
      "Iteración 10488 - Lote 280/352 - Pérdida de Entrenamiento: 1.1148, Precisión de Entrenamiento: 0.6046\n",
      "Iteración 10523 - Lote 315/352 - Pérdida de Entrenamiento: 1.1136, Precisión de Entrenamiento: 0.6038\n",
      "Iteración 10558 - Lote 350/352 - Pérdida de Entrenamiento: 1.1106, Precisión de Entrenamiento: 0.6046\n",
      "Val loss: 1.0400, Val acc: 0.6378\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_30.pth\n",
      "Checkpoint del mejor modelo guardado en la época 30\n",
      "Gradientes para features.0.0.weight: min=-0.04140755906701088, max=0.038226209580898285, mean=-0.0015698897186666727, std=0.010761882178485394\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.02838987484574318, max=0.027330700308084488, mean=7.467226623703027e-06, std=0.0013070106506347656\n",
      "Gradientes para classifier.1.weight: min=-0.015527748502790928, max=0.021035946905612946, mean=5.8207661780829145e-12, std=0.002249642740935087\n",
      "0.1\n",
      "Epoch 31/400\n",
      "Iteración 10595 - Lote 35/352 - Pérdida de Entrenamiento: 1.0585, Precisión de Entrenamiento: 0.6301\n",
      "Iteración 10630 - Lote 70/352 - Pérdida de Entrenamiento: 1.0705, Precisión de Entrenamiento: 0.6225\n",
      "Iteración 10665 - Lote 105/352 - Pérdida de Entrenamiento: 1.0818, Precisión de Entrenamiento: 0.6188\n",
      "Iteración 10700 - Lote 140/352 - Pérdida de Entrenamiento: 1.0884, Precisión de Entrenamiento: 0.6168\n",
      "Iteración 10735 - Lote 175/352 - Pérdida de Entrenamiento: 1.0940, Precisión de Entrenamiento: 0.6148\n",
      "Iteración 10770 - Lote 210/352 - Pérdida de Entrenamiento: 1.1034, Precisión de Entrenamiento: 0.6110\n",
      "Iteración 10805 - Lote 245/352 - Pérdida de Entrenamiento: 1.0994, Precisión de Entrenamiento: 0.6116\n",
      "Iteración 10840 - Lote 280/352 - Pérdida de Entrenamiento: 1.1013, Precisión de Entrenamiento: 0.6118\n",
      "Iteración 10875 - Lote 315/352 - Pérdida de Entrenamiento: 1.1020, Precisión de Entrenamiento: 0.6109\n",
      "Iteración 10910 - Lote 350/352 - Pérdida de Entrenamiento: 1.1034, Precisión de Entrenamiento: 0.6104\n",
      "Val loss: 1.2654, Val acc: 0.5830\n",
      "0.1\n",
      "Epoch 32/400\n",
      "Iteración 10947 - Lote 35/352 - Pérdida de Entrenamiento: 1.1193, Precisión de Entrenamiento: 0.6087\n",
      "Iteración 10982 - Lote 70/352 - Pérdida de Entrenamiento: 1.0936, Precisión de Entrenamiento: 0.6132\n",
      "Iteración 11017 - Lote 105/352 - Pérdida de Entrenamiento: 1.0947, Precisión de Entrenamiento: 0.6153\n",
      "Iteración 11052 - Lote 140/352 - Pérdida de Entrenamiento: 1.0947, Precisión de Entrenamiento: 0.6143\n",
      "Iteración 11087 - Lote 175/352 - Pérdida de Entrenamiento: 1.0924, Precisión de Entrenamiento: 0.6153\n",
      "Iteración 11122 - Lote 210/352 - Pérdida de Entrenamiento: 1.0976, Precisión de Entrenamiento: 0.6123\n",
      "Iteración 11157 - Lote 245/352 - Pérdida de Entrenamiento: 1.0984, Precisión de Entrenamiento: 0.6123\n",
      "Iteración 11192 - Lote 280/352 - Pérdida de Entrenamiento: 1.1016, Precisión de Entrenamiento: 0.6117\n",
      "Iteración 11227 - Lote 315/352 - Pérdida de Entrenamiento: 1.0974, Precisión de Entrenamiento: 0.6122\n",
      "Iteración 11262 - Lote 350/352 - Pérdida de Entrenamiento: 1.0939, Precisión de Entrenamiento: 0.6133\n",
      "Val loss: 1.1334, Val acc: 0.6108\n",
      "0.1\n",
      "Epoch 33/400\n",
      "Iteración 11299 - Lote 35/352 - Pérdida de Entrenamiento: 1.1288, Precisión de Entrenamiento: 0.5938\n",
      "Iteración 11334 - Lote 70/352 - Pérdida de Entrenamiento: 1.1299, Precisión de Entrenamiento: 0.5953\n",
      "Iteración 11369 - Lote 105/352 - Pérdida de Entrenamiento: 1.1164, Precisión de Entrenamiento: 0.6010\n",
      "Iteración 11404 - Lote 140/352 - Pérdida de Entrenamiento: 1.1120, Precisión de Entrenamiento: 0.6042\n",
      "Iteración 11439 - Lote 175/352 - Pérdida de Entrenamiento: 1.1099, Precisión de Entrenamiento: 0.6054\n",
      "Iteración 11474 - Lote 210/352 - Pérdida de Entrenamiento: 1.1022, Precisión de Entrenamiento: 0.6075\n",
      "Iteración 11509 - Lote 245/352 - Pérdida de Entrenamiento: 1.0975, Precisión de Entrenamiento: 0.6103\n",
      "Iteración 11544 - Lote 280/352 - Pérdida de Entrenamiento: 1.0959, Precisión de Entrenamiento: 0.6110\n",
      "Iteración 11579 - Lote 315/352 - Pérdida de Entrenamiento: 1.0941, Precisión de Entrenamiento: 0.6116\n",
      "Iteración 11614 - Lote 350/352 - Pérdida de Entrenamiento: 1.0950, Precisión de Entrenamiento: 0.6117\n",
      "Val loss: 1.0768, Val acc: 0.6284\n",
      "0.1\n",
      "Epoch 34/400\n",
      "Iteración 11651 - Lote 35/352 - Pérdida de Entrenamiento: 1.0898, Precisión de Entrenamiento: 0.6205\n",
      "Iteración 11686 - Lote 70/352 - Pérdida de Entrenamiento: 1.0803, Precisión de Entrenamiento: 0.6231\n",
      "Iteración 11721 - Lote 105/352 - Pérdida de Entrenamiento: 1.0846, Precisión de Entrenamiento: 0.6196\n",
      "Iteración 11756 - Lote 140/352 - Pérdida de Entrenamiento: 1.0851, Precisión de Entrenamiento: 0.6206\n",
      "Iteración 11791 - Lote 175/352 - Pérdida de Entrenamiento: 1.0843, Precisión de Entrenamiento: 0.6204\n",
      "Iteración 11826 - Lote 210/352 - Pérdida de Entrenamiento: 1.0841, Precisión de Entrenamiento: 0.6198\n",
      "Iteración 11861 - Lote 245/352 - Pérdida de Entrenamiento: 1.0813, Precisión de Entrenamiento: 0.6208\n",
      "Iteración 11896 - Lote 280/352 - Pérdida de Entrenamiento: 1.0885, Precisión de Entrenamiento: 0.6197\n",
      "Iteración 11931 - Lote 315/352 - Pérdida de Entrenamiento: 1.0906, Precisión de Entrenamiento: 0.6192\n",
      "Iteración 11966 - Lote 350/352 - Pérdida de Entrenamiento: 1.0872, Precisión de Entrenamiento: 0.6196\n",
      "Val loss: 1.4118, Val acc: 0.5216\n",
      "0.1\n",
      "Epoch 35/400\n",
      "Iteración 12003 - Lote 35/352 - Pérdida de Entrenamiento: 1.0722, Precisión de Entrenamiento: 0.6141\n",
      "Iteración 12038 - Lote 70/352 - Pérdida de Entrenamiento: 1.0865, Precisión de Entrenamiento: 0.6147\n",
      "Iteración 12073 - Lote 105/352 - Pérdida de Entrenamiento: 1.0849, Precisión de Entrenamiento: 0.6170\n",
      "Iteración 12108 - Lote 140/352 - Pérdida de Entrenamiento: 1.0942, Precisión de Entrenamiento: 0.6123\n",
      "Iteración 12143 - Lote 175/352 - Pérdida de Entrenamiento: 1.0934, Precisión de Entrenamiento: 0.6133\n",
      "Iteración 12178 - Lote 210/352 - Pérdida de Entrenamiento: 1.0813, Precisión de Entrenamiento: 0.6190\n",
      "Iteración 12213 - Lote 245/352 - Pérdida de Entrenamiento: 1.0828, Precisión de Entrenamiento: 0.6193\n",
      "Iteración 12248 - Lote 280/352 - Pérdida de Entrenamiento: 1.0858, Precisión de Entrenamiento: 0.6189\n",
      "Iteración 12283 - Lote 315/352 - Pérdida de Entrenamiento: 1.0858, Precisión de Entrenamiento: 0.6184\n",
      "Iteración 12318 - Lote 350/352 - Pérdida de Entrenamiento: 1.0846, Precisión de Entrenamiento: 0.6188\n",
      "Val loss: 1.1624, Val acc: 0.5976\n",
      "Gradientes para features.0.0.weight: min=-0.07668838649988174, max=0.05961276590824127, mean=-0.001223849249072373, std=0.018691161647439003\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.01744040846824646, max=0.022275220602750778, mean=-9.558910960549838e-07, std=0.0008736858144402504\n",
      "Gradientes para classifier.1.weight: min=-0.01708134450018406, max=0.020160043612122536, mean=-4.656612768993984e-12, std=0.0020559346303343773\n",
      "0.1\n",
      "Epoch 36/400\n",
      "Iteración 12355 - Lote 35/352 - Pérdida de Entrenamiento: 1.0597, Precisión de Entrenamiento: 0.6297\n",
      "Iteración 12390 - Lote 70/352 - Pérdida de Entrenamiento: 1.0805, Precisión de Entrenamiento: 0.6251\n",
      "Iteración 12425 - Lote 105/352 - Pérdida de Entrenamiento: 1.0799, Precisión de Entrenamiento: 0.6228\n",
      "Iteración 12460 - Lote 140/352 - Pérdida de Entrenamiento: 1.0761, Precisión de Entrenamiento: 0.6233\n",
      "Iteración 12495 - Lote 175/352 - Pérdida de Entrenamiento: 1.0837, Precisión de Entrenamiento: 0.6213\n",
      "Iteración 12530 - Lote 210/352 - Pérdida de Entrenamiento: 1.0819, Precisión de Entrenamiento: 0.6211\n",
      "Iteración 12565 - Lote 245/352 - Pérdida de Entrenamiento: 1.0800, Precisión de Entrenamiento: 0.6217\n",
      "Iteración 12600 - Lote 280/352 - Pérdida de Entrenamiento: 1.0851, Precisión de Entrenamiento: 0.6196\n",
      "Iteración 12635 - Lote 315/352 - Pérdida de Entrenamiento: 1.0876, Precisión de Entrenamiento: 0.6183\n",
      "Iteración 12670 - Lote 350/352 - Pérdida de Entrenamiento: 1.0876, Precisión de Entrenamiento: 0.6183\n",
      "Val loss: 1.3435, Val acc: 0.5584\n",
      "0.1\n",
      "Epoch 37/400\n",
      "Iteración 12707 - Lote 35/352 - Pérdida de Entrenamiento: 1.0996, Precisión de Entrenamiento: 0.6129\n",
      "Iteración 12742 - Lote 70/352 - Pérdida de Entrenamiento: 1.0956, Precisión de Entrenamiento: 0.6133\n",
      "Iteración 12777 - Lote 105/352 - Pérdida de Entrenamiento: 1.0926, Precisión de Entrenamiento: 0.6152\n",
      "Iteración 12812 - Lote 140/352 - Pérdida de Entrenamiento: 1.0886, Precisión de Entrenamiento: 0.6175\n",
      "Iteración 12847 - Lote 175/352 - Pérdida de Entrenamiento: 1.0902, Precisión de Entrenamiento: 0.6175\n",
      "Iteración 12882 - Lote 210/352 - Pérdida de Entrenamiento: 1.0844, Precisión de Entrenamiento: 0.6197\n",
      "Iteración 12917 - Lote 245/352 - Pérdida de Entrenamiento: 1.0886, Precisión de Entrenamiento: 0.6170\n",
      "Iteración 12952 - Lote 280/352 - Pérdida de Entrenamiento: 1.0891, Precisión de Entrenamiento: 0.6169\n",
      "Iteración 12987 - Lote 315/352 - Pérdida de Entrenamiento: 1.0865, Precisión de Entrenamiento: 0.6183\n",
      "Iteración 13022 - Lote 350/352 - Pérdida de Entrenamiento: 1.0850, Precisión de Entrenamiento: 0.6178\n",
      "Val loss: 1.1843, Val acc: 0.5904\n",
      "0.1\n",
      "Epoch 38/400\n",
      "Iteración 13059 - Lote 35/352 - Pérdida de Entrenamiento: 1.1328, Precisión de Entrenamiento: 0.6078\n",
      "Iteración 13094 - Lote 70/352 - Pérdida de Entrenamiento: 1.1015, Precisión de Entrenamiento: 0.6162\n",
      "Iteración 13129 - Lote 105/352 - Pérdida de Entrenamiento: 1.1020, Precisión de Entrenamiento: 0.6147\n",
      "Iteración 13164 - Lote 140/352 - Pérdida de Entrenamiento: 1.0873, Precisión de Entrenamiento: 0.6204\n",
      "Iteración 13199 - Lote 175/352 - Pérdida de Entrenamiento: 1.0885, Precisión de Entrenamiento: 0.6220\n",
      "Iteración 13234 - Lote 210/352 - Pérdida de Entrenamiento: 1.0870, Precisión de Entrenamiento: 0.6222\n",
      "Iteración 13269 - Lote 245/352 - Pérdida de Entrenamiento: 1.0885, Precisión de Entrenamiento: 0.6219\n",
      "Iteración 13304 - Lote 280/352 - Pérdida de Entrenamiento: 1.0888, Precisión de Entrenamiento: 0.6218\n",
      "Iteración 13339 - Lote 315/352 - Pérdida de Entrenamiento: 1.0857, Precisión de Entrenamiento: 0.6228\n",
      "Iteración 13374 - Lote 350/352 - Pérdida de Entrenamiento: 1.0840, Precisión de Entrenamiento: 0.6228\n",
      "Val loss: 1.2251, Val acc: 0.5740\n",
      "0.1\n",
      "Epoch 39/400\n",
      "Iteración 13411 - Lote 35/352 - Pérdida de Entrenamiento: 1.0668, Precisión de Entrenamiento: 0.6283\n",
      "Iteración 13446 - Lote 70/352 - Pérdida de Entrenamiento: 1.0815, Precisión de Entrenamiento: 0.6239\n",
      "Iteración 13481 - Lote 105/352 - Pérdida de Entrenamiento: 1.0766, Precisión de Entrenamiento: 0.6251\n",
      "Iteración 13516 - Lote 140/352 - Pérdida de Entrenamiento: 1.0795, Precisión de Entrenamiento: 0.6220\n",
      "Iteración 13551 - Lote 175/352 - Pérdida de Entrenamiento: 1.0824, Precisión de Entrenamiento: 0.6206\n",
      "Iteración 13586 - Lote 210/352 - Pérdida de Entrenamiento: 1.0843, Precisión de Entrenamiento: 0.6189\n",
      "Iteración 13621 - Lote 245/352 - Pérdida de Entrenamiento: 1.0825, Precisión de Entrenamiento: 0.6198\n",
      "Iteración 13656 - Lote 280/352 - Pérdida de Entrenamiento: 1.0800, Precisión de Entrenamiento: 0.6210\n",
      "Iteración 13691 - Lote 315/352 - Pérdida de Entrenamiento: 1.0857, Precisión de Entrenamiento: 0.6195\n",
      "Iteración 13726 - Lote 350/352 - Pérdida de Entrenamiento: 1.0819, Precisión de Entrenamiento: 0.6212\n",
      "Val loss: 1.0245, Val acc: 0.6476\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_39.pth\n",
      "Checkpoint del mejor modelo guardado en la época 39\n",
      "0.1\n",
      "Epoch 40/400\n",
      "Iteración 13763 - Lote 35/352 - Pérdida de Entrenamiento: 1.0811, Precisión de Entrenamiento: 0.6201\n",
      "Iteración 13798 - Lote 70/352 - Pérdida de Entrenamiento: 1.0645, Precisión de Entrenamiento: 0.6249\n",
      "Iteración 13833 - Lote 105/352 - Pérdida de Entrenamiento: 1.0565, Precisión de Entrenamiento: 0.6303\n",
      "Iteración 13868 - Lote 140/352 - Pérdida de Entrenamiento: 1.0593, Precisión de Entrenamiento: 0.6297\n",
      "Iteración 13903 - Lote 175/352 - Pérdida de Entrenamiento: 1.0594, Precisión de Entrenamiento: 0.6287\n",
      "Iteración 13938 - Lote 210/352 - Pérdida de Entrenamiento: 1.0640, Precisión de Entrenamiento: 0.6272\n",
      "Iteración 13973 - Lote 245/352 - Pérdida de Entrenamiento: 1.0621, Precisión de Entrenamiento: 0.6288\n",
      "Iteración 14008 - Lote 280/352 - Pérdida de Entrenamiento: 1.0621, Precisión de Entrenamiento: 0.6290\n",
      "Iteración 14043 - Lote 315/352 - Pérdida de Entrenamiento: 1.0624, Precisión de Entrenamiento: 0.6283\n",
      "Iteración 14078 - Lote 350/352 - Pérdida de Entrenamiento: 1.0669, Precisión de Entrenamiento: 0.6269\n",
      "Val loss: 1.1788, Val acc: 0.5920\n",
      "Gradientes para features.0.0.weight: min=-0.06925708055496216, max=0.07476285845041275, mean=-0.0018676677718758583, std=0.016936564818024635\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.019986141473054886, max=0.01974622532725334, mean=7.836358690838097e-07, std=0.0008547785109840333\n",
      "Gradientes para classifier.1.weight: min=-0.01513024140149355, max=0.011299699544906616, mean=-6.984919153490976e-12, std=0.001887502963654697\n",
      "0.1\n",
      "Epoch 41/400\n",
      "Iteración 14115 - Lote 35/352 - Pérdida de Entrenamiento: 1.0756, Precisión de Entrenamiento: 0.6138\n",
      "Iteración 14150 - Lote 70/352 - Pérdida de Entrenamiento: 1.0703, Precisión de Entrenamiento: 0.6205\n",
      "Iteración 14185 - Lote 105/352 - Pérdida de Entrenamiento: 1.0722, Precisión de Entrenamiento: 0.6205\n",
      "Iteración 14220 - Lote 140/352 - Pérdida de Entrenamiento: 1.0746, Precisión de Entrenamiento: 0.6214\n",
      "Iteración 14255 - Lote 175/352 - Pérdida de Entrenamiento: 1.0792, Precisión de Entrenamiento: 0.6208\n",
      "Iteración 14290 - Lote 210/352 - Pérdida de Entrenamiento: 1.0785, Precisión de Entrenamiento: 0.6208\n",
      "Iteración 14325 - Lote 245/352 - Pérdida de Entrenamiento: 1.0711, Precisión de Entrenamiento: 0.6240\n",
      "Iteración 14360 - Lote 280/352 - Pérdida de Entrenamiento: 1.0753, Precisión de Entrenamiento: 0.6229\n",
      "Iteración 14395 - Lote 315/352 - Pérdida de Entrenamiento: 1.0759, Precisión de Entrenamiento: 0.6225\n",
      "Iteración 14430 - Lote 350/352 - Pérdida de Entrenamiento: 1.0771, Precisión de Entrenamiento: 0.6226\n",
      "Val loss: 1.2840, Val acc: 0.5670\n",
      "0.1\n",
      "Epoch 42/400\n",
      "Iteración 14467 - Lote 35/352 - Pérdida de Entrenamiento: 1.0695, Precisión de Entrenamiento: 0.6201\n",
      "Iteración 14502 - Lote 70/352 - Pérdida de Entrenamiento: 1.0485, Precisión de Entrenamiento: 0.6347\n",
      "Iteración 14537 - Lote 105/352 - Pérdida de Entrenamiento: 1.0619, Precisión de Entrenamiento: 0.6277\n",
      "Iteración 14572 - Lote 140/352 - Pérdida de Entrenamiento: 1.0708, Precisión de Entrenamiento: 0.6255\n",
      "Iteración 14607 - Lote 175/352 - Pérdida de Entrenamiento: 1.0702, Precisión de Entrenamiento: 0.6270\n",
      "Iteración 14642 - Lote 210/352 - Pérdida de Entrenamiento: 1.0717, Precisión de Entrenamiento: 0.6247\n",
      "Iteración 14677 - Lote 245/352 - Pérdida de Entrenamiento: 1.0701, Precisión de Entrenamiento: 0.6256\n",
      "Iteración 14712 - Lote 280/352 - Pérdida de Entrenamiento: 1.0720, Precisión de Entrenamiento: 0.6248\n",
      "Iteración 14747 - Lote 315/352 - Pérdida de Entrenamiento: 1.0692, Precisión de Entrenamiento: 0.6266\n",
      "Iteración 14782 - Lote 350/352 - Pérdida de Entrenamiento: 1.0738, Precisión de Entrenamiento: 0.6243\n",
      "Val loss: 1.0827, Val acc: 0.6232\n",
      "0.1\n",
      "Epoch 43/400\n",
      "Iteración 14819 - Lote 35/352 - Pérdida de Entrenamiento: 1.0466, Precisión de Entrenamiento: 0.6337\n",
      "Iteración 14854 - Lote 70/352 - Pérdida de Entrenamiento: 1.0702, Precisión de Entrenamiento: 0.6239\n",
      "Iteración 14889 - Lote 105/352 - Pérdida de Entrenamiento: 1.0772, Precisión de Entrenamiento: 0.6215\n",
      "Iteración 14924 - Lote 140/352 - Pérdida de Entrenamiento: 1.0752, Precisión de Entrenamiento: 0.6244\n",
      "Iteración 14959 - Lote 175/352 - Pérdida de Entrenamiento: 1.0773, Precisión de Entrenamiento: 0.6230\n",
      "Iteración 14994 - Lote 210/352 - Pérdida de Entrenamiento: 1.0772, Precisión de Entrenamiento: 0.6228\n",
      "Iteración 15029 - Lote 245/352 - Pérdida de Entrenamiento: 1.0742, Precisión de Entrenamiento: 0.6246\n",
      "Iteración 15064 - Lote 280/352 - Pérdida de Entrenamiento: 1.0733, Precisión de Entrenamiento: 0.6263\n",
      "Iteración 15099 - Lote 315/352 - Pérdida de Entrenamiento: 1.0717, Precisión de Entrenamiento: 0.6273\n",
      "Iteración 15134 - Lote 350/352 - Pérdida de Entrenamiento: 1.0725, Precisión de Entrenamiento: 0.6272\n",
      "Val loss: 1.2844, Val acc: 0.5616\n",
      "0.1\n",
      "Epoch 44/400\n",
      "Iteración 15171 - Lote 35/352 - Pérdida de Entrenamiento: 1.0677, Precisión de Entrenamiento: 0.6266\n",
      "Iteración 15206 - Lote 70/352 - Pérdida de Entrenamiento: 1.0794, Precisión de Entrenamiento: 0.6235\n",
      "Iteración 15241 - Lote 105/352 - Pérdida de Entrenamiento: 1.0832, Precisión de Entrenamiento: 0.6207\n",
      "Iteración 15276 - Lote 140/352 - Pérdida de Entrenamiento: 1.0745, Precisión de Entrenamiento: 0.6235\n",
      "Iteración 15311 - Lote 175/352 - Pérdida de Entrenamiento: 1.0766, Precisión de Entrenamiento: 0.6234\n",
      "Iteración 15346 - Lote 210/352 - Pérdida de Entrenamiento: 1.0723, Precisión de Entrenamiento: 0.6245\n",
      "Iteración 15381 - Lote 245/352 - Pérdida de Entrenamiento: 1.0693, Precisión de Entrenamiento: 0.6262\n",
      "Iteración 15416 - Lote 280/352 - Pérdida de Entrenamiento: 1.0723, Precisión de Entrenamiento: 0.6253\n",
      "Iteración 15451 - Lote 315/352 - Pérdida de Entrenamiento: 1.0727, Precisión de Entrenamiento: 0.6252\n",
      "Iteración 15486 - Lote 350/352 - Pérdida de Entrenamiento: 1.0732, Precisión de Entrenamiento: 0.6251\n",
      "Val loss: 1.2439, Val acc: 0.5876\n",
      "0.1\n",
      "Epoch 45/400\n",
      "Iteración 15523 - Lote 35/352 - Pérdida de Entrenamiento: 1.0719, Precisión de Entrenamiento: 0.6237\n",
      "Iteración 15558 - Lote 70/352 - Pérdida de Entrenamiento: 1.0364, Precisión de Entrenamiento: 0.6372\n",
      "Iteración 15593 - Lote 105/352 - Pérdida de Entrenamiento: 1.0488, Precisión de Entrenamiento: 0.6314\n",
      "Iteración 15628 - Lote 140/352 - Pérdida de Entrenamiento: 1.0508, Precisión de Entrenamiento: 0.6316\n",
      "Iteración 15663 - Lote 175/352 - Pérdida de Entrenamiento: 1.0556, Precisión de Entrenamiento: 0.6306\n",
      "Iteración 15698 - Lote 210/352 - Pérdida de Entrenamiento: 1.0625, Precisión de Entrenamiento: 0.6283\n",
      "Iteración 15733 - Lote 245/352 - Pérdida de Entrenamiento: 1.0610, Precisión de Entrenamiento: 0.6300\n",
      "Iteración 15768 - Lote 280/352 - Pérdida de Entrenamiento: 1.0620, Precisión de Entrenamiento: 0.6300\n",
      "Iteración 15803 - Lote 315/352 - Pérdida de Entrenamiento: 1.0650, Precisión de Entrenamiento: 0.6286\n",
      "Iteración 15838 - Lote 350/352 - Pérdida de Entrenamiento: 1.0646, Precisión de Entrenamiento: 0.6295\n",
      "Val loss: 1.0532, Val acc: 0.6166\n",
      "Gradientes para features.0.0.weight: min=-0.048140134662389755, max=0.04476390779018402, mean=-0.001511865295469761, std=0.011366669088602066\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.020152442157268524, max=0.019046546891331673, mean=1.0653334356902633e-06, std=0.0009716107742860913\n",
      "Gradientes para classifier.1.weight: min=-0.021800709888339043, max=0.019065452739596367, mean=1.164153192248496e-12, std=0.002283078385517001\n",
      "0.1\n",
      "Epoch 46/400\n",
      "Iteración 15875 - Lote 35/352 - Pérdida de Entrenamiento: 1.0678, Precisión de Entrenamiento: 0.6364\n",
      "Iteración 15910 - Lote 70/352 - Pérdida de Entrenamiento: 1.0578, Precisión de Entrenamiento: 0.6375\n",
      "Iteración 15945 - Lote 105/352 - Pérdida de Entrenamiento: 1.0567, Precisión de Entrenamiento: 0.6343\n",
      "Iteración 15980 - Lote 140/352 - Pérdida de Entrenamiento: 1.0503, Precisión de Entrenamiento: 0.6357\n",
      "Iteración 16015 - Lote 175/352 - Pérdida de Entrenamiento: 1.0506, Precisión de Entrenamiento: 0.6364\n",
      "Iteración 16050 - Lote 210/352 - Pérdida de Entrenamiento: 1.0494, Precisión de Entrenamiento: 0.6365\n",
      "Iteración 16085 - Lote 245/352 - Pérdida de Entrenamiento: 1.0567, Precisión de Entrenamiento: 0.6338\n",
      "Iteración 16120 - Lote 280/352 - Pérdida de Entrenamiento: 1.0577, Precisión de Entrenamiento: 0.6324\n",
      "Iteración 16155 - Lote 315/352 - Pérdida de Entrenamiento: 1.0605, Precisión de Entrenamiento: 0.6317\n",
      "Iteración 16190 - Lote 350/352 - Pérdida de Entrenamiento: 1.0650, Precisión de Entrenamiento: 0.6298\n",
      "Val loss: 1.1762, Val acc: 0.5896\n",
      "0.1\n",
      "Epoch 47/400\n",
      "Iteración 16227 - Lote 35/352 - Pérdida de Entrenamiento: 1.0344, Precisión de Entrenamiento: 0.6395\n",
      "Iteración 16262 - Lote 70/352 - Pérdida de Entrenamiento: 1.0423, Precisión de Entrenamiento: 0.6354\n",
      "Iteración 16297 - Lote 105/352 - Pérdida de Entrenamiento: 1.0414, Precisión de Entrenamiento: 0.6385\n",
      "Iteración 16332 - Lote 140/352 - Pérdida de Entrenamiento: 1.0503, Precisión de Entrenamiento: 0.6359\n",
      "Iteración 16367 - Lote 175/352 - Pérdida de Entrenamiento: 1.0490, Precisión de Entrenamiento: 0.6354\n",
      "Iteración 16402 - Lote 210/352 - Pérdida de Entrenamiento: 1.0517, Precisión de Entrenamiento: 0.6342\n",
      "Iteración 16437 - Lote 245/352 - Pérdida de Entrenamiento: 1.0546, Precisión de Entrenamiento: 0.6332\n",
      "Iteración 16472 - Lote 280/352 - Pérdida de Entrenamiento: 1.0557, Precisión de Entrenamiento: 0.6315\n",
      "Iteración 16507 - Lote 315/352 - Pérdida de Entrenamiento: 1.0588, Precisión de Entrenamiento: 0.6307\n",
      "Iteración 16542 - Lote 350/352 - Pérdida de Entrenamiento: 1.0580, Precisión de Entrenamiento: 0.6319\n",
      "Val loss: 4.6370, Val acc: 0.1880\n",
      "0.1\n",
      "Epoch 48/400\n",
      "Iteración 16579 - Lote 35/352 - Pérdida de Entrenamiento: 1.0653, Precisión de Entrenamiento: 0.6292\n",
      "Iteración 16614 - Lote 70/352 - Pérdida de Entrenamiento: 1.0707, Precisión de Entrenamiento: 0.6285\n",
      "Iteración 16649 - Lote 105/352 - Pérdida de Entrenamiento: 1.0667, Precisión de Entrenamiento: 0.6256\n",
      "Iteración 16684 - Lote 140/352 - Pérdida de Entrenamiento: 1.0694, Precisión de Entrenamiento: 0.6253\n",
      "Iteración 16719 - Lote 175/352 - Pérdida de Entrenamiento: 1.0690, Precisión de Entrenamiento: 0.6255\n",
      "Iteración 16754 - Lote 210/352 - Pérdida de Entrenamiento: 1.0732, Precisión de Entrenamiento: 0.6248\n",
      "Iteración 16789 - Lote 245/352 - Pérdida de Entrenamiento: 1.0708, Precisión de Entrenamiento: 0.6244\n",
      "Iteración 16824 - Lote 280/352 - Pérdida de Entrenamiento: 1.0662, Precisión de Entrenamiento: 0.6263\n",
      "Iteración 16859 - Lote 315/352 - Pérdida de Entrenamiento: 1.0649, Precisión de Entrenamiento: 0.6270\n",
      "Iteración 16894 - Lote 350/352 - Pérdida de Entrenamiento: 1.0672, Precisión de Entrenamiento: 0.6266\n",
      "Val loss: 1.1761, Val acc: 0.6036\n",
      "0.05\n",
      "Epoch 49/400\n",
      "Iteración 16931 - Lote 35/352 - Pérdida de Entrenamiento: 0.9941, Precisión de Entrenamiento: 0.6569\n",
      "Iteración 16966 - Lote 70/352 - Pérdida de Entrenamiento: 0.9603, Precisión de Entrenamiento: 0.6663\n",
      "Iteración 17001 - Lote 105/352 - Pérdida de Entrenamiento: 0.9572, Precisión de Entrenamiento: 0.6679\n",
      "Iteración 17036 - Lote 140/352 - Pérdida de Entrenamiento: 0.9487, Precisión de Entrenamiento: 0.6720\n",
      "Iteración 17071 - Lote 175/352 - Pérdida de Entrenamiento: 0.9384, Precisión de Entrenamiento: 0.6763\n",
      "Iteración 17106 - Lote 210/352 - Pérdida de Entrenamiento: 0.9363, Precisión de Entrenamiento: 0.6783\n",
      "Iteración 17141 - Lote 245/352 - Pérdida de Entrenamiento: 0.9310, Precisión de Entrenamiento: 0.6790\n",
      "Iteración 17176 - Lote 280/352 - Pérdida de Entrenamiento: 0.9245, Precisión de Entrenamiento: 0.6807\n",
      "Iteración 17211 - Lote 315/352 - Pérdida de Entrenamiento: 0.9247, Precisión de Entrenamiento: 0.6807\n",
      "Iteración 17246 - Lote 350/352 - Pérdida de Entrenamiento: 0.9273, Precisión de Entrenamiento: 0.6793\n",
      "Val loss: 0.9806, Val acc: 0.6652\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_49.pth\n",
      "Checkpoint del mejor modelo guardado en la época 49\n",
      "0.05\n",
      "Epoch 50/400\n",
      "Iteración 17283 - Lote 35/352 - Pérdida de Entrenamiento: 0.9307, Precisión de Entrenamiento: 0.6846\n",
      "Iteración 17318 - Lote 70/352 - Pérdida de Entrenamiento: 0.9159, Precisión de Entrenamiento: 0.6862\n",
      "Iteración 17353 - Lote 105/352 - Pérdida de Entrenamiento: 0.9145, Precisión de Entrenamiento: 0.6862\n",
      "Iteración 17388 - Lote 140/352 - Pérdida de Entrenamiento: 0.9166, Precisión de Entrenamiento: 0.6860\n",
      "Iteración 17423 - Lote 175/352 - Pérdida de Entrenamiento: 0.9198, Precisión de Entrenamiento: 0.6842\n",
      "Iteración 17458 - Lote 210/352 - Pérdida de Entrenamiento: 0.9216, Precisión de Entrenamiento: 0.6834\n",
      "Iteración 17493 - Lote 245/352 - Pérdida de Entrenamiento: 0.9247, Precisión de Entrenamiento: 0.6813\n",
      "Iteración 17528 - Lote 280/352 - Pérdida de Entrenamiento: 0.9241, Precisión de Entrenamiento: 0.6819\n",
      "Iteración 17563 - Lote 315/352 - Pérdida de Entrenamiento: 0.9231, Precisión de Entrenamiento: 0.6820\n",
      "Iteración 17598 - Lote 350/352 - Pérdida de Entrenamiento: 0.9246, Precisión de Entrenamiento: 0.6821\n",
      "Val loss: 0.9403, Val acc: 0.6708\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_50.pth\n",
      "Checkpoint del mejor modelo guardado en la época 50\n",
      "Gradientes para features.0.0.weight: min=-0.035636547952890396, max=0.05730947107076645, mean=-7.812723924871534e-05, std=0.013507711701095104\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.028658617287874222, max=0.026507830247282982, mean=-3.3802291454776423e-06, std=0.0009903759928420186\n",
      "Gradientes para classifier.1.weight: min=-0.010371650569140911, max=0.013783562928438187, mean=-4.656612768993984e-12, std=0.0017255875281989574\n",
      "0.05\n",
      "Epoch 51/400\n",
      "Iteración 17635 - Lote 35/352 - Pérdida de Entrenamiento: 0.9127, Precisión de Entrenamiento: 0.6815\n",
      "Iteración 17670 - Lote 70/352 - Pérdida de Entrenamiento: 0.9148, Precisión de Entrenamiento: 0.6802\n",
      "Iteración 17705 - Lote 105/352 - Pérdida de Entrenamiento: 0.9235, Precisión de Entrenamiento: 0.6787\n",
      "Iteración 17740 - Lote 140/352 - Pérdida de Entrenamiento: 0.9254, Precisión de Entrenamiento: 0.6803\n",
      "Iteración 17775 - Lote 175/352 - Pérdida de Entrenamiento: 0.9291, Precisión de Entrenamiento: 0.6792\n",
      "Iteración 17810 - Lote 210/352 - Pérdida de Entrenamiento: 0.9285, Precisión de Entrenamiento: 0.6795\n",
      "Iteración 17845 - Lote 245/352 - Pérdida de Entrenamiento: 0.9300, Precisión de Entrenamiento: 0.6795\n",
      "Iteración 17880 - Lote 280/352 - Pérdida de Entrenamiento: 0.9305, Precisión de Entrenamiento: 0.6796\n",
      "Iteración 17915 - Lote 315/352 - Pérdida de Entrenamiento: 0.9323, Precisión de Entrenamiento: 0.6793\n",
      "Iteración 17950 - Lote 350/352 - Pérdida de Entrenamiento: 0.9335, Precisión de Entrenamiento: 0.6788\n",
      "Val loss: 0.9132, Val acc: 0.6802\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_51.pth\n",
      "Checkpoint del mejor modelo guardado en la época 51\n",
      "0.05\n",
      "Epoch 52/400\n",
      "Iteración 17987 - Lote 35/352 - Pérdida de Entrenamiento: 0.9068, Precisión de Entrenamiento: 0.6893\n",
      "Iteración 18022 - Lote 70/352 - Pérdida de Entrenamiento: 0.9402, Precisión de Entrenamiento: 0.6765\n",
      "Iteración 18057 - Lote 105/352 - Pérdida de Entrenamiento: 0.9445, Precisión de Entrenamiento: 0.6737\n",
      "Iteración 18092 - Lote 140/352 - Pérdida de Entrenamiento: 0.9394, Precisión de Entrenamiento: 0.6747\n",
      "Iteración 18127 - Lote 175/352 - Pérdida de Entrenamiento: 0.9414, Precisión de Entrenamiento: 0.6755\n",
      "Iteración 18162 - Lote 210/352 - Pérdida de Entrenamiento: 0.9399, Precisión de Entrenamiento: 0.6767\n",
      "Iteración 18197 - Lote 245/352 - Pérdida de Entrenamiento: 0.9376, Precisión de Entrenamiento: 0.6771\n",
      "Iteración 18232 - Lote 280/352 - Pérdida de Entrenamiento: 0.9332, Precisión de Entrenamiento: 0.6789\n",
      "Iteración 18267 - Lote 315/352 - Pérdida de Entrenamiento: 0.9318, Precisión de Entrenamiento: 0.6792\n",
      "Iteración 18302 - Lote 350/352 - Pérdida de Entrenamiento: 0.9321, Precisión de Entrenamiento: 0.6801\n",
      "Val loss: 0.9618, Val acc: 0.6772\n",
      "0.05\n",
      "Epoch 53/400\n",
      "Iteración 18339 - Lote 35/352 - Pérdida de Entrenamiento: 0.9377, Precisión de Entrenamiento: 0.6792\n",
      "Iteración 18374 - Lote 70/352 - Pérdida de Entrenamiento: 0.9411, Precisión de Entrenamiento: 0.6766\n",
      "Iteración 18409 - Lote 105/352 - Pérdida de Entrenamiento: 0.9323, Precisión de Entrenamiento: 0.6789\n",
      "Iteración 18444 - Lote 140/352 - Pérdida de Entrenamiento: 0.9256, Precisión de Entrenamiento: 0.6811\n",
      "Iteración 18479 - Lote 175/352 - Pérdida de Entrenamiento: 0.9315, Precisión de Entrenamiento: 0.6784\n",
      "Iteración 18514 - Lote 210/352 - Pérdida de Entrenamiento: 0.9312, Precisión de Entrenamiento: 0.6791\n",
      "Iteración 18549 - Lote 245/352 - Pérdida de Entrenamiento: 0.9359, Precisión de Entrenamiento: 0.6776\n",
      "Iteración 18584 - Lote 280/352 - Pérdida de Entrenamiento: 0.9337, Precisión de Entrenamiento: 0.6786\n",
      "Iteración 18619 - Lote 315/352 - Pérdida de Entrenamiento: 0.9317, Precisión de Entrenamiento: 0.6793\n",
      "Iteración 18654 - Lote 350/352 - Pérdida de Entrenamiento: 0.9335, Precisión de Entrenamiento: 0.6793\n",
      "Val loss: 0.9148, Val acc: 0.6872\n",
      "0.05\n",
      "Epoch 54/400\n",
      "Iteración 18691 - Lote 35/352 - Pérdida de Entrenamiento: 0.9378, Precisión de Entrenamiento: 0.6848\n",
      "Iteración 18726 - Lote 70/352 - Pérdida de Entrenamiento: 0.9211, Precisión de Entrenamiento: 0.6831\n",
      "Iteración 18761 - Lote 105/352 - Pérdida de Entrenamiento: 0.9148, Precisión de Entrenamiento: 0.6854\n",
      "Iteración 18796 - Lote 140/352 - Pérdida de Entrenamiento: 0.9305, Precisión de Entrenamiento: 0.6787\n",
      "Iteración 18831 - Lote 175/352 - Pérdida de Entrenamiento: 0.9296, Precisión de Entrenamiento: 0.6799\n",
      "Iteración 18866 - Lote 210/352 - Pérdida de Entrenamiento: 0.9317, Precisión de Entrenamiento: 0.6797\n",
      "Iteración 18901 - Lote 245/352 - Pérdida de Entrenamiento: 0.9332, Precisión de Entrenamiento: 0.6793\n",
      "Iteración 18936 - Lote 280/352 - Pérdida de Entrenamiento: 0.9330, Precisión de Entrenamiento: 0.6792\n",
      "Iteración 18971 - Lote 315/352 - Pérdida de Entrenamiento: 0.9375, Precisión de Entrenamiento: 0.6772\n",
      "Iteración 19006 - Lote 350/352 - Pérdida de Entrenamiento: 0.9353, Precisión de Entrenamiento: 0.6767\n",
      "Val loss: 0.9409, Val acc: 0.6706\n",
      "0.05\n",
      "Epoch 55/400\n",
      "Iteración 19043 - Lote 35/352 - Pérdida de Entrenamiento: 0.9064, Precisión de Entrenamiento: 0.6828\n",
      "Iteración 19078 - Lote 70/352 - Pérdida de Entrenamiento: 0.9125, Precisión de Entrenamiento: 0.6811\n",
      "Iteración 19113 - Lote 105/352 - Pérdida de Entrenamiento: 0.9142, Precisión de Entrenamiento: 0.6812\n",
      "Iteración 19148 - Lote 140/352 - Pérdida de Entrenamiento: 0.9208, Precisión de Entrenamiento: 0.6785\n",
      "Iteración 19183 - Lote 175/352 - Pérdida de Entrenamiento: 0.9262, Precisión de Entrenamiento: 0.6783\n",
      "Iteración 19218 - Lote 210/352 - Pérdida de Entrenamiento: 0.9287, Precisión de Entrenamiento: 0.6773\n",
      "Iteración 19253 - Lote 245/352 - Pérdida de Entrenamiento: 0.9268, Precisión de Entrenamiento: 0.6770\n",
      "Iteración 19288 - Lote 280/352 - Pérdida de Entrenamiento: 0.9283, Precisión de Entrenamiento: 0.6768\n",
      "Iteración 19323 - Lote 315/352 - Pérdida de Entrenamiento: 0.9281, Precisión de Entrenamiento: 0.6782\n",
      "Iteración 19358 - Lote 350/352 - Pérdida de Entrenamiento: 0.9278, Precisión de Entrenamiento: 0.6777\n",
      "Val loss: 0.8927, Val acc: 0.6964\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_55.pth\n",
      "Checkpoint del mejor modelo guardado en la época 55\n",
      "Gradientes para features.0.0.weight: min=-0.08562804758548737, max=0.06913141906261444, mean=0.0022882043849676847, std=0.021772826090455055\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.04223834350705147, max=0.044752541929483414, mean=6.070876679586945e-06, std=0.0019848495721817017\n",
      "Gradientes para classifier.1.weight: min=-0.015781700611114502, max=0.015686815604567528, mean=1.3969838306981952e-11, std=0.002805639524012804\n",
      "0.05\n",
      "Epoch 56/400\n",
      "Iteración 19395 - Lote 35/352 - Pérdida de Entrenamiento: 0.9068, Precisión de Entrenamiento: 0.6846\n",
      "Iteración 19430 - Lote 70/352 - Pérdida de Entrenamiento: 0.9278, Precisión de Entrenamiento: 0.6804\n",
      "Iteración 19465 - Lote 105/352 - Pérdida de Entrenamiento: 0.9235, Precisión de Entrenamiento: 0.6831\n",
      "Iteración 19500 - Lote 140/352 - Pérdida de Entrenamiento: 0.9256, Precisión de Entrenamiento: 0.6824\n",
      "Iteración 19535 - Lote 175/352 - Pérdida de Entrenamiento: 0.9248, Precisión de Entrenamiento: 0.6844\n",
      "Iteración 19570 - Lote 210/352 - Pérdida de Entrenamiento: 0.9220, Precisión de Entrenamiento: 0.6847\n",
      "Iteración 19605 - Lote 245/352 - Pérdida de Entrenamiento: 0.9215, Precisión de Entrenamiento: 0.6846\n",
      "Iteración 19640 - Lote 280/352 - Pérdida de Entrenamiento: 0.9273, Precisión de Entrenamiento: 0.6818\n",
      "Iteración 19675 - Lote 315/352 - Pérdida de Entrenamiento: 0.9276, Precisión de Entrenamiento: 0.6821\n",
      "Iteración 19710 - Lote 350/352 - Pérdida de Entrenamiento: 0.9323, Precisión de Entrenamiento: 0.6800\n",
      "Val loss: 1.0354, Val acc: 0.6396\n",
      "0.05\n",
      "Epoch 57/400\n",
      "Iteración 19747 - Lote 35/352 - Pérdida de Entrenamiento: 0.9346, Precisión de Entrenamiento: 0.6759\n",
      "Iteración 19782 - Lote 70/352 - Pérdida de Entrenamiento: 0.9162, Precisión de Entrenamiento: 0.6807\n",
      "Iteración 19817 - Lote 105/352 - Pérdida de Entrenamiento: 0.9278, Precisión de Entrenamiento: 0.6797\n",
      "Iteración 19852 - Lote 140/352 - Pérdida de Entrenamiento: 0.9395, Precisión de Entrenamiento: 0.6761\n",
      "Iteración 19887 - Lote 175/352 - Pérdida de Entrenamiento: 0.9368, Precisión de Entrenamiento: 0.6770\n",
      "Iteración 19922 - Lote 210/352 - Pérdida de Entrenamiento: 0.9389, Precisión de Entrenamiento: 0.6770\n",
      "Iteración 19957 - Lote 245/352 - Pérdida de Entrenamiento: 0.9426, Precisión de Entrenamiento: 0.6759\n",
      "Iteración 19992 - Lote 280/352 - Pérdida de Entrenamiento: 0.9429, Precisión de Entrenamiento: 0.6756\n",
      "Iteración 20027 - Lote 315/352 - Pérdida de Entrenamiento: 0.9403, Precisión de Entrenamiento: 0.6763\n",
      "Iteración 20062 - Lote 350/352 - Pérdida de Entrenamiento: 0.9386, Precisión de Entrenamiento: 0.6767\n",
      "Val loss: 0.9397, Val acc: 0.6766\n",
      "0.05\n",
      "Epoch 58/400\n",
      "Iteración 20099 - Lote 35/352 - Pérdida de Entrenamiento: 0.9544, Precisión de Entrenamiento: 0.6703\n",
      "Iteración 20134 - Lote 70/352 - Pérdida de Entrenamiento: 0.9334, Precisión de Entrenamiento: 0.6808\n",
      "Iteración 20169 - Lote 105/352 - Pérdida de Entrenamiento: 0.9347, Precisión de Entrenamiento: 0.6803\n",
      "Iteración 20204 - Lote 140/352 - Pérdida de Entrenamiento: 0.9305, Precisión de Entrenamiento: 0.6812\n",
      "Iteración 20239 - Lote 175/352 - Pérdida de Entrenamiento: 0.9257, Precisión de Entrenamiento: 0.6832\n",
      "Iteración 20274 - Lote 210/352 - Pérdida de Entrenamiento: 0.9240, Precisión de Entrenamiento: 0.6832\n",
      "Iteración 20309 - Lote 245/352 - Pérdida de Entrenamiento: 0.9232, Precisión de Entrenamiento: 0.6843\n",
      "Iteración 20344 - Lote 280/352 - Pérdida de Entrenamiento: 0.9233, Precisión de Entrenamiento: 0.6859\n",
      "Iteración 20379 - Lote 315/352 - Pérdida de Entrenamiento: 0.9245, Precisión de Entrenamiento: 0.6849\n",
      "Iteración 20414 - Lote 350/352 - Pérdida de Entrenamiento: 0.9262, Precisión de Entrenamiento: 0.6847\n",
      "Val loss: 0.9208, Val acc: 0.6848\n",
      "0.05\n",
      "Epoch 59/400\n",
      "Iteración 20451 - Lote 35/352 - Pérdida de Entrenamiento: 0.9295, Precisión de Entrenamiento: 0.6783\n",
      "Iteración 20486 - Lote 70/352 - Pérdida de Entrenamiento: 0.9189, Precisión de Entrenamiento: 0.6828\n",
      "Iteración 20521 - Lote 105/352 - Pérdida de Entrenamiento: 0.9269, Precisión de Entrenamiento: 0.6802\n",
      "Iteración 20556 - Lote 140/352 - Pérdida de Entrenamiento: 0.9264, Precisión de Entrenamiento: 0.6822\n",
      "Iteración 20591 - Lote 175/352 - Pérdida de Entrenamiento: 0.9238, Precisión de Entrenamiento: 0.6820\n",
      "Iteración 20626 - Lote 210/352 - Pérdida de Entrenamiento: 0.9207, Precisión de Entrenamiento: 0.6825\n",
      "Iteración 20661 - Lote 245/352 - Pérdida de Entrenamiento: 0.9227, Precisión de Entrenamiento: 0.6824\n",
      "Iteración 20696 - Lote 280/352 - Pérdida de Entrenamiento: 0.9250, Precisión de Entrenamiento: 0.6819\n",
      "Iteración 20731 - Lote 315/352 - Pérdida de Entrenamiento: 0.9225, Precisión de Entrenamiento: 0.6829\n",
      "Iteración 20766 - Lote 350/352 - Pérdida de Entrenamiento: 0.9193, Precisión de Entrenamiento: 0.6840\n",
      "Val loss: 0.9690, Val acc: 0.6644\n",
      "0.05\n",
      "Epoch 60/400\n",
      "Iteración 20803 - Lote 35/352 - Pérdida de Entrenamiento: 0.9081, Precisión de Entrenamiento: 0.6844\n",
      "Iteración 20838 - Lote 70/352 - Pérdida de Entrenamiento: 0.9221, Precisión de Entrenamiento: 0.6854\n",
      "Iteración 20873 - Lote 105/352 - Pérdida de Entrenamiento: 0.9148, Precisión de Entrenamiento: 0.6859\n",
      "Iteración 20908 - Lote 140/352 - Pérdida de Entrenamiento: 0.9155, Precisión de Entrenamiento: 0.6857\n",
      "Iteración 20943 - Lote 175/352 - Pérdida de Entrenamiento: 0.9206, Precisión de Entrenamiento: 0.6841\n",
      "Iteración 20978 - Lote 210/352 - Pérdida de Entrenamiento: 0.9242, Precisión de Entrenamiento: 0.6817\n",
      "Iteración 21013 - Lote 245/352 - Pérdida de Entrenamiento: 0.9243, Precisión de Entrenamiento: 0.6815\n",
      "Iteración 21048 - Lote 280/352 - Pérdida de Entrenamiento: 0.9247, Precisión de Entrenamiento: 0.6818\n",
      "Iteración 21083 - Lote 315/352 - Pérdida de Entrenamiento: 0.9237, Precisión de Entrenamiento: 0.6827\n",
      "Iteración 21118 - Lote 350/352 - Pérdida de Entrenamiento: 0.9241, Precisión de Entrenamiento: 0.6821\n",
      "Val loss: 1.0625, Val acc: 0.6468\n",
      "Gradientes para features.0.0.weight: min=-0.08702361583709717, max=0.05285707488656044, mean=-0.002351307775825262, std=0.017686884850263596\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.02072436362504959, max=0.0411502905189991, mean=3.479882479950902e-06, std=0.0011506628943607211\n",
      "Gradientes para classifier.1.weight: min=-0.015328842215240002, max=0.014298930764198303, mean=4.656612768993984e-12, std=0.0023424436803907156\n",
      "0.05\n",
      "Epoch 61/400\n",
      "Iteración 21155 - Lote 35/352 - Pérdida de Entrenamiento: 0.9159, Precisión de Entrenamiento: 0.6868\n",
      "Iteración 21190 - Lote 70/352 - Pérdida de Entrenamiento: 0.9050, Precisión de Entrenamiento: 0.6911\n",
      "Iteración 21225 - Lote 105/352 - Pérdida de Entrenamiento: 0.9164, Precisión de Entrenamiento: 0.6873\n",
      "Iteración 21260 - Lote 140/352 - Pérdida de Entrenamiento: 0.9237, Precisión de Entrenamiento: 0.6855\n",
      "Iteración 21295 - Lote 175/352 - Pérdida de Entrenamiento: 0.9206, Precisión de Entrenamiento: 0.6855\n",
      "Iteración 21330 - Lote 210/352 - Pérdida de Entrenamiento: 0.9170, Precisión de Entrenamiento: 0.6863\n",
      "Iteración 21365 - Lote 245/352 - Pérdida de Entrenamiento: 0.9172, Precisión de Entrenamiento: 0.6864\n",
      "Iteración 21400 - Lote 280/352 - Pérdida de Entrenamiento: 0.9171, Precisión de Entrenamiento: 0.6854\n",
      "Iteración 21435 - Lote 315/352 - Pérdida de Entrenamiento: 0.9169, Precisión de Entrenamiento: 0.6858\n",
      "Iteración 21470 - Lote 350/352 - Pérdida de Entrenamiento: 0.9200, Precisión de Entrenamiento: 0.6856\n",
      "Val loss: 0.9618, Val acc: 0.6634\n",
      "0.05\n",
      "Epoch 62/400\n",
      "Iteración 21507 - Lote 35/352 - Pérdida de Entrenamiento: 0.9146, Precisión de Entrenamiento: 0.6904\n",
      "Iteración 21542 - Lote 70/352 - Pérdida de Entrenamiento: 0.9069, Precisión de Entrenamiento: 0.6922\n",
      "Iteración 21577 - Lote 105/352 - Pérdida de Entrenamiento: 0.9086, Precisión de Entrenamiento: 0.6906\n",
      "Iteración 21612 - Lote 140/352 - Pérdida de Entrenamiento: 0.9082, Precisión de Entrenamiento: 0.6891\n",
      "Iteración 21647 - Lote 175/352 - Pérdida de Entrenamiento: 0.9112, Precisión de Entrenamiento: 0.6883\n",
      "Iteración 21682 - Lote 210/352 - Pérdida de Entrenamiento: 0.9137, Precisión de Entrenamiento: 0.6872\n",
      "Iteración 21717 - Lote 245/352 - Pérdida de Entrenamiento: 0.9114, Precisión de Entrenamiento: 0.6884\n",
      "Iteración 21752 - Lote 280/352 - Pérdida de Entrenamiento: 0.9140, Precisión de Entrenamiento: 0.6864\n",
      "Iteración 21787 - Lote 315/352 - Pérdida de Entrenamiento: 0.9101, Precisión de Entrenamiento: 0.6874\n",
      "Iteración 21822 - Lote 350/352 - Pérdida de Entrenamiento: 0.9101, Precisión de Entrenamiento: 0.6879\n",
      "Val loss: 0.9639, Val acc: 0.6654\n",
      "0.05\n",
      "Epoch 63/400\n",
      "Iteración 21859 - Lote 35/352 - Pérdida de Entrenamiento: 0.9452, Precisión de Entrenamiento: 0.6859\n",
      "Iteración 21894 - Lote 70/352 - Pérdida de Entrenamiento: 0.9271, Precisión de Entrenamiento: 0.6882\n",
      "Iteración 21929 - Lote 105/352 - Pérdida de Entrenamiento: 0.9107, Precisión de Entrenamiento: 0.6937\n",
      "Iteración 21964 - Lote 140/352 - Pérdida de Entrenamiento: 0.9117, Precisión de Entrenamiento: 0.6906\n",
      "Iteración 21999 - Lote 175/352 - Pérdida de Entrenamiento: 0.9112, Precisión de Entrenamiento: 0.6890\n",
      "Iteración 22034 - Lote 210/352 - Pérdida de Entrenamiento: 0.9131, Precisión de Entrenamiento: 0.6883\n",
      "Iteración 22069 - Lote 245/352 - Pérdida de Entrenamiento: 0.9115, Precisión de Entrenamiento: 0.6892\n",
      "Iteración 22104 - Lote 280/352 - Pérdida de Entrenamiento: 0.9097, Precisión de Entrenamiento: 0.6901\n",
      "Iteración 22139 - Lote 315/352 - Pérdida de Entrenamiento: 0.9104, Precisión de Entrenamiento: 0.6894\n",
      "Iteración 22174 - Lote 350/352 - Pérdida de Entrenamiento: 0.9121, Precisión de Entrenamiento: 0.6894\n",
      "Val loss: 0.9980, Val acc: 0.6658\n",
      "0.05\n",
      "Epoch 64/400\n",
      "Iteración 22211 - Lote 35/352 - Pérdida de Entrenamiento: 0.8836, Precisión de Entrenamiento: 0.6949\n",
      "Iteración 22246 - Lote 70/352 - Pérdida de Entrenamiento: 0.9006, Precisión de Entrenamiento: 0.6946\n",
      "Iteración 22281 - Lote 105/352 - Pérdida de Entrenamiento: 0.8966, Precisión de Entrenamiento: 0.6968\n",
      "Iteración 22316 - Lote 140/352 - Pérdida de Entrenamiento: 0.8977, Precisión de Entrenamiento: 0.6958\n",
      "Iteración 22351 - Lote 175/352 - Pérdida de Entrenamiento: 0.9035, Precisión de Entrenamiento: 0.6934\n",
      "Iteración 22386 - Lote 210/352 - Pérdida de Entrenamiento: 0.9083, Precisión de Entrenamiento: 0.6917\n",
      "Iteración 22421 - Lote 245/352 - Pérdida de Entrenamiento: 0.9145, Precisión de Entrenamiento: 0.6894\n",
      "Iteración 22456 - Lote 280/352 - Pérdida de Entrenamiento: 0.9167, Precisión de Entrenamiento: 0.6896\n",
      "Iteración 22491 - Lote 315/352 - Pérdida de Entrenamiento: 0.9097, Precisión de Entrenamiento: 0.6920\n",
      "Iteración 22526 - Lote 350/352 - Pérdida de Entrenamiento: 0.9094, Precisión de Entrenamiento: 0.6915\n",
      "Val loss: 0.9974, Val acc: 0.6542\n",
      "0.025\n",
      "Epoch 65/400\n",
      "Iteración 22563 - Lote 35/352 - Pérdida de Entrenamiento: 0.8610, Precisión de Entrenamiento: 0.7033\n",
      "Iteración 22598 - Lote 70/352 - Pérdida de Entrenamiento: 0.8312, Precisión de Entrenamiento: 0.7141\n",
      "Iteración 22633 - Lote 105/352 - Pérdida de Entrenamiento: 0.8208, Precisión de Entrenamiento: 0.7168\n",
      "Iteración 22668 - Lote 140/352 - Pérdida de Entrenamiento: 0.8116, Precisión de Entrenamiento: 0.7215\n",
      "Iteración 22703 - Lote 175/352 - Pérdida de Entrenamiento: 0.8093, Precisión de Entrenamiento: 0.7223\n",
      "Iteración 22738 - Lote 210/352 - Pérdida de Entrenamiento: 0.8059, Precisión de Entrenamiento: 0.7235\n",
      "Iteración 22773 - Lote 245/352 - Pérdida de Entrenamiento: 0.8024, Precisión de Entrenamiento: 0.7257\n",
      "Iteración 22808 - Lote 280/352 - Pérdida de Entrenamiento: 0.7984, Precisión de Entrenamiento: 0.7281\n",
      "Iteración 22843 - Lote 315/352 - Pérdida de Entrenamiento: 0.7999, Precisión de Entrenamiento: 0.7272\n",
      "Iteración 22878 - Lote 350/352 - Pérdida de Entrenamiento: 0.7991, Precisión de Entrenamiento: 0.7272\n",
      "Val loss: 0.7974, Val acc: 0.7312\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_65.pth\n",
      "Checkpoint del mejor modelo guardado en la época 65\n",
      "Gradientes para features.0.0.weight: min=-0.0631551668047905, max=0.11892867088317871, mean=0.007712736260145903, std=0.03295198827981949\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.03846728429198265, max=0.04876388609409332, mean=-1.044076543621486e-05, std=0.0019033929565921426\n",
      "Gradientes para classifier.1.weight: min=-0.01823613978922367, max=0.01903880015015602, mean=0.0, std=0.0024441361892968416\n",
      "0.025\n",
      "Epoch 66/400\n",
      "Iteración 22915 - Lote 35/352 - Pérdida de Entrenamiento: 0.8047, Precisión de Entrenamiento: 0.7299\n",
      "Iteración 22950 - Lote 70/352 - Pérdida de Entrenamiento: 0.8089, Precisión de Entrenamiento: 0.7265\n",
      "Iteración 22985 - Lote 105/352 - Pérdida de Entrenamiento: 0.8116, Precisión de Entrenamiento: 0.7231\n",
      "Iteración 23020 - Lote 140/352 - Pérdida de Entrenamiento: 0.8046, Precisión de Entrenamiento: 0.7242\n",
      "Iteración 23055 - Lote 175/352 - Pérdida de Entrenamiento: 0.8044, Precisión de Entrenamiento: 0.7245\n",
      "Iteración 23090 - Lote 210/352 - Pérdida de Entrenamiento: 0.8028, Precisión de Entrenamiento: 0.7251\n",
      "Iteración 23125 - Lote 245/352 - Pérdida de Entrenamiento: 0.7982, Precisión de Entrenamiento: 0.7268\n",
      "Iteración 23160 - Lote 280/352 - Pérdida de Entrenamiento: 0.7980, Precisión de Entrenamiento: 0.7284\n",
      "Iteración 23195 - Lote 315/352 - Pérdida de Entrenamiento: 0.8009, Precisión de Entrenamiento: 0.7273\n",
      "Iteración 23230 - Lote 350/352 - Pérdida de Entrenamiento: 0.8026, Precisión de Entrenamiento: 0.7266\n",
      "Val loss: 0.7891, Val acc: 0.7324\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_66.pth\n",
      "Checkpoint del mejor modelo guardado en la época 66\n",
      "0.025\n",
      "Epoch 67/400\n",
      "Iteración 23267 - Lote 35/352 - Pérdida de Entrenamiento: 0.8167, Precisión de Entrenamiento: 0.7283\n",
      "Iteración 23302 - Lote 70/352 - Pérdida de Entrenamiento: 0.7999, Precisión de Entrenamiento: 0.7279\n",
      "Iteración 23337 - Lote 105/352 - Pérdida de Entrenamiento: 0.8026, Precisión de Entrenamiento: 0.7258\n",
      "Iteración 23372 - Lote 140/352 - Pérdida de Entrenamiento: 0.8008, Precisión de Entrenamiento: 0.7256\n",
      "Iteración 23407 - Lote 175/352 - Pérdida de Entrenamiento: 0.7999, Precisión de Entrenamiento: 0.7279\n",
      "Iteración 23442 - Lote 210/352 - Pérdida de Entrenamiento: 0.7953, Precisión de Entrenamiento: 0.7302\n",
      "Iteración 23477 - Lote 245/352 - Pérdida de Entrenamiento: 0.7992, Precisión de Entrenamiento: 0.7283\n",
      "Iteración 23512 - Lote 280/352 - Pérdida de Entrenamiento: 0.7954, Precisión de Entrenamiento: 0.7291\n",
      "Iteración 23547 - Lote 315/352 - Pérdida de Entrenamiento: 0.7960, Precisión de Entrenamiento: 0.7289\n",
      "Iteración 23582 - Lote 350/352 - Pérdida de Entrenamiento: 0.7972, Precisión de Entrenamiento: 0.7280\n",
      "Val loss: 0.7794, Val acc: 0.7286\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_67.pth\n",
      "Checkpoint del mejor modelo guardado en la época 67\n",
      "0.025\n",
      "Epoch 68/400\n",
      "Iteración 23619 - Lote 35/352 - Pérdida de Entrenamiento: 0.8102, Precisión de Entrenamiento: 0.7292\n",
      "Iteración 23654 - Lote 70/352 - Pérdida de Entrenamiento: 0.7989, Precisión de Entrenamiento: 0.7317\n",
      "Iteración 23689 - Lote 105/352 - Pérdida de Entrenamiento: 0.7981, Precisión de Entrenamiento: 0.7324\n",
      "Iteración 23724 - Lote 140/352 - Pérdida de Entrenamiento: 0.8008, Precisión de Entrenamiento: 0.7297\n",
      "Iteración 23759 - Lote 175/352 - Pérdida de Entrenamiento: 0.8023, Precisión de Entrenamiento: 0.7277\n",
      "Iteración 23794 - Lote 210/352 - Pérdida de Entrenamiento: 0.8004, Precisión de Entrenamiento: 0.7294\n",
      "Iteración 23829 - Lote 245/352 - Pérdida de Entrenamiento: 0.8002, Precisión de Entrenamiento: 0.7292\n",
      "Iteración 23864 - Lote 280/352 - Pérdida de Entrenamiento: 0.8046, Precisión de Entrenamiento: 0.7282\n",
      "Iteración 23899 - Lote 315/352 - Pérdida de Entrenamiento: 0.8053, Precisión de Entrenamiento: 0.7275\n",
      "Iteración 23934 - Lote 350/352 - Pérdida de Entrenamiento: 0.8059, Precisión de Entrenamiento: 0.7270\n",
      "Val loss: 0.8320, Val acc: 0.7112\n",
      "0.025\n",
      "Epoch 69/400\n",
      "Iteración 23971 - Lote 35/352 - Pérdida de Entrenamiento: 0.8052, Precisión de Entrenamiento: 0.7272\n",
      "Iteración 24006 - Lote 70/352 - Pérdida de Entrenamiento: 0.8066, Precisión de Entrenamiento: 0.7231\n",
      "Iteración 24041 - Lote 105/352 - Pérdida de Entrenamiento: 0.8070, Precisión de Entrenamiento: 0.7257\n",
      "Iteración 24076 - Lote 140/352 - Pérdida de Entrenamiento: 0.8046, Precisión de Entrenamiento: 0.7300\n",
      "Iteración 24111 - Lote 175/352 - Pérdida de Entrenamiento: 0.8013, Precisión de Entrenamiento: 0.7300\n",
      "Iteración 24146 - Lote 210/352 - Pérdida de Entrenamiento: 0.8004, Precisión de Entrenamiento: 0.7309\n",
      "Iteración 24181 - Lote 245/352 - Pérdida de Entrenamiento: 0.7996, Precisión de Entrenamiento: 0.7318\n",
      "Iteración 24216 - Lote 280/352 - Pérdida de Entrenamiento: 0.7988, Precisión de Entrenamiento: 0.7319\n",
      "Iteración 24251 - Lote 315/352 - Pérdida de Entrenamiento: 0.8014, Precisión de Entrenamiento: 0.7307\n",
      "Iteración 24286 - Lote 350/352 - Pérdida de Entrenamiento: 0.8018, Precisión de Entrenamiento: 0.7297\n",
      "Val loss: 0.8558, Val acc: 0.7116\n",
      "0.025\n",
      "Epoch 70/400\n",
      "Iteración 24323 - Lote 35/352 - Pérdida de Entrenamiento: 0.8199, Precisión de Entrenamiento: 0.7185\n",
      "Iteración 24358 - Lote 70/352 - Pérdida de Entrenamiento: 0.8020, Precisión de Entrenamiento: 0.7263\n",
      "Iteración 24393 - Lote 105/352 - Pérdida de Entrenamiento: 0.8016, Precisión de Entrenamiento: 0.7269\n",
      "Iteración 24428 - Lote 140/352 - Pérdida de Entrenamiento: 0.8043, Precisión de Entrenamiento: 0.7268\n",
      "Iteración 24463 - Lote 175/352 - Pérdida de Entrenamiento: 0.8047, Precisión de Entrenamiento: 0.7246\n",
      "Iteración 24498 - Lote 210/352 - Pérdida de Entrenamiento: 0.8040, Precisión de Entrenamiento: 0.7257\n",
      "Iteración 24533 - Lote 245/352 - Pérdida de Entrenamiento: 0.8060, Precisión de Entrenamiento: 0.7256\n",
      "Iteración 24568 - Lote 280/352 - Pérdida de Entrenamiento: 0.8049, Precisión de Entrenamiento: 0.7266\n",
      "Iteración 24603 - Lote 315/352 - Pérdida de Entrenamiento: 0.8079, Precisión de Entrenamiento: 0.7251\n",
      "Iteración 24638 - Lote 350/352 - Pérdida de Entrenamiento: 0.8048, Precisión de Entrenamiento: 0.7262\n",
      "Val loss: 0.8933, Val acc: 0.6996\n",
      "Gradientes para features.0.0.weight: min=-0.06670870631933212, max=0.07549725472927094, mean=0.0013277018442749977, std=0.014448094181716442\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.04063527658581734, max=0.0748639702796936, mean=-3.082559715039679e-06, std=0.0014371100114658475\n",
      "Gradientes para classifier.1.weight: min=-0.013108581304550171, max=0.012215392664074898, mean=-8.149072562579907e-12, std=0.0016881955089047551\n",
      "0.025\n",
      "Epoch 71/400\n",
      "Iteración 24675 - Lote 35/352 - Pérdida de Entrenamiento: 0.8306, Precisión de Entrenamiento: 0.7152\n",
      "Iteración 24710 - Lote 70/352 - Pérdida de Entrenamiento: 0.8131, Precisión de Entrenamiento: 0.7195\n",
      "Iteración 24745 - Lote 105/352 - Pérdida de Entrenamiento: 0.8007, Precisión de Entrenamiento: 0.7246\n",
      "Iteración 24780 - Lote 140/352 - Pérdida de Entrenamiento: 0.8072, Precisión de Entrenamiento: 0.7244\n",
      "Iteración 24815 - Lote 175/352 - Pérdida de Entrenamiento: 0.8067, Precisión de Entrenamiento: 0.7250\n",
      "Iteración 24850 - Lote 210/352 - Pérdida de Entrenamiento: 0.8069, Precisión de Entrenamiento: 0.7254\n",
      "Iteración 24885 - Lote 245/352 - Pérdida de Entrenamiento: 0.8076, Precisión de Entrenamiento: 0.7256\n",
      "Iteración 24920 - Lote 280/352 - Pérdida de Entrenamiento: 0.8085, Precisión de Entrenamiento: 0.7253\n",
      "Iteración 24955 - Lote 315/352 - Pérdida de Entrenamiento: 0.8083, Precisión de Entrenamiento: 0.7254\n",
      "Iteración 24990 - Lote 350/352 - Pérdida de Entrenamiento: 0.8090, Precisión de Entrenamiento: 0.7254\n",
      "Val loss: 0.8130, Val acc: 0.7228\n",
      "0.025\n",
      "Epoch 72/400\n",
      "Iteración 25027 - Lote 35/352 - Pérdida de Entrenamiento: 0.7861, Precisión de Entrenamiento: 0.7371\n",
      "Iteración 25062 - Lote 70/352 - Pérdida de Entrenamiento: 0.8059, Precisión de Entrenamiento: 0.7288\n",
      "Iteración 25097 - Lote 105/352 - Pérdida de Entrenamiento: 0.8164, Precisión de Entrenamiento: 0.7214\n",
      "Iteración 25132 - Lote 140/352 - Pérdida de Entrenamiento: 0.8098, Precisión de Entrenamiento: 0.7242\n",
      "Iteración 25167 - Lote 175/352 - Pérdida de Entrenamiento: 0.8109, Precisión de Entrenamiento: 0.7242\n",
      "Iteración 25202 - Lote 210/352 - Pérdida de Entrenamiento: 0.8118, Precisión de Entrenamiento: 0.7231\n",
      "Iteración 25237 - Lote 245/352 - Pérdida de Entrenamiento: 0.8060, Precisión de Entrenamiento: 0.7254\n",
      "Iteración 25272 - Lote 280/352 - Pérdida de Entrenamiento: 0.8070, Precisión de Entrenamiento: 0.7255\n",
      "Iteración 25307 - Lote 315/352 - Pérdida de Entrenamiento: 0.8063, Precisión de Entrenamiento: 0.7258\n",
      "Iteración 25342 - Lote 350/352 - Pérdida de Entrenamiento: 0.8037, Precisión de Entrenamiento: 0.7270\n",
      "Val loss: 0.7822, Val acc: 0.7322\n",
      "0.025\n",
      "Epoch 73/400\n",
      "Iteración 25379 - Lote 35/352 - Pérdida de Entrenamiento: 0.7681, Precisión de Entrenamiento: 0.7362\n",
      "Iteración 25414 - Lote 70/352 - Pérdida de Entrenamiento: 0.7857, Precisión de Entrenamiento: 0.7286\n",
      "Iteración 25449 - Lote 105/352 - Pérdida de Entrenamiento: 0.7954, Precisión de Entrenamiento: 0.7263\n",
      "Iteración 25484 - Lote 140/352 - Pérdida de Entrenamiento: 0.8007, Precisión de Entrenamiento: 0.7259\n",
      "Iteración 25519 - Lote 175/352 - Pérdida de Entrenamiento: 0.8002, Precisión de Entrenamiento: 0.7273\n",
      "Iteración 25554 - Lote 210/352 - Pérdida de Entrenamiento: 0.8035, Precisión de Entrenamiento: 0.7260\n",
      "Iteración 25589 - Lote 245/352 - Pérdida de Entrenamiento: 0.8073, Precisión de Entrenamiento: 0.7251\n",
      "Iteración 25624 - Lote 280/352 - Pérdida de Entrenamiento: 0.8067, Precisión de Entrenamiento: 0.7255\n",
      "Iteración 25659 - Lote 315/352 - Pérdida de Entrenamiento: 0.8054, Precisión de Entrenamiento: 0.7260\n",
      "Iteración 25694 - Lote 350/352 - Pérdida de Entrenamiento: 0.8034, Precisión de Entrenamiento: 0.7262\n",
      "Val loss: 0.8145, Val acc: 0.7230\n",
      "0.025\n",
      "Epoch 74/400\n",
      "Iteración 25731 - Lote 35/352 - Pérdida de Entrenamiento: 0.8230, Precisión de Entrenamiento: 0.7254\n",
      "Iteración 25766 - Lote 70/352 - Pérdida de Entrenamiento: 0.7987, Precisión de Entrenamiento: 0.7308\n",
      "Iteración 25801 - Lote 105/352 - Pérdida de Entrenamiento: 0.7971, Precisión de Entrenamiento: 0.7283\n",
      "Iteración 25836 - Lote 140/352 - Pérdida de Entrenamiento: 0.8006, Precisión de Entrenamiento: 0.7267\n",
      "Iteración 25871 - Lote 175/352 - Pérdida de Entrenamiento: 0.7975, Precisión de Entrenamiento: 0.7280\n",
      "Iteración 25906 - Lote 210/352 - Pérdida de Entrenamiento: 0.7964, Precisión de Entrenamiento: 0.7285\n",
      "Iteración 25941 - Lote 245/352 - Pérdida de Entrenamiento: 0.7981, Precisión de Entrenamiento: 0.7276\n",
      "Iteración 25976 - Lote 280/352 - Pérdida de Entrenamiento: 0.7978, Precisión de Entrenamiento: 0.7279\n",
      "Iteración 26011 - Lote 315/352 - Pérdida de Entrenamiento: 0.7990, Precisión de Entrenamiento: 0.7271\n",
      "Iteración 26046 - Lote 350/352 - Pérdida de Entrenamiento: 0.8038, Precisión de Entrenamiento: 0.7253\n",
      "Val loss: 0.8572, Val acc: 0.7140\n",
      "0.025\n",
      "Epoch 75/400\n",
      "Iteración 26083 - Lote 35/352 - Pérdida de Entrenamiento: 0.7597, Precisión de Entrenamiento: 0.7415\n",
      "Iteración 26118 - Lote 70/352 - Pérdida de Entrenamiento: 0.7787, Precisión de Entrenamiento: 0.7362\n",
      "Iteración 26153 - Lote 105/352 - Pérdida de Entrenamiento: 0.7969, Precisión de Entrenamiento: 0.7304\n",
      "Iteración 26188 - Lote 140/352 - Pérdida de Entrenamiento: 0.7961, Precisión de Entrenamiento: 0.7290\n",
      "Iteración 26223 - Lote 175/352 - Pérdida de Entrenamiento: 0.8030, Precisión de Entrenamiento: 0.7274\n",
      "Iteración 26258 - Lote 210/352 - Pérdida de Entrenamiento: 0.8005, Precisión de Entrenamiento: 0.7284\n",
      "Iteración 26293 - Lote 245/352 - Pérdida de Entrenamiento: 0.8062, Precisión de Entrenamiento: 0.7275\n",
      "Iteración 26328 - Lote 280/352 - Pérdida de Entrenamiento: 0.8036, Precisión de Entrenamiento: 0.7284\n",
      "Iteración 26363 - Lote 315/352 - Pérdida de Entrenamiento: 0.8027, Precisión de Entrenamiento: 0.7281\n",
      "Iteración 26398 - Lote 350/352 - Pérdida de Entrenamiento: 0.8015, Precisión de Entrenamiento: 0.7286\n",
      "Val loss: 0.7950, Val acc: 0.7314\n",
      "Gradientes para features.0.0.weight: min=-0.06486622244119644, max=0.08127609640359879, mean=-5.939678885624744e-05, std=0.023355985060334206\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.0389639213681221, max=0.047021426260471344, mean=2.357857056267676e-06, std=0.0019132866291329265\n",
      "Gradientes para classifier.1.weight: min=-0.016981827095150948, max=0.01607731357216835, mean=-9.313225537987968e-12, std=0.0017658569850027561\n",
      "0.025\n",
      "Epoch 76/400\n",
      "Iteración 26435 - Lote 35/352 - Pérdida de Entrenamiento: 0.7821, Precisión de Entrenamiento: 0.7310\n",
      "Iteración 26470 - Lote 70/352 - Pérdida de Entrenamiento: 0.7858, Precisión de Entrenamiento: 0.7309\n",
      "Iteración 26505 - Lote 105/352 - Pérdida de Entrenamiento: 0.7953, Precisión de Entrenamiento: 0.7304\n",
      "Iteración 26540 - Lote 140/352 - Pérdida de Entrenamiento: 0.8007, Precisión de Entrenamiento: 0.7300\n",
      "Iteración 26575 - Lote 175/352 - Pérdida de Entrenamiento: 0.8051, Precisión de Entrenamiento: 0.7279\n",
      "Iteración 26610 - Lote 210/352 - Pérdida de Entrenamiento: 0.7987, Precisión de Entrenamiento: 0.7303\n",
      "Iteración 26645 - Lote 245/352 - Pérdida de Entrenamiento: 0.8038, Precisión de Entrenamiento: 0.7283\n",
      "Iteración 26680 - Lote 280/352 - Pérdida de Entrenamiento: 0.8033, Precisión de Entrenamiento: 0.7276\n",
      "Iteración 26715 - Lote 315/352 - Pérdida de Entrenamiento: 0.8023, Precisión de Entrenamiento: 0.7281\n",
      "Iteración 26750 - Lote 350/352 - Pérdida de Entrenamiento: 0.8007, Precisión de Entrenamiento: 0.7293\n",
      "Val loss: 0.8255, Val acc: 0.7148\n",
      "0.0125\n",
      "Epoch 77/400\n",
      "Iteración 26787 - Lote 35/352 - Pérdida de Entrenamiento: 0.7403, Precisión de Entrenamiento: 0.7518\n",
      "Iteración 26822 - Lote 70/352 - Pérdida de Entrenamiento: 0.7214, Precisión de Entrenamiento: 0.7571\n",
      "Iteración 26857 - Lote 105/352 - Pérdida de Entrenamiento: 0.7164, Precisión de Entrenamiento: 0.7577\n",
      "Iteración 26892 - Lote 140/352 - Pérdida de Entrenamiento: 0.7217, Precisión de Entrenamiento: 0.7565\n",
      "Iteración 26927 - Lote 175/352 - Pérdida de Entrenamiento: 0.7166, Precisión de Entrenamiento: 0.7582\n",
      "Iteración 26962 - Lote 210/352 - Pérdida de Entrenamiento: 0.7159, Precisión de Entrenamiento: 0.7576\n",
      "Iteración 26997 - Lote 245/352 - Pérdida de Entrenamiento: 0.7152, Precisión de Entrenamiento: 0.7577\n",
      "Iteración 27032 - Lote 280/352 - Pérdida de Entrenamiento: 0.7169, Precisión de Entrenamiento: 0.7570\n",
      "Iteración 27067 - Lote 315/352 - Pérdida de Entrenamiento: 0.7146, Precisión de Entrenamiento: 0.7588\n",
      "Iteración 27102 - Lote 350/352 - Pérdida de Entrenamiento: 0.7163, Precisión de Entrenamiento: 0.7579\n",
      "Val loss: 0.7362, Val acc: 0.7524\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_77.pth\n",
      "Checkpoint del mejor modelo guardado en la época 77\n",
      "0.0125\n",
      "Epoch 78/400\n",
      "Iteración 27139 - Lote 35/352 - Pérdida de Entrenamiento: 0.7073, Precisión de Entrenamiento: 0.7600\n",
      "Iteración 27174 - Lote 70/352 - Pérdida de Entrenamiento: 0.7090, Precisión de Entrenamiento: 0.7645\n",
      "Iteración 27209 - Lote 105/352 - Pérdida de Entrenamiento: 0.6978, Precisión de Entrenamiento: 0.7668\n",
      "Iteración 27244 - Lote 140/352 - Pérdida de Entrenamiento: 0.6985, Precisión de Entrenamiento: 0.7664\n",
      "Iteración 27279 - Lote 175/352 - Pérdida de Entrenamiento: 0.7037, Precisión de Entrenamiento: 0.7648\n",
      "Iteración 27314 - Lote 210/352 - Pérdida de Entrenamiento: 0.7021, Precisión de Entrenamiento: 0.7633\n",
      "Iteración 27349 - Lote 245/352 - Pérdida de Entrenamiento: 0.7049, Precisión de Entrenamiento: 0.7633\n",
      "Iteración 27384 - Lote 280/352 - Pérdida de Entrenamiento: 0.7000, Precisión de Entrenamiento: 0.7648\n",
      "Iteración 27419 - Lote 315/352 - Pérdida de Entrenamiento: 0.7034, Precisión de Entrenamiento: 0.7641\n",
      "Iteración 27454 - Lote 350/352 - Pérdida de Entrenamiento: 0.7016, Precisión de Entrenamiento: 0.7644\n",
      "Val loss: 0.6853, Val acc: 0.7674\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_78.pth\n",
      "Checkpoint del mejor modelo guardado en la época 78\n",
      "0.0125\n",
      "Epoch 79/400\n",
      "Iteración 27491 - Lote 35/352 - Pérdida de Entrenamiento: 0.7405, Precisión de Entrenamiento: 0.7542\n",
      "Iteración 27526 - Lote 70/352 - Pérdida de Entrenamiento: 0.7204, Precisión de Entrenamiento: 0.7592\n",
      "Iteración 27561 - Lote 105/352 - Pérdida de Entrenamiento: 0.7130, Precisión de Entrenamiento: 0.7597\n",
      "Iteración 27596 - Lote 140/352 - Pérdida de Entrenamiento: 0.7066, Precisión de Entrenamiento: 0.7616\n",
      "Iteración 27631 - Lote 175/352 - Pérdida de Entrenamiento: 0.7041, Precisión de Entrenamiento: 0.7620\n",
      "Iteración 27666 - Lote 210/352 - Pérdida de Entrenamiento: 0.7054, Precisión de Entrenamiento: 0.7627\n",
      "Iteración 27701 - Lote 245/352 - Pérdida de Entrenamiento: 0.7043, Precisión de Entrenamiento: 0.7630\n",
      "Iteración 27736 - Lote 280/352 - Pérdida de Entrenamiento: 0.7039, Precisión de Entrenamiento: 0.7628\n",
      "Iteración 27771 - Lote 315/352 - Pérdida de Entrenamiento: 0.7032, Precisión de Entrenamiento: 0.7627\n",
      "Iteración 27806 - Lote 350/352 - Pérdida de Entrenamiento: 0.7043, Precisión de Entrenamiento: 0.7625\n",
      "Val loss: 0.6825, Val acc: 0.7660\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_79.pth\n",
      "Checkpoint del mejor modelo guardado en la época 79\n",
      "0.0125\n",
      "Epoch 80/400\n",
      "Iteración 27843 - Lote 35/352 - Pérdida de Entrenamiento: 0.7014, Precisión de Entrenamiento: 0.7612\n",
      "Iteración 27878 - Lote 70/352 - Pérdida de Entrenamiento: 0.7183, Precisión de Entrenamiento: 0.7583\n",
      "Iteración 27913 - Lote 105/352 - Pérdida de Entrenamiento: 0.7077, Precisión de Entrenamiento: 0.7616\n",
      "Iteración 27948 - Lote 140/352 - Pérdida de Entrenamiento: 0.7094, Precisión de Entrenamiento: 0.7621\n",
      "Iteración 27983 - Lote 175/352 - Pérdida de Entrenamiento: 0.7062, Precisión de Entrenamiento: 0.7641\n",
      "Iteración 28018 - Lote 210/352 - Pérdida de Entrenamiento: 0.7048, Precisión de Entrenamiento: 0.7639\n",
      "Iteración 28053 - Lote 245/352 - Pérdida de Entrenamiento: 0.7065, Precisión de Entrenamiento: 0.7628\n",
      "Iteración 28088 - Lote 280/352 - Pérdida de Entrenamiento: 0.7035, Precisión de Entrenamiento: 0.7636\n",
      "Iteración 28123 - Lote 315/352 - Pérdida de Entrenamiento: 0.7058, Precisión de Entrenamiento: 0.7630\n",
      "Iteración 28158 - Lote 350/352 - Pérdida de Entrenamiento: 0.7071, Precisión de Entrenamiento: 0.7623\n",
      "Val loss: 0.6886, Val acc: 0.7710\n",
      "Gradientes para features.0.0.weight: min=-0.09967111796140671, max=0.05583096668124199, mean=-0.0003809167246799916, std=0.019622761756181717\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.035365380346775055, max=0.035325903445482254, mean=7.403228323710209e-07, std=0.0010237066308036447\n",
      "Gradientes para classifier.1.weight: min=-0.009017759934067726, max=0.010149262845516205, mean=2.3283064712331658e-11, std=0.001558138756081462\n",
      "0.0125\n",
      "Epoch 81/400\n",
      "Iteración 28195 - Lote 35/352 - Pérdida de Entrenamiento: 0.7170, Precisión de Entrenamiento: 0.7583\n",
      "Iteración 28230 - Lote 70/352 - Pérdida de Entrenamiento: 0.7167, Precisión de Entrenamiento: 0.7609\n",
      "Iteración 28265 - Lote 105/352 - Pérdida de Entrenamiento: 0.7111, Precisión de Entrenamiento: 0.7628\n",
      "Iteración 28300 - Lote 140/352 - Pérdida de Entrenamiento: 0.7156, Precisión de Entrenamiento: 0.7616\n",
      "Iteración 28335 - Lote 175/352 - Pérdida de Entrenamiento: 0.7144, Precisión de Entrenamiento: 0.7608\n",
      "Iteración 28370 - Lote 210/352 - Pérdida de Entrenamiento: 0.7145, Precisión de Entrenamiento: 0.7603\n",
      "Iteración 28405 - Lote 245/352 - Pérdida de Entrenamiento: 0.7163, Precisión de Entrenamiento: 0.7591\n",
      "Iteración 28440 - Lote 280/352 - Pérdida de Entrenamiento: 0.7142, Precisión de Entrenamiento: 0.7599\n",
      "Iteración 28475 - Lote 315/352 - Pérdida de Entrenamiento: 0.7119, Precisión de Entrenamiento: 0.7603\n",
      "Iteración 28510 - Lote 350/352 - Pérdida de Entrenamiento: 0.7099, Precisión de Entrenamiento: 0.7606\n",
      "Val loss: 0.6997, Val acc: 0.7644\n",
      "0.0125\n",
      "Epoch 82/400\n",
      "Iteración 28547 - Lote 35/352 - Pérdida de Entrenamiento: 0.7059, Precisión de Entrenamiento: 0.7616\n",
      "Iteración 28582 - Lote 70/352 - Pérdida de Entrenamiento: 0.7084, Precisión de Entrenamiento: 0.7616\n",
      "Iteración 28617 - Lote 105/352 - Pérdida de Entrenamiento: 0.6971, Precisión de Entrenamiento: 0.7646\n",
      "Iteración 28652 - Lote 140/352 - Pérdida de Entrenamiento: 0.6981, Precisión de Entrenamiento: 0.7640\n",
      "Iteración 28687 - Lote 175/352 - Pérdida de Entrenamiento: 0.6975, Precisión de Entrenamiento: 0.7650\n",
      "Iteración 28722 - Lote 210/352 - Pérdida de Entrenamiento: 0.7050, Precisión de Entrenamiento: 0.7631\n",
      "Iteración 28757 - Lote 245/352 - Pérdida de Entrenamiento: 0.7012, Precisión de Entrenamiento: 0.7646\n",
      "Iteración 28792 - Lote 280/352 - Pérdida de Entrenamiento: 0.7065, Precisión de Entrenamiento: 0.7637\n",
      "Iteración 28827 - Lote 315/352 - Pérdida de Entrenamiento: 0.7083, Precisión de Entrenamiento: 0.7623\n",
      "Iteración 28862 - Lote 350/352 - Pérdida de Entrenamiento: 0.7081, Precisión de Entrenamiento: 0.7624\n",
      "Val loss: 0.6810, Val acc: 0.7774\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_82.pth\n",
      "Checkpoint del mejor modelo guardado en la época 82\n",
      "0.0125\n",
      "Epoch 83/400\n",
      "Iteración 28899 - Lote 35/352 - Pérdida de Entrenamiento: 0.7117, Precisión de Entrenamiento: 0.7632\n",
      "Iteración 28934 - Lote 70/352 - Pérdida de Entrenamiento: 0.7027, Precisión de Entrenamiento: 0.7671\n",
      "Iteración 28969 - Lote 105/352 - Pérdida de Entrenamiento: 0.7081, Precisión de Entrenamiento: 0.7650\n",
      "Iteración 29004 - Lote 140/352 - Pérdida de Entrenamiento: 0.7093, Precisión de Entrenamiento: 0.7636\n",
      "Iteración 29039 - Lote 175/352 - Pérdida de Entrenamiento: 0.7067, Precisión de Entrenamiento: 0.7632\n",
      "Iteración 29074 - Lote 210/352 - Pérdida de Entrenamiento: 0.7033, Precisión de Entrenamiento: 0.7627\n",
      "Iteración 29109 - Lote 245/352 - Pérdida de Entrenamiento: 0.7042, Precisión de Entrenamiento: 0.7626\n",
      "Iteración 29144 - Lote 280/352 - Pérdida de Entrenamiento: 0.7034, Precisión de Entrenamiento: 0.7629\n",
      "Iteración 29179 - Lote 315/352 - Pérdida de Entrenamiento: 0.7034, Precisión de Entrenamiento: 0.7636\n",
      "Iteración 29214 - Lote 350/352 - Pérdida de Entrenamiento: 0.7064, Precisión de Entrenamiento: 0.7623\n",
      "Val loss: 0.7075, Val acc: 0.7622\n",
      "0.0125\n",
      "Epoch 84/400\n",
      "Iteración 29251 - Lote 35/352 - Pérdida de Entrenamiento: 0.7245, Precisión de Entrenamiento: 0.7525\n",
      "Iteración 29286 - Lote 70/352 - Pérdida de Entrenamiento: 0.7219, Precisión de Entrenamiento: 0.7551\n",
      "Iteración 29321 - Lote 105/352 - Pérdida de Entrenamiento: 0.7182, Precisión de Entrenamiento: 0.7568\n",
      "Iteración 29356 - Lote 140/352 - Pérdida de Entrenamiento: 0.7146, Precisión de Entrenamiento: 0.7576\n",
      "Iteración 29391 - Lote 175/352 - Pérdida de Entrenamiento: 0.7178, Precisión de Entrenamiento: 0.7569\n",
      "Iteración 29426 - Lote 210/352 - Pérdida de Entrenamiento: 0.7168, Precisión de Entrenamiento: 0.7567\n",
      "Iteración 29461 - Lote 245/352 - Pérdida de Entrenamiento: 0.7193, Precisión de Entrenamiento: 0.7562\n",
      "Iteración 29496 - Lote 280/352 - Pérdida de Entrenamiento: 0.7198, Precisión de Entrenamiento: 0.7552\n",
      "Iteración 29531 - Lote 315/352 - Pérdida de Entrenamiento: 0.7164, Precisión de Entrenamiento: 0.7562\n",
      "Iteración 29566 - Lote 350/352 - Pérdida de Entrenamiento: 0.7166, Precisión de Entrenamiento: 0.7567\n",
      "Val loss: 0.6895, Val acc: 0.7646\n",
      "0.0125\n",
      "Epoch 85/400\n",
      "Iteración 29603 - Lote 35/352 - Pérdida de Entrenamiento: 0.6823, Precisión de Entrenamiento: 0.7730\n",
      "Iteración 29638 - Lote 70/352 - Pérdida de Entrenamiento: 0.6916, Precisión de Entrenamiento: 0.7672\n",
      "Iteración 29673 - Lote 105/352 - Pérdida de Entrenamiento: 0.6832, Precisión de Entrenamiento: 0.7686\n",
      "Iteración 29708 - Lote 140/352 - Pérdida de Entrenamiento: 0.6914, Precisión de Entrenamiento: 0.7660\n",
      "Iteración 29743 - Lote 175/352 - Pérdida de Entrenamiento: 0.6968, Precisión de Entrenamiento: 0.7647\n",
      "Iteración 29778 - Lote 210/352 - Pérdida de Entrenamiento: 0.7028, Precisión de Entrenamiento: 0.7618\n",
      "Iteración 29813 - Lote 245/352 - Pérdida de Entrenamiento: 0.7021, Precisión de Entrenamiento: 0.7618\n",
      "Iteración 29848 - Lote 280/352 - Pérdida de Entrenamiento: 0.7066, Precisión de Entrenamiento: 0.7599\n",
      "Iteración 29883 - Lote 315/352 - Pérdida de Entrenamiento: 0.7079, Precisión de Entrenamiento: 0.7593\n",
      "Iteración 29918 - Lote 350/352 - Pérdida de Entrenamiento: 0.7057, Precisión de Entrenamiento: 0.7601\n",
      "Val loss: 0.7277, Val acc: 0.7558\n",
      "Gradientes para features.0.0.weight: min=-0.11882983148097992, max=0.08349163830280304, mean=-0.0026277333963662386, std=0.02665098011493683\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.06302955746650696, max=0.046722885221242905, mean=-4.481274572754046e-06, std=0.00197038846090436\n",
      "Gradientes para classifier.1.weight: min=-0.028707921504974365, max=0.02001367323100567, mean=0.0, std=0.00312232063151896\n",
      "0.0125\n",
      "Epoch 86/400\n",
      "Iteración 29955 - Lote 35/352 - Pérdida de Entrenamiento: 0.7060, Precisión de Entrenamiento: 0.7667\n",
      "Iteración 29990 - Lote 70/352 - Pérdida de Entrenamiento: 0.6978, Precisión de Entrenamiento: 0.7699\n",
      "Iteración 30025 - Lote 105/352 - Pérdida de Entrenamiento: 0.6958, Precisión de Entrenamiento: 0.7697\n",
      "Iteración 30060 - Lote 140/352 - Pérdida de Entrenamiento: 0.6967, Precisión de Entrenamiento: 0.7669\n",
      "Iteración 30095 - Lote 175/352 - Pérdida de Entrenamiento: 0.6992, Precisión de Entrenamiento: 0.7663\n",
      "Iteración 30130 - Lote 210/352 - Pérdida de Entrenamiento: 0.7024, Precisión de Entrenamiento: 0.7648\n",
      "Iteración 30165 - Lote 245/352 - Pérdida de Entrenamiento: 0.7013, Precisión de Entrenamiento: 0.7649\n",
      "Iteración 30200 - Lote 280/352 - Pérdida de Entrenamiento: 0.7086, Precisión de Entrenamiento: 0.7626\n",
      "Iteración 30235 - Lote 315/352 - Pérdida de Entrenamiento: 0.7070, Precisión de Entrenamiento: 0.7633\n",
      "Iteración 30270 - Lote 350/352 - Pérdida de Entrenamiento: 0.7086, Precisión de Entrenamiento: 0.7630\n",
      "Val loss: 0.7371, Val acc: 0.7464\n",
      "0.0125\n",
      "Epoch 87/400\n",
      "Iteración 30307 - Lote 35/352 - Pérdida de Entrenamiento: 0.6892, Precisión de Entrenamiento: 0.7725\n",
      "Iteración 30342 - Lote 70/352 - Pérdida de Entrenamiento: 0.7047, Precisión de Entrenamiento: 0.7673\n",
      "Iteración 30377 - Lote 105/352 - Pérdida de Entrenamiento: 0.7123, Precisión de Entrenamiento: 0.7635\n",
      "Iteración 30412 - Lote 140/352 - Pérdida de Entrenamiento: 0.7118, Precisión de Entrenamiento: 0.7624\n",
      "Iteración 30447 - Lote 175/352 - Pérdida de Entrenamiento: 0.7175, Precisión de Entrenamiento: 0.7617\n",
      "Iteración 30482 - Lote 210/352 - Pérdida de Entrenamiento: 0.7175, Precisión de Entrenamiento: 0.7611\n",
      "Iteración 30517 - Lote 245/352 - Pérdida de Entrenamiento: 0.7142, Precisión de Entrenamiento: 0.7610\n",
      "Iteración 30552 - Lote 280/352 - Pérdida de Entrenamiento: 0.7137, Precisión de Entrenamiento: 0.7608\n",
      "Iteración 30587 - Lote 315/352 - Pérdida de Entrenamiento: 0.7139, Precisión de Entrenamiento: 0.7597\n",
      "Iteración 30622 - Lote 350/352 - Pérdida de Entrenamiento: 0.7157, Precisión de Entrenamiento: 0.7593\n",
      "Val loss: 0.7576, Val acc: 0.7448\n",
      "0.0125\n",
      "Epoch 88/400\n",
      "Iteración 30659 - Lote 35/352 - Pérdida de Entrenamiento: 0.7193, Precisión de Entrenamiento: 0.7587\n",
      "Iteración 30694 - Lote 70/352 - Pérdida de Entrenamiento: 0.7029, Precisión de Entrenamiento: 0.7626\n",
      "Iteración 30729 - Lote 105/352 - Pérdida de Entrenamiento: 0.7076, Precisión de Entrenamiento: 0.7604\n",
      "Iteración 30764 - Lote 140/352 - Pérdida de Entrenamiento: 0.7067, Precisión de Entrenamiento: 0.7603\n",
      "Iteración 30799 - Lote 175/352 - Pérdida de Entrenamiento: 0.7073, Precisión de Entrenamiento: 0.7599\n",
      "Iteración 30834 - Lote 210/352 - Pérdida de Entrenamiento: 0.7125, Precisión de Entrenamiento: 0.7583\n",
      "Iteración 30869 - Lote 245/352 - Pérdida de Entrenamiento: 0.7164, Precisión de Entrenamiento: 0.7574\n",
      "Iteración 30904 - Lote 280/352 - Pérdida de Entrenamiento: 0.7187, Precisión de Entrenamiento: 0.7564\n",
      "Iteración 30939 - Lote 315/352 - Pérdida de Entrenamiento: 0.7174, Precisión de Entrenamiento: 0.7574\n",
      "Iteración 30974 - Lote 350/352 - Pérdida de Entrenamiento: 0.7166, Precisión de Entrenamiento: 0.7574\n",
      "Val loss: 0.6698, Val acc: 0.7758\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_88.pth\n",
      "Checkpoint del mejor modelo guardado en la época 88\n",
      "0.0125\n",
      "Epoch 89/400\n",
      "Iteración 31011 - Lote 35/352 - Pérdida de Entrenamiento: 0.6988, Precisión de Entrenamiento: 0.7650\n",
      "Iteración 31046 - Lote 70/352 - Pérdida de Entrenamiento: 0.7218, Precisión de Entrenamiento: 0.7555\n",
      "Iteración 31081 - Lote 105/352 - Pérdida de Entrenamiento: 0.7140, Precisión de Entrenamiento: 0.7583\n",
      "Iteración 31116 - Lote 140/352 - Pérdida de Entrenamiento: 0.7151, Precisión de Entrenamiento: 0.7585\n",
      "Iteración 31151 - Lote 175/352 - Pérdida de Entrenamiento: 0.7168, Precisión de Entrenamiento: 0.7579\n",
      "Iteración 31186 - Lote 210/352 - Pérdida de Entrenamiento: 0.7122, Precisión de Entrenamiento: 0.7580\n",
      "Iteración 31221 - Lote 245/352 - Pérdida de Entrenamiento: 0.7089, Precisión de Entrenamiento: 0.7587\n",
      "Iteración 31256 - Lote 280/352 - Pérdida de Entrenamiento: 0.7088, Precisión de Entrenamiento: 0.7593\n",
      "Iteración 31291 - Lote 315/352 - Pérdida de Entrenamiento: 0.7118, Precisión de Entrenamiento: 0.7586\n",
      "Iteración 31326 - Lote 350/352 - Pérdida de Entrenamiento: 0.7120, Precisión de Entrenamiento: 0.7578\n",
      "Val loss: 0.7095, Val acc: 0.7614\n",
      "0.0125\n",
      "Epoch 90/400\n",
      "Iteración 31363 - Lote 35/352 - Pérdida de Entrenamiento: 0.7094, Precisión de Entrenamiento: 0.7647\n",
      "Iteración 31398 - Lote 70/352 - Pérdida de Entrenamiento: 0.7054, Precisión de Entrenamiento: 0.7635\n",
      "Iteración 31433 - Lote 105/352 - Pérdida de Entrenamiento: 0.7033, Precisión de Entrenamiento: 0.7650\n",
      "Iteración 31468 - Lote 140/352 - Pérdida de Entrenamiento: 0.7090, Precisión de Entrenamiento: 0.7628\n",
      "Iteración 31503 - Lote 175/352 - Pérdida de Entrenamiento: 0.7058, Precisión de Entrenamiento: 0.7629\n",
      "Iteración 31538 - Lote 210/352 - Pérdida de Entrenamiento: 0.7016, Precisión de Entrenamiento: 0.7644\n",
      "Iteración 31573 - Lote 245/352 - Pérdida de Entrenamiento: 0.7041, Precisión de Entrenamiento: 0.7624\n",
      "Iteración 31608 - Lote 280/352 - Pérdida de Entrenamiento: 0.7052, Precisión de Entrenamiento: 0.7619\n",
      "Iteración 31643 - Lote 315/352 - Pérdida de Entrenamiento: 0.7063, Precisión de Entrenamiento: 0.7618\n",
      "Iteración 31678 - Lote 350/352 - Pérdida de Entrenamiento: 0.7063, Precisión de Entrenamiento: 0.7620\n",
      "Val loss: 0.6634, Val acc: 0.7716\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_90.pth\n",
      "Checkpoint del mejor modelo guardado en la época 90\n",
      "Gradientes para features.0.0.weight: min=-0.07955855876207352, max=0.10436974465847015, mean=0.003864910686388612, std=0.02577035501599312\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.05999891087412834, max=0.04287440702319145, mean=-6.653879154328024e-06, std=0.0018034729873761535\n",
      "Gradientes para classifier.1.weight: min=-0.013075985945761204, max=0.016060279682278633, mean=2.561137066314778e-11, std=0.002028045477345586\n",
      "0.0125\n",
      "Epoch 91/400\n",
      "Iteración 31715 - Lote 35/352 - Pérdida de Entrenamiento: 0.6979, Precisión de Entrenamiento: 0.7576\n",
      "Iteración 31750 - Lote 70/352 - Pérdida de Entrenamiento: 0.7085, Precisión de Entrenamiento: 0.7567\n",
      "Iteración 31785 - Lote 105/352 - Pérdida de Entrenamiento: 0.7096, Precisión de Entrenamiento: 0.7560\n",
      "Iteración 31820 - Lote 140/352 - Pérdida de Entrenamiento: 0.7175, Precisión de Entrenamiento: 0.7545\n",
      "Iteración 31855 - Lote 175/352 - Pérdida de Entrenamiento: 0.7122, Precisión de Entrenamiento: 0.7563\n",
      "Iteración 31890 - Lote 210/352 - Pérdida de Entrenamiento: 0.7104, Precisión de Entrenamiento: 0.7576\n",
      "Iteración 31925 - Lote 245/352 - Pérdida de Entrenamiento: 0.7108, Precisión de Entrenamiento: 0.7574\n",
      "Iteración 31960 - Lote 280/352 - Pérdida de Entrenamiento: 0.7102, Precisión de Entrenamiento: 0.7576\n",
      "Iteración 31995 - Lote 315/352 - Pérdida de Entrenamiento: 0.7093, Precisión de Entrenamiento: 0.7582\n",
      "Iteración 32030 - Lote 350/352 - Pérdida de Entrenamiento: 0.7109, Precisión de Entrenamiento: 0.7583\n",
      "Val loss: 0.7183, Val acc: 0.7648\n",
      "0.0125\n",
      "Epoch 92/400\n",
      "Iteración 32067 - Lote 35/352 - Pérdida de Entrenamiento: 0.7134, Precisión de Entrenamiento: 0.7600\n",
      "Iteración 32102 - Lote 70/352 - Pérdida de Entrenamiento: 0.7035, Precisión de Entrenamiento: 0.7616\n",
      "Iteración 32137 - Lote 105/352 - Pérdida de Entrenamiento: 0.7129, Precisión de Entrenamiento: 0.7572\n",
      "Iteración 32172 - Lote 140/352 - Pérdida de Entrenamiento: 0.7151, Precisión de Entrenamiento: 0.7570\n",
      "Iteración 32207 - Lote 175/352 - Pérdida de Entrenamiento: 0.7165, Precisión de Entrenamiento: 0.7564\n",
      "Iteración 32242 - Lote 210/352 - Pérdida de Entrenamiento: 0.7109, Precisión de Entrenamiento: 0.7584\n",
      "Iteración 32277 - Lote 245/352 - Pérdida de Entrenamiento: 0.7093, Precisión de Entrenamiento: 0.7594\n",
      "Iteración 32312 - Lote 280/352 - Pérdida de Entrenamiento: 0.7099, Precisión de Entrenamiento: 0.7592\n",
      "Iteración 32347 - Lote 315/352 - Pérdida de Entrenamiento: 0.7159, Precisión de Entrenamiento: 0.7575\n",
      "Iteración 32382 - Lote 350/352 - Pérdida de Entrenamiento: 0.7129, Precisión de Entrenamiento: 0.7588\n",
      "Val loss: 0.6730, Val acc: 0.7712\n",
      "0.0125\n",
      "Epoch 93/400\n",
      "Iteración 32419 - Lote 35/352 - Pérdida de Entrenamiento: 0.7201, Precisión de Entrenamiento: 0.7513\n",
      "Iteración 32454 - Lote 70/352 - Pérdida de Entrenamiento: 0.7037, Precisión de Entrenamiento: 0.7599\n",
      "Iteración 32489 - Lote 105/352 - Pérdida de Entrenamiento: 0.7027, Precisión de Entrenamiento: 0.7598\n",
      "Iteración 32524 - Lote 140/352 - Pérdida de Entrenamiento: 0.7007, Precisión de Entrenamiento: 0.7593\n",
      "Iteración 32559 - Lote 175/352 - Pérdida de Entrenamiento: 0.7039, Precisión de Entrenamiento: 0.7590\n",
      "Iteración 32594 - Lote 210/352 - Pérdida de Entrenamiento: 0.7033, Precisión de Entrenamiento: 0.7597\n",
      "Iteración 32629 - Lote 245/352 - Pérdida de Entrenamiento: 0.7006, Precisión de Entrenamiento: 0.7603\n",
      "Iteración 32664 - Lote 280/352 - Pérdida de Entrenamiento: 0.7049, Precisión de Entrenamiento: 0.7589\n",
      "Iteración 32699 - Lote 315/352 - Pérdida de Entrenamiento: 0.7085, Precisión de Entrenamiento: 0.7575\n",
      "Iteración 32734 - Lote 350/352 - Pérdida de Entrenamiento: 0.7083, Precisión de Entrenamiento: 0.7587\n",
      "Val loss: 0.7167, Val acc: 0.7574\n",
      "0.0125\n",
      "Epoch 94/400\n",
      "Iteración 32771 - Lote 35/352 - Pérdida de Entrenamiento: 0.7083, Precisión de Entrenamiento: 0.7699\n",
      "Iteración 32806 - Lote 70/352 - Pérdida de Entrenamiento: 0.7136, Precisión de Entrenamiento: 0.7632\n",
      "Iteración 32841 - Lote 105/352 - Pérdida de Entrenamiento: 0.7163, Precisión de Entrenamiento: 0.7616\n",
      "Iteración 32876 - Lote 140/352 - Pérdida de Entrenamiento: 0.7142, Precisión de Entrenamiento: 0.7607\n",
      "Iteración 32911 - Lote 175/352 - Pérdida de Entrenamiento: 0.7061, Precisión de Entrenamiento: 0.7637\n",
      "Iteración 32946 - Lote 210/352 - Pérdida de Entrenamiento: 0.7048, Precisión de Entrenamiento: 0.7634\n",
      "Iteración 32981 - Lote 245/352 - Pérdida de Entrenamiento: 0.7069, Precisión de Entrenamiento: 0.7621\n",
      "Iteración 33016 - Lote 280/352 - Pérdida de Entrenamiento: 0.7073, Precisión de Entrenamiento: 0.7623\n",
      "Iteración 33051 - Lote 315/352 - Pérdida de Entrenamiento: 0.7061, Precisión de Entrenamiento: 0.7625\n",
      "Iteración 33086 - Lote 350/352 - Pérdida de Entrenamiento: 0.7068, Precisión de Entrenamiento: 0.7622\n",
      "Val loss: 0.7015, Val acc: 0.7642\n",
      "0.0125\n",
      "Epoch 95/400\n",
      "Iteración 33123 - Lote 35/352 - Pérdida de Entrenamiento: 0.6961, Precisión de Entrenamiento: 0.7627\n",
      "Iteración 33158 - Lote 70/352 - Pérdida de Entrenamiento: 0.6944, Precisión de Entrenamiento: 0.7644\n",
      "Iteración 33193 - Lote 105/352 - Pérdida de Entrenamiento: 0.6896, Precisión de Entrenamiento: 0.7625\n",
      "Iteración 33228 - Lote 140/352 - Pérdida de Entrenamiento: 0.6911, Precisión de Entrenamiento: 0.7625\n",
      "Iteración 33263 - Lote 175/352 - Pérdida de Entrenamiento: 0.6898, Precisión de Entrenamiento: 0.7638\n",
      "Iteración 33298 - Lote 210/352 - Pérdida de Entrenamiento: 0.6914, Precisión de Entrenamiento: 0.7628\n",
      "Iteración 33333 - Lote 245/352 - Pérdida de Entrenamiento: 0.6968, Precisión de Entrenamiento: 0.7629\n",
      "Iteración 33368 - Lote 280/352 - Pérdida de Entrenamiento: 0.7001, Precisión de Entrenamiento: 0.7613\n",
      "Iteración 33403 - Lote 315/352 - Pérdida de Entrenamiento: 0.7033, Precisión de Entrenamiento: 0.7602\n",
      "Iteración 33438 - Lote 350/352 - Pérdida de Entrenamiento: 0.7049, Precisión de Entrenamiento: 0.7599\n",
      "Val loss: 0.6886, Val acc: 0.7734\n",
      "Gradientes para features.0.0.weight: min=-0.09349890053272247, max=0.07777708768844604, mean=-0.0042389617301523685, std=0.02499239332973957\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.054502326995134354, max=0.05286877974867821, mean=1.1471896868897602e-05, std=0.002133273286744952\n",
      "Gradientes para classifier.1.weight: min=-0.012644503265619278, max=0.013441091403365135, mean=0.0, std=0.0020753105636686087\n",
      "0.0125\n",
      "Epoch 96/400\n",
      "Iteración 33475 - Lote 35/352 - Pérdida de Entrenamiento: 0.7167, Precisión de Entrenamiento: 0.7558\n",
      "Iteración 33510 - Lote 70/352 - Pérdida de Entrenamiento: 0.7043, Precisión de Entrenamiento: 0.7629\n",
      "Iteración 33545 - Lote 105/352 - Pérdida de Entrenamiento: 0.6972, Precisión de Entrenamiento: 0.7655\n",
      "Iteración 33580 - Lote 140/352 - Pérdida de Entrenamiento: 0.6984, Precisión de Entrenamiento: 0.7640\n",
      "Iteración 33615 - Lote 175/352 - Pérdida de Entrenamiento: 0.6997, Precisión de Entrenamiento: 0.7618\n",
      "Iteración 33650 - Lote 210/352 - Pérdida de Entrenamiento: 0.7008, Precisión de Entrenamiento: 0.7616\n",
      "Iteración 33685 - Lote 245/352 - Pérdida de Entrenamiento: 0.6993, Precisión de Entrenamiento: 0.7622\n",
      "Iteración 33720 - Lote 280/352 - Pérdida de Entrenamiento: 0.7016, Precisión de Entrenamiento: 0.7624\n",
      "Iteración 33755 - Lote 315/352 - Pérdida de Entrenamiento: 0.7024, Precisión de Entrenamiento: 0.7625\n",
      "Iteración 33790 - Lote 350/352 - Pérdida de Entrenamiento: 0.7041, Precisión de Entrenamiento: 0.7627\n",
      "Val loss: 0.6608, Val acc: 0.7750\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_96.pth\n",
      "Checkpoint del mejor modelo guardado en la época 96\n",
      "0.0125\n",
      "Epoch 97/400\n",
      "Iteración 33827 - Lote 35/352 - Pérdida de Entrenamiento: 0.6664, Precisión de Entrenamiento: 0.7770\n",
      "Iteración 33862 - Lote 70/352 - Pérdida de Entrenamiento: 0.6911, Precisión de Entrenamiento: 0.7679\n",
      "Iteración 33897 - Lote 105/352 - Pérdida de Entrenamiento: 0.6907, Precisión de Entrenamiento: 0.7690\n",
      "Iteración 33932 - Lote 140/352 - Pérdida de Entrenamiento: 0.6933, Precisión de Entrenamiento: 0.7662\n",
      "Iteración 33967 - Lote 175/352 - Pérdida de Entrenamiento: 0.6929, Precisión de Entrenamiento: 0.7653\n",
      "Iteración 34002 - Lote 210/352 - Pérdida de Entrenamiento: 0.6995, Precisión de Entrenamiento: 0.7629\n",
      "Iteración 34037 - Lote 245/352 - Pérdida de Entrenamiento: 0.7036, Precisión de Entrenamiento: 0.7620\n",
      "Iteración 34072 - Lote 280/352 - Pérdida de Entrenamiento: 0.7039, Precisión de Entrenamiento: 0.7620\n",
      "Iteración 34107 - Lote 315/352 - Pérdida de Entrenamiento: 0.7056, Precisión de Entrenamiento: 0.7617\n",
      "Iteración 34142 - Lote 350/352 - Pérdida de Entrenamiento: 0.7046, Precisión de Entrenamiento: 0.7618\n",
      "Val loss: 0.6736, Val acc: 0.7644\n",
      "0.0125\n",
      "Epoch 98/400\n",
      "Iteración 34179 - Lote 35/352 - Pérdida de Entrenamiento: 0.6988, Precisión de Entrenamiento: 0.7643\n",
      "Iteración 34214 - Lote 70/352 - Pérdida de Entrenamiento: 0.7044, Precisión de Entrenamiento: 0.7604\n",
      "Iteración 34249 - Lote 105/352 - Pérdida de Entrenamiento: 0.7040, Precisión de Entrenamiento: 0.7634\n",
      "Iteración 34284 - Lote 140/352 - Pérdida de Entrenamiento: 0.7042, Precisión de Entrenamiento: 0.7636\n",
      "Iteración 34319 - Lote 175/352 - Pérdida de Entrenamiento: 0.6952, Precisión de Entrenamiento: 0.7662\n",
      "Iteración 34354 - Lote 210/352 - Pérdida de Entrenamiento: 0.6984, Precisión de Entrenamiento: 0.7661\n",
      "Iteración 34389 - Lote 245/352 - Pérdida de Entrenamiento: 0.6998, Precisión de Entrenamiento: 0.7663\n",
      "Iteración 34424 - Lote 280/352 - Pérdida de Entrenamiento: 0.7044, Precisión de Entrenamiento: 0.7643\n",
      "Iteración 34459 - Lote 315/352 - Pérdida de Entrenamiento: 0.7060, Precisión de Entrenamiento: 0.7642\n",
      "Iteración 34494 - Lote 350/352 - Pérdida de Entrenamiento: 0.7053, Precisión de Entrenamiento: 0.7645\n",
      "Val loss: 0.7796, Val acc: 0.7484\n",
      "0.0125\n",
      "Epoch 99/400\n",
      "Iteración 34531 - Lote 35/352 - Pérdida de Entrenamiento: 0.6940, Precisión de Entrenamiento: 0.7665\n",
      "Iteración 34566 - Lote 70/352 - Pérdida de Entrenamiento: 0.6885, Precisión de Entrenamiento: 0.7674\n",
      "Iteración 34601 - Lote 105/352 - Pérdida de Entrenamiento: 0.6914, Precisión de Entrenamiento: 0.7650\n",
      "Iteración 34636 - Lote 140/352 - Pérdida de Entrenamiento: 0.6899, Precisión de Entrenamiento: 0.7668\n",
      "Iteración 34671 - Lote 175/352 - Pérdida de Entrenamiento: 0.6958, Precisión de Entrenamiento: 0.7657\n",
      "Iteración 34706 - Lote 210/352 - Pérdida de Entrenamiento: 0.7002, Precisión de Entrenamiento: 0.7654\n",
      "Iteración 34741 - Lote 245/352 - Pérdida de Entrenamiento: 0.7007, Precisión de Entrenamiento: 0.7657\n",
      "Iteración 34776 - Lote 280/352 - Pérdida de Entrenamiento: 0.7058, Precisión de Entrenamiento: 0.7646\n",
      "Iteración 34811 - Lote 315/352 - Pérdida de Entrenamiento: 0.7030, Precisión de Entrenamiento: 0.7648\n",
      "Iteración 34846 - Lote 350/352 - Pérdida de Entrenamiento: 0.7037, Precisión de Entrenamiento: 0.7652\n",
      "Val loss: 0.6703, Val acc: 0.7712\n",
      "0.0125\n",
      "Epoch 100/400\n",
      "Iteración 34883 - Lote 35/352 - Pérdida de Entrenamiento: 0.6853, Precisión de Entrenamiento: 0.7676\n",
      "Iteración 34918 - Lote 70/352 - Pérdida de Entrenamiento: 0.6889, Precisión de Entrenamiento: 0.7647\n",
      "Iteración 34953 - Lote 105/352 - Pérdida de Entrenamiento: 0.6976, Precisión de Entrenamiento: 0.7614\n",
      "Iteración 34988 - Lote 140/352 - Pérdida de Entrenamiento: 0.7029, Precisión de Entrenamiento: 0.7619\n",
      "Iteración 35023 - Lote 175/352 - Pérdida de Entrenamiento: 0.7044, Precisión de Entrenamiento: 0.7613\n",
      "Iteración 35058 - Lote 210/352 - Pérdida de Entrenamiento: 0.7063, Precisión de Entrenamiento: 0.7616\n",
      "Iteración 35093 - Lote 245/352 - Pérdida de Entrenamiento: 0.7088, Precisión de Entrenamiento: 0.7607\n",
      "Iteración 35128 - Lote 280/352 - Pérdida de Entrenamiento: 0.7069, Precisión de Entrenamiento: 0.7617\n",
      "Iteración 35163 - Lote 315/352 - Pérdida de Entrenamiento: 0.7058, Precisión de Entrenamiento: 0.7619\n",
      "Iteración 35198 - Lote 350/352 - Pérdida de Entrenamiento: 0.7062, Precisión de Entrenamiento: 0.7623\n",
      "Val loss: 0.6606, Val acc: 0.7754\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_100.pth\n",
      "Checkpoint del mejor modelo guardado en la época 100\n",
      "Gradientes para features.0.0.weight: min=-0.12016739696264267, max=0.1557508409023285, mean=-0.000773605948779732, std=0.03631678968667984\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.053188491612672806, max=0.0750841349363327, mean=-1.128094027080806e-05, std=0.0023947376757860184\n",
      "Gradientes para classifier.1.weight: min=-0.01382275391370058, max=0.015486410818994045, mean=1.8626451075975936e-11, std=0.0021647310350090265\n",
      "0.0125\n",
      "Epoch 101/400\n",
      "Iteración 35235 - Lote 35/352 - Pérdida de Entrenamiento: 0.6919, Precisión de Entrenamiento: 0.7656\n",
      "Iteración 35270 - Lote 70/352 - Pérdida de Entrenamiento: 0.6779, Precisión de Entrenamiento: 0.7703\n",
      "Iteración 35305 - Lote 105/352 - Pérdida de Entrenamiento: 0.6860, Precisión de Entrenamiento: 0.7670\n",
      "Iteración 35340 - Lote 140/352 - Pérdida de Entrenamiento: 0.6898, Precisión de Entrenamiento: 0.7664\n",
      "Iteración 35375 - Lote 175/352 - Pérdida de Entrenamiento: 0.6961, Precisión de Entrenamiento: 0.7660\n",
      "Iteración 35410 - Lote 210/352 - Pérdida de Entrenamiento: 0.6999, Precisión de Entrenamiento: 0.7645\n",
      "Iteración 35445 - Lote 245/352 - Pérdida de Entrenamiento: 0.6972, Precisión de Entrenamiento: 0.7666\n",
      "Iteración 35480 - Lote 280/352 - Pérdida de Entrenamiento: 0.6981, Precisión de Entrenamiento: 0.7660\n",
      "Iteración 35515 - Lote 315/352 - Pérdida de Entrenamiento: 0.6947, Precisión de Entrenamiento: 0.7666\n",
      "Iteración 35550 - Lote 350/352 - Pérdida de Entrenamiento: 0.6965, Precisión de Entrenamiento: 0.7661\n",
      "Val loss: 0.6708, Val acc: 0.7726\n",
      "0.0125\n",
      "Epoch 102/400\n",
      "Iteración 35587 - Lote 35/352 - Pérdida de Entrenamiento: 0.6990, Precisión de Entrenamiento: 0.7583\n",
      "Iteración 35622 - Lote 70/352 - Pérdida de Entrenamiento: 0.7016, Precisión de Entrenamiento: 0.7592\n",
      "Iteración 35657 - Lote 105/352 - Pérdida de Entrenamiento: 0.6926, Precisión de Entrenamiento: 0.7652\n",
      "Iteración 35692 - Lote 140/352 - Pérdida de Entrenamiento: 0.6974, Precisión de Entrenamiento: 0.7640\n",
      "Iteración 35727 - Lote 175/352 - Pérdida de Entrenamiento: 0.6972, Precisión de Entrenamiento: 0.7645\n",
      "Iteración 35762 - Lote 210/352 - Pérdida de Entrenamiento: 0.7003, Precisión de Entrenamiento: 0.7642\n",
      "Iteración 35797 - Lote 245/352 - Pérdida de Entrenamiento: 0.7030, Precisión de Entrenamiento: 0.7633\n",
      "Iteración 35832 - Lote 280/352 - Pérdida de Entrenamiento: 0.7014, Precisión de Entrenamiento: 0.7630\n",
      "Iteración 35867 - Lote 315/352 - Pérdida de Entrenamiento: 0.6998, Precisión de Entrenamiento: 0.7644\n",
      "Iteración 35902 - Lote 350/352 - Pérdida de Entrenamiento: 0.7020, Precisión de Entrenamiento: 0.7638\n",
      "Val loss: 0.6736, Val acc: 0.7728\n",
      "0.0125\n",
      "Epoch 103/400\n",
      "Iteración 35939 - Lote 35/352 - Pérdida de Entrenamiento: 0.6720, Precisión de Entrenamiento: 0.7683\n",
      "Iteración 35974 - Lote 70/352 - Pérdida de Entrenamiento: 0.6995, Precisión de Entrenamiento: 0.7588\n",
      "Iteración 36009 - Lote 105/352 - Pérdida de Entrenamiento: 0.6895, Precisión de Entrenamiento: 0.7626\n",
      "Iteración 36044 - Lote 140/352 - Pérdida de Entrenamiento: 0.6845, Precisión de Entrenamiento: 0.7660\n",
      "Iteración 36079 - Lote 175/352 - Pérdida de Entrenamiento: 0.6860, Precisión de Entrenamiento: 0.7653\n",
      "Iteración 36114 - Lote 210/352 - Pérdida de Entrenamiento: 0.6889, Precisión de Entrenamiento: 0.7647\n",
      "Iteración 36149 - Lote 245/352 - Pérdida de Entrenamiento: 0.6913, Precisión de Entrenamiento: 0.7639\n",
      "Iteración 36184 - Lote 280/352 - Pérdida de Entrenamiento: 0.6966, Precisión de Entrenamiento: 0.7624\n",
      "Iteración 36219 - Lote 315/352 - Pérdida de Entrenamiento: 0.6997, Precisión de Entrenamiento: 0.7618\n",
      "Iteración 36254 - Lote 350/352 - Pérdida de Entrenamiento: 0.7019, Precisión de Entrenamiento: 0.7617\n",
      "Val loss: 0.7035, Val acc: 0.7596\n",
      "0.0125\n",
      "Epoch 104/400\n",
      "Iteración 36291 - Lote 35/352 - Pérdida de Entrenamiento: 0.7013, Precisión de Entrenamiento: 0.7714\n",
      "Iteración 36326 - Lote 70/352 - Pérdida de Entrenamiento: 0.7102, Precisión de Entrenamiento: 0.7694\n",
      "Iteración 36361 - Lote 105/352 - Pérdida de Entrenamiento: 0.6994, Precisión de Entrenamiento: 0.7693\n",
      "Iteración 36396 - Lote 140/352 - Pérdida de Entrenamiento: 0.6992, Precisión de Entrenamiento: 0.7687\n",
      "Iteración 36431 - Lote 175/352 - Pérdida de Entrenamiento: 0.6955, Precisión de Entrenamiento: 0.7685\n",
      "Iteración 36466 - Lote 210/352 - Pérdida de Entrenamiento: 0.6981, Precisión de Entrenamiento: 0.7666\n",
      "Iteración 36501 - Lote 245/352 - Pérdida de Entrenamiento: 0.6946, Precisión de Entrenamiento: 0.7669\n",
      "Iteración 36536 - Lote 280/352 - Pérdida de Entrenamiento: 0.6908, Precisión de Entrenamiento: 0.7686\n",
      "Iteración 36571 - Lote 315/352 - Pérdida de Entrenamiento: 0.6919, Precisión de Entrenamiento: 0.7682\n",
      "Iteración 36606 - Lote 350/352 - Pérdida de Entrenamiento: 0.6946, Precisión de Entrenamiento: 0.7676\n",
      "Val loss: 0.6527, Val acc: 0.7844\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_104.pth\n",
      "Checkpoint del mejor modelo guardado en la época 104\n",
      "0.0125\n",
      "Epoch 105/400\n",
      "Iteración 36643 - Lote 35/352 - Pérdida de Entrenamiento: 0.6928, Precisión de Entrenamiento: 0.7690\n",
      "Iteración 36678 - Lote 70/352 - Pérdida de Entrenamiento: 0.6783, Precisión de Entrenamiento: 0.7741\n",
      "Iteración 36713 - Lote 105/352 - Pérdida de Entrenamiento: 0.6841, Precisión de Entrenamiento: 0.7692\n",
      "Iteración 36748 - Lote 140/352 - Pérdida de Entrenamiento: 0.6840, Precisión de Entrenamiento: 0.7716\n",
      "Iteración 36783 - Lote 175/352 - Pérdida de Entrenamiento: 0.6868, Precisión de Entrenamiento: 0.7720\n",
      "Iteración 36818 - Lote 210/352 - Pérdida de Entrenamiento: 0.6899, Precisión de Entrenamiento: 0.7704\n",
      "Iteración 36853 - Lote 245/352 - Pérdida de Entrenamiento: 0.6907, Precisión de Entrenamiento: 0.7700\n",
      "Iteración 36888 - Lote 280/352 - Pérdida de Entrenamiento: 0.6944, Precisión de Entrenamiento: 0.7685\n",
      "Iteración 36923 - Lote 315/352 - Pérdida de Entrenamiento: 0.6964, Precisión de Entrenamiento: 0.7680\n",
      "Iteración 36958 - Lote 350/352 - Pérdida de Entrenamiento: 0.6942, Precisión de Entrenamiento: 0.7686\n",
      "Val loss: 0.6611, Val acc: 0.7800\n",
      "Gradientes para features.0.0.weight: min=-0.078354611992836, max=0.05558186024427414, mean=-7.376322901109233e-05, std=0.017895782366394997\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.053458038717508316, max=0.049752190709114075, mean=9.565111440679175e-07, std=0.0021657010074704885\n",
      "Gradientes para classifier.1.weight: min=-0.0173968318849802, max=0.02348143793642521, mean=-1.8626451075975936e-11, std=0.0025380400475114584\n",
      "0.0125\n",
      "Epoch 106/400\n",
      "Iteración 36995 - Lote 35/352 - Pérdida de Entrenamiento: 0.6722, Precisión de Entrenamiento: 0.7721\n",
      "Iteración 37030 - Lote 70/352 - Pérdida de Entrenamiento: 0.6891, Precisión de Entrenamiento: 0.7657\n",
      "Iteración 37065 - Lote 105/352 - Pérdida de Entrenamiento: 0.6881, Precisión de Entrenamiento: 0.7667\n",
      "Iteración 37100 - Lote 140/352 - Pérdida de Entrenamiento: 0.6892, Precisión de Entrenamiento: 0.7648\n",
      "Iteración 37135 - Lote 175/352 - Pérdida de Entrenamiento: 0.6883, Precisión de Entrenamiento: 0.7668\n",
      "Iteración 37170 - Lote 210/352 - Pérdida de Entrenamiento: 0.6866, Precisión de Entrenamiento: 0.7663\n",
      "Iteración 37205 - Lote 245/352 - Pérdida de Entrenamiento: 0.6900, Precisión de Entrenamiento: 0.7662\n",
      "Iteración 37240 - Lote 280/352 - Pérdida de Entrenamiento: 0.6924, Precisión de Entrenamiento: 0.7654\n",
      "Iteración 37275 - Lote 315/352 - Pérdida de Entrenamiento: 0.6938, Precisión de Entrenamiento: 0.7658\n",
      "Iteración 37310 - Lote 350/352 - Pérdida de Entrenamiento: 0.6963, Precisión de Entrenamiento: 0.7652\n",
      "Val loss: 0.6824, Val acc: 0.7656\n",
      "0.0125\n",
      "Epoch 107/400\n",
      "Iteración 37347 - Lote 35/352 - Pérdida de Entrenamiento: 0.6819, Precisión de Entrenamiento: 0.7790\n",
      "Iteración 37382 - Lote 70/352 - Pérdida de Entrenamiento: 0.6781, Precisión de Entrenamiento: 0.7765\n",
      "Iteración 37417 - Lote 105/352 - Pérdida de Entrenamiento: 0.6817, Precisión de Entrenamiento: 0.7711\n",
      "Iteración 37452 - Lote 140/352 - Pérdida de Entrenamiento: 0.6825, Precisión de Entrenamiento: 0.7705\n",
      "Iteración 37487 - Lote 175/352 - Pérdida de Entrenamiento: 0.6871, Precisión de Entrenamiento: 0.7686\n",
      "Iteración 37522 - Lote 210/352 - Pérdida de Entrenamiento: 0.6967, Precisión de Entrenamiento: 0.7643\n",
      "Iteración 37557 - Lote 245/352 - Pérdida de Entrenamiento: 0.6965, Precisión de Entrenamiento: 0.7643\n",
      "Iteración 37592 - Lote 280/352 - Pérdida de Entrenamiento: 0.6994, Precisión de Entrenamiento: 0.7634\n",
      "Iteración 37627 - Lote 315/352 - Pérdida de Entrenamiento: 0.6986, Precisión de Entrenamiento: 0.7641\n",
      "Iteración 37662 - Lote 350/352 - Pérdida de Entrenamiento: 0.6973, Precisión de Entrenamiento: 0.7647\n",
      "Val loss: 0.6769, Val acc: 0.7708\n",
      "0.0125\n",
      "Epoch 108/400\n",
      "Iteración 37699 - Lote 35/352 - Pérdida de Entrenamiento: 0.6800, Precisión de Entrenamiento: 0.7721\n",
      "Iteración 37734 - Lote 70/352 - Pérdida de Entrenamiento: 0.6721, Precisión de Entrenamiento: 0.7766\n",
      "Iteración 37769 - Lote 105/352 - Pérdida de Entrenamiento: 0.6775, Precisión de Entrenamiento: 0.7729\n",
      "Iteración 37804 - Lote 140/352 - Pérdida de Entrenamiento: 0.6778, Precisión de Entrenamiento: 0.7718\n",
      "Iteración 37839 - Lote 175/352 - Pérdida de Entrenamiento: 0.6793, Precisión de Entrenamiento: 0.7721\n",
      "Iteración 37874 - Lote 210/352 - Pérdida de Entrenamiento: 0.6911, Precisión de Entrenamiento: 0.7683\n",
      "Iteración 37909 - Lote 245/352 - Pérdida de Entrenamiento: 0.6935, Precisión de Entrenamiento: 0.7676\n",
      "Iteración 37944 - Lote 280/352 - Pérdida de Entrenamiento: 0.6948, Precisión de Entrenamiento: 0.7669\n",
      "Iteración 37979 - Lote 315/352 - Pérdida de Entrenamiento: 0.6965, Precisión de Entrenamiento: 0.7658\n",
      "Iteración 38014 - Lote 350/352 - Pérdida de Entrenamiento: 0.6959, Precisión de Entrenamiento: 0.7660\n",
      "Val loss: 0.7002, Val acc: 0.7632\n",
      "0.0125\n",
      "Epoch 109/400\n",
      "Iteración 38051 - Lote 35/352 - Pérdida de Entrenamiento: 0.6754, Precisión de Entrenamiento: 0.7683\n",
      "Iteración 38086 - Lote 70/352 - Pérdida de Entrenamiento: 0.6870, Precisión de Entrenamiento: 0.7673\n",
      "Iteración 38121 - Lote 105/352 - Pérdida de Entrenamiento: 0.6870, Precisión de Entrenamiento: 0.7667\n",
      "Iteración 38156 - Lote 140/352 - Pérdida de Entrenamiento: 0.6913, Precisión de Entrenamiento: 0.7649\n",
      "Iteración 38191 - Lote 175/352 - Pérdida de Entrenamiento: 0.6879, Precisión de Entrenamiento: 0.7672\n",
      "Iteración 38226 - Lote 210/352 - Pérdida de Entrenamiento: 0.6883, Precisión de Entrenamiento: 0.7660\n",
      "Iteración 38261 - Lote 245/352 - Pérdida de Entrenamiento: 0.6933, Precisión de Entrenamiento: 0.7647\n",
      "Iteración 38296 - Lote 280/352 - Pérdida de Entrenamiento: 0.6957, Precisión de Entrenamiento: 0.7646\n",
      "Iteración 38331 - Lote 315/352 - Pérdida de Entrenamiento: 0.6914, Precisión de Entrenamiento: 0.7664\n",
      "Iteración 38366 - Lote 350/352 - Pérdida de Entrenamiento: 0.6948, Precisión de Entrenamiento: 0.7650\n",
      "Val loss: 0.7158, Val acc: 0.7596\n",
      "0.0125\n",
      "Epoch 110/400\n",
      "Iteración 38403 - Lote 35/352 - Pérdida de Entrenamiento: 0.6723, Precisión de Entrenamiento: 0.7688\n",
      "Iteración 38438 - Lote 70/352 - Pérdida de Entrenamiento: 0.6712, Precisión de Entrenamiento: 0.7676\n",
      "Iteración 38473 - Lote 105/352 - Pérdida de Entrenamiento: 0.6798, Precisión de Entrenamiento: 0.7658\n",
      "Iteración 38508 - Lote 140/352 - Pérdida de Entrenamiento: 0.6805, Precisión de Entrenamiento: 0.7669\n",
      "Iteración 38543 - Lote 175/352 - Pérdida de Entrenamiento: 0.6856, Precisión de Entrenamiento: 0.7656\n",
      "Iteración 38578 - Lote 210/352 - Pérdida de Entrenamiento: 0.6842, Precisión de Entrenamiento: 0.7676\n",
      "Iteración 38613 - Lote 245/352 - Pérdida de Entrenamiento: 0.6841, Precisión de Entrenamiento: 0.7678\n",
      "Iteración 38648 - Lote 280/352 - Pérdida de Entrenamiento: 0.6880, Precisión de Entrenamiento: 0.7673\n",
      "Iteración 38683 - Lote 315/352 - Pérdida de Entrenamiento: 0.6885, Precisión de Entrenamiento: 0.7670\n",
      "Iteración 38718 - Lote 350/352 - Pérdida de Entrenamiento: 0.6892, Precisión de Entrenamiento: 0.7671\n",
      "Val loss: 0.7278, Val acc: 0.7512\n",
      "Gradientes para features.0.0.weight: min=-0.08661124855279922, max=0.07385925203561783, mean=0.0013037404278293252, std=0.021836135536432266\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.046457450836896896, max=0.05947446450591087, mean=8.182031479009311e-07, std=0.0018957105930894613\n",
      "Gradientes para classifier.1.weight: min=-0.015985840931534767, max=0.016296764835715294, mean=-3.14321381422733e-11, std=0.0015525472117587924\n",
      "0.0125\n",
      "Epoch 111/400\n",
      "Iteración 38755 - Lote 35/352 - Pérdida de Entrenamiento: 0.6831, Precisión de Entrenamiento: 0.7705\n",
      "Iteración 38790 - Lote 70/352 - Pérdida de Entrenamiento: 0.6887, Precisión de Entrenamiento: 0.7723\n",
      "Iteración 38825 - Lote 105/352 - Pérdida de Entrenamiento: 0.6950, Precisión de Entrenamiento: 0.7677\n",
      "Iteración 38860 - Lote 140/352 - Pérdida de Entrenamiento: 0.6918, Precisión de Entrenamiento: 0.7681\n",
      "Iteración 38895 - Lote 175/352 - Pérdida de Entrenamiento: 0.6945, Precisión de Entrenamiento: 0.7662\n",
      "Iteración 38930 - Lote 210/352 - Pérdida de Entrenamiento: 0.6956, Precisión de Entrenamiento: 0.7652\n",
      "Iteración 38965 - Lote 245/352 - Pérdida de Entrenamiento: 0.6944, Precisión de Entrenamiento: 0.7652\n",
      "Iteración 39000 - Lote 280/352 - Pérdida de Entrenamiento: 0.6926, Precisión de Entrenamiento: 0.7659\n",
      "Iteración 39035 - Lote 315/352 - Pérdida de Entrenamiento: 0.6939, Precisión de Entrenamiento: 0.7660\n",
      "Iteración 39070 - Lote 350/352 - Pérdida de Entrenamiento: 0.6941, Precisión de Entrenamiento: 0.7664\n",
      "Val loss: 0.6712, Val acc: 0.7736\n",
      "0.0125\n",
      "Epoch 112/400\n",
      "Iteración 39107 - Lote 35/352 - Pérdida de Entrenamiento: 0.6521, Precisión de Entrenamiento: 0.7795\n",
      "Iteración 39142 - Lote 70/352 - Pérdida de Entrenamiento: 0.6740, Precisión de Entrenamiento: 0.7713\n",
      "Iteración 39177 - Lote 105/352 - Pérdida de Entrenamiento: 0.6733, Precisión de Entrenamiento: 0.7705\n",
      "Iteración 39212 - Lote 140/352 - Pérdida de Entrenamiento: 0.6832, Precisión de Entrenamiento: 0.7688\n",
      "Iteración 39247 - Lote 175/352 - Pérdida de Entrenamiento: 0.6847, Precisión de Entrenamiento: 0.7688\n",
      "Iteración 39282 - Lote 210/352 - Pérdida de Entrenamiento: 0.6852, Precisión de Entrenamiento: 0.7686\n",
      "Iteración 39317 - Lote 245/352 - Pérdida de Entrenamiento: 0.6861, Precisión de Entrenamiento: 0.7678\n",
      "Iteración 39352 - Lote 280/352 - Pérdida de Entrenamiento: 0.6859, Precisión de Entrenamiento: 0.7681\n",
      "Iteración 39387 - Lote 315/352 - Pérdida de Entrenamiento: 0.6876, Precisión de Entrenamiento: 0.7676\n",
      "Iteración 39422 - Lote 350/352 - Pérdida de Entrenamiento: 0.6902, Precisión de Entrenamiento: 0.7667\n",
      "Val loss: 0.6562, Val acc: 0.7774\n",
      "0.0125\n",
      "Epoch 113/400\n",
      "Iteración 39459 - Lote 35/352 - Pérdida de Entrenamiento: 0.6732, Precisión de Entrenamiento: 0.7797\n",
      "Iteración 39494 - Lote 70/352 - Pérdida de Entrenamiento: 0.6577, Precisión de Entrenamiento: 0.7820\n",
      "Iteración 39529 - Lote 105/352 - Pérdida de Entrenamiento: 0.6722, Precisión de Entrenamiento: 0.7750\n",
      "Iteración 39564 - Lote 140/352 - Pérdida de Entrenamiento: 0.6797, Precisión de Entrenamiento: 0.7725\n",
      "Iteración 39599 - Lote 175/352 - Pérdida de Entrenamiento: 0.6833, Precisión de Entrenamiento: 0.7715\n",
      "Iteración 39634 - Lote 210/352 - Pérdida de Entrenamiento: 0.6889, Precisión de Entrenamiento: 0.7689\n",
      "Iteración 39669 - Lote 245/352 - Pérdida de Entrenamiento: 0.6890, Precisión de Entrenamiento: 0.7680\n",
      "Iteración 39704 - Lote 280/352 - Pérdida de Entrenamiento: 0.6911, Precisión de Entrenamiento: 0.7666\n",
      "Iteración 39739 - Lote 315/352 - Pérdida de Entrenamiento: 0.6909, Precisión de Entrenamiento: 0.7663\n",
      "Iteración 39774 - Lote 350/352 - Pérdida de Entrenamiento: 0.6899, Precisión de Entrenamiento: 0.7663\n",
      "Val loss: 0.6457, Val acc: 0.7816\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_113.pth\n",
      "Checkpoint del mejor modelo guardado en la época 113\n",
      "0.0125\n",
      "Epoch 114/400\n",
      "Iteración 39811 - Lote 35/352 - Pérdida de Entrenamiento: 0.6992, Precisión de Entrenamiento: 0.7605\n",
      "Iteración 39846 - Lote 70/352 - Pérdida de Entrenamiento: 0.6966, Precisión de Entrenamiento: 0.7629\n",
      "Iteración 39881 - Lote 105/352 - Pérdida de Entrenamiento: 0.6990, Precisión de Entrenamiento: 0.7634\n",
      "Iteración 39916 - Lote 140/352 - Pérdida de Entrenamiento: 0.6943, Precisión de Entrenamiento: 0.7660\n",
      "Iteración 39951 - Lote 175/352 - Pérdida de Entrenamiento: 0.6939, Precisión de Entrenamiento: 0.7659\n",
      "Iteración 39986 - Lote 210/352 - Pérdida de Entrenamiento: 0.6959, Precisión de Entrenamiento: 0.7654\n",
      "Iteración 40021 - Lote 245/352 - Pérdida de Entrenamiento: 0.6898, Precisión de Entrenamiento: 0.7675\n",
      "Iteración 40056 - Lote 280/352 - Pérdida de Entrenamiento: 0.6920, Precisión de Entrenamiento: 0.7671\n",
      "Iteración 40091 - Lote 315/352 - Pérdida de Entrenamiento: 0.6945, Precisión de Entrenamiento: 0.7659\n",
      "Iteración 40126 - Lote 350/352 - Pérdida de Entrenamiento: 0.6966, Precisión de Entrenamiento: 0.7644\n",
      "Val loss: 0.6934, Val acc: 0.7652\n",
      "0.0125\n",
      "Epoch 115/400\n",
      "Iteración 40163 - Lote 35/352 - Pérdida de Entrenamiento: 0.6719, Precisión de Entrenamiento: 0.7672\n",
      "Iteración 40198 - Lote 70/352 - Pérdida de Entrenamiento: 0.6627, Precisión de Entrenamiento: 0.7709\n",
      "Iteración 40233 - Lote 105/352 - Pérdida de Entrenamiento: 0.6638, Precisión de Entrenamiento: 0.7728\n",
      "Iteración 40268 - Lote 140/352 - Pérdida de Entrenamiento: 0.6627, Precisión de Entrenamiento: 0.7732\n",
      "Iteración 40303 - Lote 175/352 - Pérdida de Entrenamiento: 0.6771, Precisión de Entrenamiento: 0.7685\n",
      "Iteración 40338 - Lote 210/352 - Pérdida de Entrenamiento: 0.6825, Precisión de Entrenamiento: 0.7662\n",
      "Iteración 40373 - Lote 245/352 - Pérdida de Entrenamiento: 0.6826, Precisión de Entrenamiento: 0.7662\n",
      "Iteración 40408 - Lote 280/352 - Pérdida de Entrenamiento: 0.6863, Precisión de Entrenamiento: 0.7653\n",
      "Iteración 40443 - Lote 315/352 - Pérdida de Entrenamiento: 0.6900, Precisión de Entrenamiento: 0.7654\n",
      "Iteración 40478 - Lote 350/352 - Pérdida de Entrenamiento: 0.6872, Precisión de Entrenamiento: 0.7669\n",
      "Val loss: 0.6736, Val acc: 0.7802\n",
      "Gradientes para features.0.0.weight: min=-0.08192992210388184, max=0.10910797864198685, mean=0.001318492810241878, std=0.026485517621040344\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.061787158250808716, max=0.048205967992544174, mean=-1.9035030618397286e-06, std=0.0019466090016067028\n",
      "Gradientes para classifier.1.weight: min=-0.015477873384952545, max=0.013811913318932056, mean=-4.656612768993984e-12, std=0.0021203889045864344\n",
      "0.0125\n",
      "Epoch 116/400\n",
      "Iteración 40515 - Lote 35/352 - Pérdida de Entrenamiento: 0.7070, Precisión de Entrenamiento: 0.7643\n",
      "Iteración 40550 - Lote 70/352 - Pérdida de Entrenamiento: 0.6904, Precisión de Entrenamiento: 0.7706\n",
      "Iteración 40585 - Lote 105/352 - Pérdida de Entrenamiento: 0.6989, Precisión de Entrenamiento: 0.7676\n",
      "Iteración 40620 - Lote 140/352 - Pérdida de Entrenamiento: 0.6926, Precisión de Entrenamiento: 0.7688\n",
      "Iteración 40655 - Lote 175/352 - Pérdida de Entrenamiento: 0.6895, Precisión de Entrenamiento: 0.7686\n",
      "Iteración 40690 - Lote 210/352 - Pérdida de Entrenamiento: 0.6914, Precisión de Entrenamiento: 0.7677\n",
      "Iteración 40725 - Lote 245/352 - Pérdida de Entrenamiento: 0.6909, Precisión de Entrenamiento: 0.7667\n",
      "Iteración 40760 - Lote 280/352 - Pérdida de Entrenamiento: 0.6928, Precisión de Entrenamiento: 0.7663\n",
      "Iteración 40795 - Lote 315/352 - Pérdida de Entrenamiento: 0.6913, Precisión de Entrenamiento: 0.7667\n",
      "Iteración 40830 - Lote 350/352 - Pérdida de Entrenamiento: 0.6896, Precisión de Entrenamiento: 0.7678\n",
      "Val loss: 0.6776, Val acc: 0.7762\n",
      "0.0125\n",
      "Epoch 117/400\n",
      "Iteración 40867 - Lote 35/352 - Pérdida de Entrenamiento: 0.6932, Precisión de Entrenamiento: 0.7667\n",
      "Iteración 40902 - Lote 70/352 - Pérdida de Entrenamiento: 0.6830, Precisión de Entrenamiento: 0.7700\n",
      "Iteración 40937 - Lote 105/352 - Pérdida de Entrenamiento: 0.6877, Precisión de Entrenamiento: 0.7696\n",
      "Iteración 40972 - Lote 140/352 - Pérdida de Entrenamiento: 0.6890, Precisión de Entrenamiento: 0.7690\n",
      "Iteración 41007 - Lote 175/352 - Pérdida de Entrenamiento: 0.6917, Precisión de Entrenamiento: 0.7679\n",
      "Iteración 41042 - Lote 210/352 - Pérdida de Entrenamiento: 0.6903, Precisión de Entrenamiento: 0.7687\n",
      "Iteración 41077 - Lote 245/352 - Pérdida de Entrenamiento: 0.6901, Precisión de Entrenamiento: 0.7679\n",
      "Iteración 41112 - Lote 280/352 - Pérdida de Entrenamiento: 0.6895, Precisión de Entrenamiento: 0.7683\n",
      "Iteración 41147 - Lote 315/352 - Pérdida de Entrenamiento: 0.6881, Precisión de Entrenamiento: 0.7683\n",
      "Iteración 41182 - Lote 350/352 - Pérdida de Entrenamiento: 0.6873, Precisión de Entrenamiento: 0.7685\n",
      "Val loss: 0.7010, Val acc: 0.7606\n",
      "0.0125\n",
      "Epoch 118/400\n",
      "Iteración 41219 - Lote 35/352 - Pérdida de Entrenamiento: 0.6782, Precisión de Entrenamiento: 0.7719\n",
      "Iteración 41254 - Lote 70/352 - Pérdida de Entrenamiento: 0.6652, Precisión de Entrenamiento: 0.7771\n",
      "Iteración 41289 - Lote 105/352 - Pérdida de Entrenamiento: 0.6730, Precisión de Entrenamiento: 0.7748\n",
      "Iteración 41324 - Lote 140/352 - Pérdida de Entrenamiento: 0.6746, Precisión de Entrenamiento: 0.7719\n",
      "Iteración 41359 - Lote 175/352 - Pérdida de Entrenamiento: 0.6758, Precisión de Entrenamiento: 0.7715\n",
      "Iteración 41394 - Lote 210/352 - Pérdida de Entrenamiento: 0.6768, Precisión de Entrenamiento: 0.7711\n",
      "Iteración 41429 - Lote 245/352 - Pérdida de Entrenamiento: 0.6813, Precisión de Entrenamiento: 0.7705\n",
      "Iteración 41464 - Lote 280/352 - Pérdida de Entrenamiento: 0.6847, Precisión de Entrenamiento: 0.7696\n",
      "Iteración 41499 - Lote 315/352 - Pérdida de Entrenamiento: 0.6852, Precisión de Entrenamiento: 0.7694\n",
      "Iteración 41534 - Lote 350/352 - Pérdida de Entrenamiento: 0.6865, Precisión de Entrenamiento: 0.7688\n",
      "Val loss: 0.6990, Val acc: 0.7624\n",
      "0.0125\n",
      "Epoch 119/400\n",
      "Iteración 41571 - Lote 35/352 - Pérdida de Entrenamiento: 0.6737, Precisión de Entrenamiento: 0.7801\n",
      "Iteración 41606 - Lote 70/352 - Pérdida de Entrenamiento: 0.6692, Precisión de Entrenamiento: 0.7778\n",
      "Iteración 41641 - Lote 105/352 - Pérdida de Entrenamiento: 0.6681, Precisión de Entrenamiento: 0.7766\n",
      "Iteración 41676 - Lote 140/352 - Pérdida de Entrenamiento: 0.6795, Precisión de Entrenamiento: 0.7713\n",
      "Iteración 41711 - Lote 175/352 - Pérdida de Entrenamiento: 0.6792, Precisión de Entrenamiento: 0.7714\n",
      "Iteración 41746 - Lote 210/352 - Pérdida de Entrenamiento: 0.6790, Precisión de Entrenamiento: 0.7716\n",
      "Iteración 41781 - Lote 245/352 - Pérdida de Entrenamiento: 0.6783, Precisión de Entrenamiento: 0.7716\n",
      "Iteración 41816 - Lote 280/352 - Pérdida de Entrenamiento: 0.6785, Precisión de Entrenamiento: 0.7708\n",
      "Iteración 41851 - Lote 315/352 - Pérdida de Entrenamiento: 0.6812, Precisión de Entrenamiento: 0.7702\n",
      "Iteración 41886 - Lote 350/352 - Pérdida de Entrenamiento: 0.6823, Precisión de Entrenamiento: 0.7701\n",
      "Val loss: 0.6448, Val acc: 0.7790\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_119.pth\n",
      "Checkpoint del mejor modelo guardado en la época 119\n",
      "0.0125\n",
      "Epoch 120/400\n",
      "Iteración 41923 - Lote 35/352 - Pérdida de Entrenamiento: 0.6797, Precisión de Entrenamiento: 0.7730\n",
      "Iteración 41958 - Lote 70/352 - Pérdida de Entrenamiento: 0.6928, Precisión de Entrenamiento: 0.7690\n",
      "Iteración 41993 - Lote 105/352 - Pérdida de Entrenamiento: 0.6881, Precisión de Entrenamiento: 0.7682\n",
      "Iteración 42028 - Lote 140/352 - Pérdida de Entrenamiento: 0.6800, Precisión de Entrenamiento: 0.7723\n",
      "Iteración 42063 - Lote 175/352 - Pérdida de Entrenamiento: 0.6751, Precisión de Entrenamiento: 0.7739\n",
      "Iteración 42098 - Lote 210/352 - Pérdida de Entrenamiento: 0.6786, Precisión de Entrenamiento: 0.7714\n",
      "Iteración 42133 - Lote 245/352 - Pérdida de Entrenamiento: 0.6803, Precisión de Entrenamiento: 0.7707\n",
      "Iteración 42168 - Lote 280/352 - Pérdida de Entrenamiento: 0.6805, Precisión de Entrenamiento: 0.7701\n",
      "Iteración 42203 - Lote 315/352 - Pérdida de Entrenamiento: 0.6835, Precisión de Entrenamiento: 0.7697\n",
      "Iteración 42238 - Lote 350/352 - Pérdida de Entrenamiento: 0.6845, Precisión de Entrenamiento: 0.7698\n",
      "Val loss: 0.6556, Val acc: 0.7782\n",
      "Gradientes para features.0.0.weight: min=-0.14607077836990356, max=0.10645359009504318, mean=0.0004960930673405528, std=0.02861049957573414\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.07773986458778381, max=0.06766504049301147, mean=-1.1562125337150064e-06, std=0.0020379615016281605\n",
      "Gradientes para classifier.1.weight: min=-0.018775027245283127, max=0.015772417187690735, mean=3.725290215195187e-11, std=0.002080331789329648\n",
      "0.0125\n",
      "Epoch 121/400\n",
      "Iteración 42275 - Lote 35/352 - Pérdida de Entrenamiento: 0.6759, Precisión de Entrenamiento: 0.7634\n",
      "Iteración 42310 - Lote 70/352 - Pérdida de Entrenamiento: 0.6800, Precisión de Entrenamiento: 0.7690\n",
      "Iteración 42345 - Lote 105/352 - Pérdida de Entrenamiento: 0.6801, Precisión de Entrenamiento: 0.7693\n",
      "Iteración 42380 - Lote 140/352 - Pérdida de Entrenamiento: 0.6815, Precisión de Entrenamiento: 0.7694\n",
      "Iteración 42415 - Lote 175/352 - Pérdida de Entrenamiento: 0.6834, Precisión de Entrenamiento: 0.7683\n",
      "Iteración 42450 - Lote 210/352 - Pérdida de Entrenamiento: 0.6879, Precisión de Entrenamiento: 0.7672\n",
      "Iteración 42485 - Lote 245/352 - Pérdida de Entrenamiento: 0.6864, Precisión de Entrenamiento: 0.7678\n",
      "Iteración 42520 - Lote 280/352 - Pérdida de Entrenamiento: 0.6875, Precisión de Entrenamiento: 0.7675\n",
      "Iteración 42555 - Lote 315/352 - Pérdida de Entrenamiento: 0.6872, Precisión de Entrenamiento: 0.7675\n",
      "Iteración 42590 - Lote 350/352 - Pérdida de Entrenamiento: 0.6899, Precisión de Entrenamiento: 0.7668\n",
      "Val loss: 0.6643, Val acc: 0.7718\n",
      "0.0125\n",
      "Epoch 122/400\n",
      "Iteración 42627 - Lote 35/352 - Pérdida de Entrenamiento: 0.6926, Precisión de Entrenamiento: 0.7679\n",
      "Iteración 42662 - Lote 70/352 - Pérdida de Entrenamiento: 0.6781, Precisión de Entrenamiento: 0.7719\n",
      "Iteración 42697 - Lote 105/352 - Pérdida de Entrenamiento: 0.6770, Precisión de Entrenamiento: 0.7721\n",
      "Iteración 42732 - Lote 140/352 - Pérdida de Entrenamiento: 0.6781, Precisión de Entrenamiento: 0.7709\n",
      "Iteración 42767 - Lote 175/352 - Pérdida de Entrenamiento: 0.6814, Precisión de Entrenamiento: 0.7695\n",
      "Iteración 42802 - Lote 210/352 - Pérdida de Entrenamiento: 0.6814, Precisión de Entrenamiento: 0.7702\n",
      "Iteración 42837 - Lote 245/352 - Pérdida de Entrenamiento: 0.6839, Precisión de Entrenamiento: 0.7689\n",
      "Iteración 42872 - Lote 280/352 - Pérdida de Entrenamiento: 0.6805, Precisión de Entrenamiento: 0.7696\n",
      "Iteración 42907 - Lote 315/352 - Pérdida de Entrenamiento: 0.6834, Precisión de Entrenamiento: 0.7684\n",
      "Iteración 42942 - Lote 350/352 - Pérdida de Entrenamiento: 0.6821, Precisión de Entrenamiento: 0.7692\n",
      "Val loss: 0.6702, Val acc: 0.7718\n",
      "0.0125\n",
      "Epoch 123/400\n",
      "Iteración 42979 - Lote 35/352 - Pérdida de Entrenamiento: 0.6880, Precisión de Entrenamiento: 0.7681\n",
      "Iteración 43014 - Lote 70/352 - Pérdida de Entrenamiento: 0.6762, Precisión de Entrenamiento: 0.7733\n",
      "Iteración 43049 - Lote 105/352 - Pérdida de Entrenamiento: 0.6835, Precisión de Entrenamiento: 0.7708\n",
      "Iteración 43084 - Lote 140/352 - Pérdida de Entrenamiento: 0.6822, Precisión de Entrenamiento: 0.7695\n",
      "Iteración 43119 - Lote 175/352 - Pérdida de Entrenamiento: 0.6835, Precisión de Entrenamiento: 0.7702\n",
      "Iteración 43154 - Lote 210/352 - Pérdida de Entrenamiento: 0.6827, Precisión de Entrenamiento: 0.7695\n",
      "Iteración 43189 - Lote 245/352 - Pérdida de Entrenamiento: 0.6872, Precisión de Entrenamiento: 0.7690\n",
      "Iteración 43224 - Lote 280/352 - Pérdida de Entrenamiento: 0.6874, Precisión de Entrenamiento: 0.7693\n",
      "Iteración 43259 - Lote 315/352 - Pérdida de Entrenamiento: 0.6854, Precisión de Entrenamiento: 0.7695\n",
      "Iteración 43294 - Lote 350/352 - Pérdida de Entrenamiento: 0.6844, Precisión de Entrenamiento: 0.7704\n",
      "Val loss: 0.6360, Val acc: 0.7840\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_123.pth\n",
      "Checkpoint del mejor modelo guardado en la época 123\n",
      "0.0125\n",
      "Epoch 124/400\n",
      "Iteración 43331 - Lote 35/352 - Pérdida de Entrenamiento: 0.6923, Precisión de Entrenamiento: 0.7629\n",
      "Iteración 43366 - Lote 70/352 - Pérdida de Entrenamiento: 0.6961, Precisión de Entrenamiento: 0.7661\n",
      "Iteración 43401 - Lote 105/352 - Pérdida de Entrenamiento: 0.7021, Precisión de Entrenamiento: 0.7636\n",
      "Iteración 43436 - Lote 140/352 - Pérdida de Entrenamiento: 0.6933, Precisión de Entrenamiento: 0.7676\n",
      "Iteración 43471 - Lote 175/352 - Pérdida de Entrenamiento: 0.6869, Precisión de Entrenamiento: 0.7692\n",
      "Iteración 43506 - Lote 210/352 - Pérdida de Entrenamiento: 0.6877, Precisión de Entrenamiento: 0.7691\n",
      "Iteración 43541 - Lote 245/352 - Pérdida de Entrenamiento: 0.6839, Precisión de Entrenamiento: 0.7707\n",
      "Iteración 43576 - Lote 280/352 - Pérdida de Entrenamiento: 0.6810, Precisión de Entrenamiento: 0.7712\n",
      "Iteración 43611 - Lote 315/352 - Pérdida de Entrenamiento: 0.6803, Precisión de Entrenamiento: 0.7713\n",
      "Iteración 43646 - Lote 350/352 - Pérdida de Entrenamiento: 0.6798, Precisión de Entrenamiento: 0.7715\n",
      "Val loss: 0.6701, Val acc: 0.7768\n",
      "0.0125\n",
      "Epoch 125/400\n",
      "Iteración 43683 - Lote 35/352 - Pérdida de Entrenamiento: 0.6587, Precisión de Entrenamiento: 0.7788\n",
      "Iteración 43718 - Lote 70/352 - Pérdida de Entrenamiento: 0.6810, Precisión de Entrenamiento: 0.7727\n",
      "Iteración 43753 - Lote 105/352 - Pérdida de Entrenamiento: 0.6780, Precisión de Entrenamiento: 0.7727\n",
      "Iteración 43788 - Lote 140/352 - Pérdida de Entrenamiento: 0.6784, Precisión de Entrenamiento: 0.7727\n",
      "Iteración 43823 - Lote 175/352 - Pérdida de Entrenamiento: 0.6816, Precisión de Entrenamiento: 0.7714\n",
      "Iteración 43858 - Lote 210/352 - Pérdida de Entrenamiento: 0.6809, Precisión de Entrenamiento: 0.7724\n",
      "Iteración 43893 - Lote 245/352 - Pérdida de Entrenamiento: 0.6807, Precisión de Entrenamiento: 0.7723\n",
      "Iteración 43928 - Lote 280/352 - Pérdida de Entrenamiento: 0.6807, Precisión de Entrenamiento: 0.7727\n",
      "Iteración 43963 - Lote 315/352 - Pérdida de Entrenamiento: 0.6833, Precisión de Entrenamiento: 0.7704\n",
      "Iteración 43998 - Lote 350/352 - Pérdida de Entrenamiento: 0.6837, Precisión de Entrenamiento: 0.7700\n",
      "Val loss: 0.6667, Val acc: 0.7758\n",
      "Gradientes para features.0.0.weight: min=-0.11471258848905563, max=0.17329777777194977, mean=0.012672585435211658, std=0.03994273766875267\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.06252280622720718, max=0.0780092403292656, mean=-3.3272669952566503e-06, std=0.002216980094090104\n",
      "Gradientes para classifier.1.weight: min=-0.013004998676478863, max=0.013506773859262466, mean=9.313225537987968e-12, std=0.0019136567134410143\n",
      "0.0125\n",
      "Epoch 126/400\n",
      "Iteración 44035 - Lote 35/352 - Pérdida de Entrenamiento: 0.6756, Precisión de Entrenamiento: 0.7714\n",
      "Iteración 44070 - Lote 70/352 - Pérdida de Entrenamiento: 0.6937, Precisión de Entrenamiento: 0.7669\n",
      "Iteración 44105 - Lote 105/352 - Pérdida de Entrenamiento: 0.6846, Precisión de Entrenamiento: 0.7707\n",
      "Iteración 44140 - Lote 140/352 - Pérdida de Entrenamiento: 0.6777, Precisión de Entrenamiento: 0.7729\n",
      "Iteración 44175 - Lote 175/352 - Pérdida de Entrenamiento: 0.6799, Precisión de Entrenamiento: 0.7736\n",
      "Iteración 44210 - Lote 210/352 - Pérdida de Entrenamiento: 0.6813, Precisión de Entrenamiento: 0.7744\n",
      "Iteración 44245 - Lote 245/352 - Pérdida de Entrenamiento: 0.6778, Precisión de Entrenamiento: 0.7744\n",
      "Iteración 44280 - Lote 280/352 - Pérdida de Entrenamiento: 0.6773, Precisión de Entrenamiento: 0.7734\n",
      "Iteración 44315 - Lote 315/352 - Pérdida de Entrenamiento: 0.6809, Precisión de Entrenamiento: 0.7723\n",
      "Iteración 44350 - Lote 350/352 - Pérdida de Entrenamiento: 0.6802, Precisión de Entrenamiento: 0.7721\n",
      "Val loss: 0.7231, Val acc: 0.7552\n",
      "0.0125\n",
      "Epoch 127/400\n",
      "Iteración 44387 - Lote 35/352 - Pérdida de Entrenamiento: 0.6684, Precisión de Entrenamiento: 0.7801\n",
      "Iteración 44422 - Lote 70/352 - Pérdida de Entrenamiento: 0.6592, Precisión de Entrenamiento: 0.7790\n",
      "Iteración 44457 - Lote 105/352 - Pérdida de Entrenamiento: 0.6719, Precisión de Entrenamiento: 0.7769\n",
      "Iteración 44492 - Lote 140/352 - Pérdida de Entrenamiento: 0.6672, Precisión de Entrenamiento: 0.7783\n",
      "Iteración 44527 - Lote 175/352 - Pérdida de Entrenamiento: 0.6682, Precisión de Entrenamiento: 0.7780\n",
      "Iteración 44562 - Lote 210/352 - Pérdida de Entrenamiento: 0.6694, Precisión de Entrenamiento: 0.7776\n",
      "Iteración 44597 - Lote 245/352 - Pérdida de Entrenamiento: 0.6729, Precisión de Entrenamiento: 0.7751\n",
      "Iteración 44632 - Lote 280/352 - Pérdida de Entrenamiento: 0.6726, Precisión de Entrenamiento: 0.7761\n",
      "Iteración 44667 - Lote 315/352 - Pérdida de Entrenamiento: 0.6749, Precisión de Entrenamiento: 0.7753\n",
      "Iteración 44702 - Lote 350/352 - Pérdida de Entrenamiento: 0.6769, Precisión de Entrenamiento: 0.7741\n",
      "Val loss: 0.6556, Val acc: 0.7764\n",
      "0.0125\n",
      "Epoch 128/400\n",
      "Iteración 44739 - Lote 35/352 - Pérdida de Entrenamiento: 0.6723, Precisión de Entrenamiento: 0.7783\n",
      "Iteración 44774 - Lote 70/352 - Pérdida de Entrenamiento: 0.6771, Precisión de Entrenamiento: 0.7724\n",
      "Iteración 44809 - Lote 105/352 - Pérdida de Entrenamiento: 0.6731, Precisión de Entrenamiento: 0.7746\n",
      "Iteración 44844 - Lote 140/352 - Pérdida de Entrenamiento: 0.6699, Precisión de Entrenamiento: 0.7735\n",
      "Iteración 44879 - Lote 175/352 - Pérdida de Entrenamiento: 0.6755, Precisión de Entrenamiento: 0.7704\n",
      "Iteración 44914 - Lote 210/352 - Pérdida de Entrenamiento: 0.6740, Precisión de Entrenamiento: 0.7715\n",
      "Iteración 44949 - Lote 245/352 - Pérdida de Entrenamiento: 0.6781, Precisión de Entrenamiento: 0.7711\n",
      "Iteración 44984 - Lote 280/352 - Pérdida de Entrenamiento: 0.6796, Precisión de Entrenamiento: 0.7707\n",
      "Iteración 45019 - Lote 315/352 - Pérdida de Entrenamiento: 0.6813, Precisión de Entrenamiento: 0.7705\n",
      "Iteración 45054 - Lote 350/352 - Pérdida de Entrenamiento: 0.6834, Precisión de Entrenamiento: 0.7702\n",
      "Val loss: 0.7043, Val acc: 0.7650\n",
      "0.0125\n",
      "Epoch 129/400\n",
      "Iteración 45091 - Lote 35/352 - Pérdida de Entrenamiento: 0.6603, Precisión de Entrenamiento: 0.7848\n",
      "Iteración 45126 - Lote 70/352 - Pérdida de Entrenamiento: 0.6686, Precisión de Entrenamiento: 0.7756\n",
      "Iteración 45161 - Lote 105/352 - Pérdida de Entrenamiento: 0.6615, Precisión de Entrenamiento: 0.7798\n",
      "Iteración 45196 - Lote 140/352 - Pérdida de Entrenamiento: 0.6671, Precisión de Entrenamiento: 0.7781\n",
      "Iteración 45231 - Lote 175/352 - Pérdida de Entrenamiento: 0.6681, Precisión de Entrenamiento: 0.7766\n",
      "Iteración 45266 - Lote 210/352 - Pérdida de Entrenamiento: 0.6714, Precisión de Entrenamiento: 0.7756\n",
      "Iteración 45301 - Lote 245/352 - Pérdida de Entrenamiento: 0.6764, Precisión de Entrenamiento: 0.7742\n",
      "Iteración 45336 - Lote 280/352 - Pérdida de Entrenamiento: 0.6787, Precisión de Entrenamiento: 0.7729\n",
      "Iteración 45371 - Lote 315/352 - Pérdida de Entrenamiento: 0.6771, Precisión de Entrenamiento: 0.7735\n",
      "Iteración 45406 - Lote 350/352 - Pérdida de Entrenamiento: 0.6762, Precisión de Entrenamiento: 0.7739\n",
      "Val loss: 0.6381, Val acc: 0.7808\n",
      "0.0125\n",
      "Epoch 130/400\n",
      "Iteración 45443 - Lote 35/352 - Pérdida de Entrenamiento: 0.6635, Precisión de Entrenamiento: 0.7775\n",
      "Iteración 45478 - Lote 70/352 - Pérdida de Entrenamiento: 0.6570, Precisión de Entrenamiento: 0.7789\n",
      "Iteración 45513 - Lote 105/352 - Pérdida de Entrenamiento: 0.6636, Precisión de Entrenamiento: 0.7759\n",
      "Iteración 45548 - Lote 140/352 - Pérdida de Entrenamiento: 0.6717, Precisión de Entrenamiento: 0.7739\n",
      "Iteración 45583 - Lote 175/352 - Pérdida de Entrenamiento: 0.6800, Precisión de Entrenamiento: 0.7706\n",
      "Iteración 45618 - Lote 210/352 - Pérdida de Entrenamiento: 0.6819, Precisión de Entrenamiento: 0.7694\n",
      "Iteración 45653 - Lote 245/352 - Pérdida de Entrenamiento: 0.6832, Precisión de Entrenamiento: 0.7692\n",
      "Iteración 45688 - Lote 280/352 - Pérdida de Entrenamiento: 0.6793, Precisión de Entrenamiento: 0.7698\n",
      "Iteración 45723 - Lote 315/352 - Pérdida de Entrenamiento: 0.6801, Precisión de Entrenamiento: 0.7699\n",
      "Iteración 45758 - Lote 350/352 - Pérdida de Entrenamiento: 0.6808, Precisión de Entrenamiento: 0.7704\n",
      "Val loss: 0.6810, Val acc: 0.7728\n",
      "Gradientes para features.0.0.weight: min=-0.11034666001796722, max=0.06546158343553543, mean=-0.001277154660783708, std=0.025926705449819565\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.06700387597084045, max=0.036630984395742416, mean=-1.1576369018939658e-07, std=0.0014257784932851791\n",
      "Gradientes para classifier.1.weight: min=-0.011421305127441883, max=0.009954944252967834, mean=-8.149072562579907e-12, std=0.0016409156378358603\n",
      "0.0125\n",
      "Epoch 131/400\n",
      "Iteración 45795 - Lote 35/352 - Pérdida de Entrenamiento: 0.6553, Precisión de Entrenamiento: 0.7877\n",
      "Iteración 45830 - Lote 70/352 - Pérdida de Entrenamiento: 0.6553, Precisión de Entrenamiento: 0.7796\n",
      "Iteración 45865 - Lote 105/352 - Pérdida de Entrenamiento: 0.6556, Precisión de Entrenamiento: 0.7806\n",
      "Iteración 45900 - Lote 140/352 - Pérdida de Entrenamiento: 0.6590, Precisión de Entrenamiento: 0.7790\n",
      "Iteración 45935 - Lote 175/352 - Pérdida de Entrenamiento: 0.6635, Precisión de Entrenamiento: 0.7779\n",
      "Iteración 45970 - Lote 210/352 - Pérdida de Entrenamiento: 0.6674, Precisión de Entrenamiento: 0.7773\n",
      "Iteración 46005 - Lote 245/352 - Pérdida de Entrenamiento: 0.6683, Precisión de Entrenamiento: 0.7767\n",
      "Iteración 46040 - Lote 280/352 - Pérdida de Entrenamiento: 0.6696, Precisión de Entrenamiento: 0.7761\n",
      "Iteración 46075 - Lote 315/352 - Pérdida de Entrenamiento: 0.6716, Precisión de Entrenamiento: 0.7749\n",
      "Iteración 46110 - Lote 350/352 - Pérdida de Entrenamiento: 0.6709, Precisión de Entrenamiento: 0.7750\n",
      "Val loss: 0.6956, Val acc: 0.7636\n",
      "0.0125\n",
      "Epoch 132/400\n",
      "Iteración 46147 - Lote 35/352 - Pérdida de Entrenamiento: 0.6737, Precisión de Entrenamiento: 0.7817\n",
      "Iteración 46182 - Lote 70/352 - Pérdida de Entrenamiento: 0.6802, Precisión de Entrenamiento: 0.7766\n",
      "Iteración 46217 - Lote 105/352 - Pérdida de Entrenamiento: 0.6878, Precisión de Entrenamiento: 0.7740\n",
      "Iteración 46252 - Lote 140/352 - Pérdida de Entrenamiento: 0.6792, Precisión de Entrenamiento: 0.7759\n",
      "Iteración 46287 - Lote 175/352 - Pérdida de Entrenamiento: 0.6794, Precisión de Entrenamiento: 0.7727\n",
      "Iteración 46322 - Lote 210/352 - Pérdida de Entrenamiento: 0.6793, Precisión de Entrenamiento: 0.7726\n",
      "Iteración 46357 - Lote 245/352 - Pérdida de Entrenamiento: 0.6770, Precisión de Entrenamiento: 0.7729\n",
      "Iteración 46392 - Lote 280/352 - Pérdida de Entrenamiento: 0.6775, Precisión de Entrenamiento: 0.7727\n",
      "Iteración 46427 - Lote 315/352 - Pérdida de Entrenamiento: 0.6776, Precisión de Entrenamiento: 0.7735\n",
      "Iteración 46462 - Lote 350/352 - Pérdida de Entrenamiento: 0.6777, Precisión de Entrenamiento: 0.7734\n",
      "Val loss: 0.6450, Val acc: 0.7816\n",
      "0.00625\n",
      "Epoch 133/400\n",
      "Iteración 46499 - Lote 35/352 - Pérdida de Entrenamiento: 0.6280, Precisión de Entrenamiento: 0.7991\n",
      "Iteración 46534 - Lote 70/352 - Pérdida de Entrenamiento: 0.6274, Precisión de Entrenamiento: 0.7925\n",
      "Iteración 46569 - Lote 105/352 - Pérdida de Entrenamiento: 0.6185, Precisión de Entrenamiento: 0.7940\n",
      "Iteración 46604 - Lote 140/352 - Pérdida de Entrenamiento: 0.6145, Precisión de Entrenamiento: 0.7964\n",
      "Iteración 46639 - Lote 175/352 - Pérdida de Entrenamiento: 0.6177, Precisión de Entrenamiento: 0.7935\n",
      "Iteración 46674 - Lote 210/352 - Pérdida de Entrenamiento: 0.6142, Precisión de Entrenamiento: 0.7947\n",
      "Iteración 46709 - Lote 245/352 - Pérdida de Entrenamiento: 0.6109, Precisión de Entrenamiento: 0.7960\n",
      "Iteración 46744 - Lote 280/352 - Pérdida de Entrenamiento: 0.6093, Precisión de Entrenamiento: 0.7965\n",
      "Iteración 46779 - Lote 315/352 - Pérdida de Entrenamiento: 0.6090, Precisión de Entrenamiento: 0.7960\n",
      "Iteración 46814 - Lote 350/352 - Pérdida de Entrenamiento: 0.6088, Precisión de Entrenamiento: 0.7962\n",
      "Val loss: 0.5888, Val acc: 0.7982\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_133.pth\n",
      "Checkpoint del mejor modelo guardado en la época 133\n",
      "0.00625\n",
      "Epoch 134/400\n",
      "Iteración 46851 - Lote 35/352 - Pérdida de Entrenamiento: 0.5958, Precisión de Entrenamiento: 0.8022\n",
      "Iteración 46886 - Lote 70/352 - Pérdida de Entrenamiento: 0.5945, Precisión de Entrenamiento: 0.8027\n",
      "Iteración 46921 - Lote 105/352 - Pérdida de Entrenamiento: 0.5883, Precisión de Entrenamiento: 0.8053\n",
      "Iteración 46956 - Lote 140/352 - Pérdida de Entrenamiento: 0.5849, Precisión de Entrenamiento: 0.8056\n",
      "Iteración 46991 - Lote 175/352 - Pérdida de Entrenamiento: 0.5884, Precisión de Entrenamiento: 0.8036\n",
      "Iteración 47026 - Lote 210/352 - Pérdida de Entrenamiento: 0.5906, Precisión de Entrenamiento: 0.8022\n",
      "Iteración 47061 - Lote 245/352 - Pérdida de Entrenamiento: 0.5940, Precisión de Entrenamiento: 0.8011\n",
      "Iteración 47096 - Lote 280/352 - Pérdida de Entrenamiento: 0.5947, Precisión de Entrenamiento: 0.8010\n",
      "Iteración 47131 - Lote 315/352 - Pérdida de Entrenamiento: 0.5955, Precisión de Entrenamiento: 0.8002\n",
      "Iteración 47166 - Lote 350/352 - Pérdida de Entrenamiento: 0.5976, Precisión de Entrenamiento: 0.7984\n",
      "Val loss: 0.5869, Val acc: 0.7970\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_134.pth\n",
      "Checkpoint del mejor modelo guardado en la época 134\n",
      "0.00625\n",
      "Epoch 135/400\n",
      "Iteración 47203 - Lote 35/352 - Pérdida de Entrenamiento: 0.5926, Precisión de Entrenamiento: 0.7978\n",
      "Iteración 47238 - Lote 70/352 - Pérdida de Entrenamiento: 0.5953, Precisión de Entrenamiento: 0.7979\n",
      "Iteración 47273 - Lote 105/352 - Pérdida de Entrenamiento: 0.5921, Precisión de Entrenamiento: 0.7984\n",
      "Iteración 47308 - Lote 140/352 - Pérdida de Entrenamiento: 0.5950, Precisión de Entrenamiento: 0.7975\n",
      "Iteración 47343 - Lote 175/352 - Pérdida de Entrenamiento: 0.5913, Precisión de Entrenamiento: 0.8001\n",
      "Iteración 47378 - Lote 210/352 - Pérdida de Entrenamiento: 0.5919, Precisión de Entrenamiento: 0.8000\n",
      "Iteración 47413 - Lote 245/352 - Pérdida de Entrenamiento: 0.5900, Precisión de Entrenamiento: 0.8008\n",
      "Iteración 47448 - Lote 280/352 - Pérdida de Entrenamiento: 0.5905, Precisión de Entrenamiento: 0.8012\n",
      "Iteración 47483 - Lote 315/352 - Pérdida de Entrenamiento: 0.5928, Precisión de Entrenamiento: 0.8008\n",
      "Iteración 47518 - Lote 350/352 - Pérdida de Entrenamiento: 0.5899, Precisión de Entrenamiento: 0.8025\n",
      "Val loss: 0.5634, Val acc: 0.8070\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_135.pth\n",
      "Checkpoint del mejor modelo guardado en la época 135\n",
      "Gradientes para features.0.0.weight: min=-0.13604307174682617, max=0.1384628266096115, mean=-0.0006799157708883286, std=0.034030839800834656\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.0924232006072998, max=0.07901931554079056, mean=-6.40708776700194e-06, std=0.0021846576128154993\n",
      "Gradientes para classifier.1.weight: min=-0.01483860146254301, max=0.011555660516023636, mean=0.0, std=0.0019138016505166888\n",
      "0.00625\n",
      "Epoch 136/400\n",
      "Iteración 47555 - Lote 35/352 - Pérdida de Entrenamiento: 0.5803, Precisión de Entrenamiento: 0.8089\n",
      "Iteración 47590 - Lote 70/352 - Pérdida de Entrenamiento: 0.5855, Precisión de Entrenamiento: 0.8035\n",
      "Iteración 47625 - Lote 105/352 - Pérdida de Entrenamiento: 0.5879, Precisión de Entrenamiento: 0.8025\n",
      "Iteración 47660 - Lote 140/352 - Pérdida de Entrenamiento: 0.5892, Precisión de Entrenamiento: 0.8011\n",
      "Iteración 47695 - Lote 175/352 - Pérdida de Entrenamiento: 0.5892, Precisión de Entrenamiento: 0.8008\n",
      "Iteración 47730 - Lote 210/352 - Pérdida de Entrenamiento: 0.5896, Precisión de Entrenamiento: 0.8008\n",
      "Iteración 47765 - Lote 245/352 - Pérdida de Entrenamiento: 0.5912, Precisión de Entrenamiento: 0.8004\n",
      "Iteración 47800 - Lote 280/352 - Pérdida de Entrenamiento: 0.5904, Precisión de Entrenamiento: 0.8002\n",
      "Iteración 47835 - Lote 315/352 - Pérdida de Entrenamiento: 0.5900, Precisión de Entrenamiento: 0.8004\n",
      "Iteración 47870 - Lote 350/352 - Pérdida de Entrenamiento: 0.5917, Precisión de Entrenamiento: 0.8006\n",
      "Val loss: 0.5767, Val acc: 0.8030\n",
      "0.00625\n",
      "Epoch 137/400\n",
      "Iteración 47907 - Lote 35/352 - Pérdida de Entrenamiento: 0.5804, Precisión de Entrenamiento: 0.8022\n",
      "Iteración 47942 - Lote 70/352 - Pérdida de Entrenamiento: 0.5780, Precisión de Entrenamiento: 0.8019\n",
      "Iteración 47977 - Lote 105/352 - Pérdida de Entrenamiento: 0.5849, Precisión de Entrenamiento: 0.8006\n",
      "Iteración 48012 - Lote 140/352 - Pérdida de Entrenamiento: 0.5913, Precisión de Entrenamiento: 0.7984\n",
      "Iteración 48047 - Lote 175/352 - Pérdida de Entrenamiento: 0.5928, Precisión de Entrenamiento: 0.7987\n",
      "Iteración 48082 - Lote 210/352 - Pérdida de Entrenamiento: 0.5915, Precisión de Entrenamiento: 0.8001\n",
      "Iteración 48117 - Lote 245/352 - Pérdida de Entrenamiento: 0.5945, Precisión de Entrenamiento: 0.8000\n",
      "Iteración 48152 - Lote 280/352 - Pérdida de Entrenamiento: 0.5932, Precisión de Entrenamiento: 0.8006\n",
      "Iteración 48187 - Lote 315/352 - Pérdida de Entrenamiento: 0.5921, Precisión de Entrenamiento: 0.8009\n",
      "Iteración 48222 - Lote 350/352 - Pérdida de Entrenamiento: 0.5948, Precisión de Entrenamiento: 0.7996\n",
      "Val loss: 0.5765, Val acc: 0.8054\n",
      "0.00625\n",
      "Epoch 138/400\n",
      "Iteración 48259 - Lote 35/352 - Pérdida de Entrenamiento: 0.5787, Precisión de Entrenamiento: 0.8027\n",
      "Iteración 48294 - Lote 70/352 - Pérdida de Entrenamiento: 0.5729, Precisión de Entrenamiento: 0.8040\n",
      "Iteración 48329 - Lote 105/352 - Pérdida de Entrenamiento: 0.5690, Precisión de Entrenamiento: 0.8068\n",
      "Iteración 48364 - Lote 140/352 - Pérdida de Entrenamiento: 0.5756, Precisión de Entrenamiento: 0.8045\n",
      "Iteración 48399 - Lote 175/352 - Pérdida de Entrenamiento: 0.5805, Precisión de Entrenamiento: 0.8042\n",
      "Iteración 48434 - Lote 210/352 - Pérdida de Entrenamiento: 0.5856, Precisión de Entrenamiento: 0.8032\n",
      "Iteración 48469 - Lote 245/352 - Pérdida de Entrenamiento: 0.5876, Precisión de Entrenamiento: 0.8024\n",
      "Iteración 48504 - Lote 280/352 - Pérdida de Entrenamiento: 0.5884, Precisión de Entrenamiento: 0.8023\n",
      "Iteración 48539 - Lote 315/352 - Pérdida de Entrenamiento: 0.5892, Precisión de Entrenamiento: 0.8018\n",
      "Iteración 48574 - Lote 350/352 - Pérdida de Entrenamiento: 0.5895, Precisión de Entrenamiento: 0.8014\n",
      "Val loss: 0.6060, Val acc: 0.8008\n",
      "0.00625\n",
      "Epoch 139/400\n",
      "Iteración 48611 - Lote 35/352 - Pérdida de Entrenamiento: 0.5820, Precisión de Entrenamiento: 0.8100\n",
      "Iteración 48646 - Lote 70/352 - Pérdida de Entrenamiento: 0.5966, Precisión de Entrenamiento: 0.8012\n",
      "Iteración 48681 - Lote 105/352 - Pérdida de Entrenamiento: 0.5927, Precisión de Entrenamiento: 0.8016\n",
      "Iteración 48716 - Lote 140/352 - Pérdida de Entrenamiento: 0.5932, Precisión de Entrenamiento: 0.8016\n",
      "Iteración 48751 - Lote 175/352 - Pérdida de Entrenamiento: 0.5900, Precisión de Entrenamiento: 0.8025\n",
      "Iteración 48786 - Lote 210/352 - Pérdida de Entrenamiento: 0.5909, Precisión de Entrenamiento: 0.8028\n",
      "Iteración 48821 - Lote 245/352 - Pérdida de Entrenamiento: 0.5941, Precisión de Entrenamiento: 0.8022\n",
      "Iteración 48856 - Lote 280/352 - Pérdida de Entrenamiento: 0.5960, Precisión de Entrenamiento: 0.8013\n",
      "Iteración 48891 - Lote 315/352 - Pérdida de Entrenamiento: 0.5968, Precisión de Entrenamiento: 0.8016\n",
      "Iteración 48926 - Lote 350/352 - Pérdida de Entrenamiento: 0.5971, Precisión de Entrenamiento: 0.8013\n",
      "Val loss: 0.5956, Val acc: 0.7998\n",
      "0.00625\n",
      "Epoch 140/400\n",
      "Iteración 48963 - Lote 35/352 - Pérdida de Entrenamiento: 0.5785, Precisión de Entrenamiento: 0.8027\n",
      "Iteración 48998 - Lote 70/352 - Pérdida de Entrenamiento: 0.5700, Precisión de Entrenamiento: 0.8046\n",
      "Iteración 49033 - Lote 105/352 - Pérdida de Entrenamiento: 0.5715, Precisión de Entrenamiento: 0.8056\n",
      "Iteración 49068 - Lote 140/352 - Pérdida de Entrenamiento: 0.5755, Precisión de Entrenamiento: 0.8042\n",
      "Iteración 49103 - Lote 175/352 - Pérdida de Entrenamiento: 0.5802, Precisión de Entrenamiento: 0.8024\n",
      "Iteración 49138 - Lote 210/352 - Pérdida de Entrenamiento: 0.5846, Precisión de Entrenamiento: 0.8007\n",
      "Iteración 49173 - Lote 245/352 - Pérdida de Entrenamiento: 0.5859, Precisión de Entrenamiento: 0.8014\n",
      "Iteración 49208 - Lote 280/352 - Pérdida de Entrenamiento: 0.5880, Precisión de Entrenamiento: 0.8012\n",
      "Iteración 49243 - Lote 315/352 - Pérdida de Entrenamiento: 0.5891, Precisión de Entrenamiento: 0.8010\n",
      "Iteración 49278 - Lote 350/352 - Pérdida de Entrenamiento: 0.5912, Precisión de Entrenamiento: 0.8007\n",
      "Val loss: 0.5713, Val acc: 0.8034\n",
      "Gradientes para features.0.0.weight: min=-0.08922508358955383, max=0.11588262766599655, mean=0.0011475621722638607, std=0.028475655242800713\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.06558925658464432, max=0.08116842806339264, mean=-3.8053779007896082e-06, std=0.0021586446091532707\n",
      "Gradientes para classifier.1.weight: min=-0.015172066166996956, max=0.01349869929254055, mean=9.313225537987968e-12, std=0.0017605733592063189\n",
      "0.00625\n",
      "Epoch 141/400\n",
      "Iteración 49315 - Lote 35/352 - Pérdida de Entrenamiento: 0.5845, Precisión de Entrenamiento: 0.8040\n",
      "Iteración 49350 - Lote 70/352 - Pérdida de Entrenamiento: 0.5959, Precisión de Entrenamiento: 0.7996\n",
      "Iteración 49385 - Lote 105/352 - Pérdida de Entrenamiento: 0.5931, Precisión de Entrenamiento: 0.7985\n",
      "Iteración 49420 - Lote 140/352 - Pérdida de Entrenamiento: 0.5927, Precisión de Entrenamiento: 0.7992\n",
      "Iteración 49455 - Lote 175/352 - Pérdida de Entrenamiento: 0.5921, Precisión de Entrenamiento: 0.7992\n",
      "Iteración 49490 - Lote 210/352 - Pérdida de Entrenamiento: 0.5921, Precisión de Entrenamiento: 0.7999\n",
      "Iteración 49525 - Lote 245/352 - Pérdida de Entrenamiento: 0.5899, Precisión de Entrenamiento: 0.8011\n",
      "Iteración 49560 - Lote 280/352 - Pérdida de Entrenamiento: 0.5900, Precisión de Entrenamiento: 0.8009\n",
      "Iteración 49595 - Lote 315/352 - Pérdida de Entrenamiento: 0.5902, Precisión de Entrenamiento: 0.8006\n",
      "Iteración 49630 - Lote 350/352 - Pérdida de Entrenamiento: 0.5927, Precisión de Entrenamiento: 0.8001\n",
      "Val loss: 0.5960, Val acc: 0.7990\n",
      "0.00625\n",
      "Epoch 142/400\n",
      "Iteración 49667 - Lote 35/352 - Pérdida de Entrenamiento: 0.5701, Precisión de Entrenamiento: 0.8051\n",
      "Iteración 49702 - Lote 70/352 - Pérdida de Entrenamiento: 0.5831, Precisión de Entrenamiento: 0.8003\n",
      "Iteración 49737 - Lote 105/352 - Pérdida de Entrenamiento: 0.5917, Precisión de Entrenamiento: 0.7998\n",
      "Iteración 49772 - Lote 140/352 - Pérdida de Entrenamiento: 0.5825, Precisión de Entrenamiento: 0.8044\n",
      "Iteración 49807 - Lote 175/352 - Pérdida de Entrenamiento: 0.5893, Precisión de Entrenamiento: 0.8031\n",
      "Iteración 49842 - Lote 210/352 - Pérdida de Entrenamiento: 0.5876, Precisión de Entrenamiento: 0.8026\n",
      "Iteración 49877 - Lote 245/352 - Pérdida de Entrenamiento: 0.5903, Precisión de Entrenamiento: 0.8016\n",
      "Iteración 49912 - Lote 280/352 - Pérdida de Entrenamiento: 0.5944, Precisión de Entrenamiento: 0.8007\n",
      "Iteración 49947 - Lote 315/352 - Pérdida de Entrenamiento: 0.5945, Precisión de Entrenamiento: 0.8008\n",
      "Iteración 49982 - Lote 350/352 - Pérdida de Entrenamiento: 0.5971, Precisión de Entrenamiento: 0.8003\n",
      "Val loss: 0.5729, Val acc: 0.8040\n",
      "0.00625\n",
      "Epoch 143/400\n",
      "Iteración 50019 - Lote 35/352 - Pérdida de Entrenamiento: 0.5986, Precisión de Entrenamiento: 0.8027\n",
      "Iteración 50054 - Lote 70/352 - Pérdida de Entrenamiento: 0.5902, Precisión de Entrenamiento: 0.8044\n",
      "Iteración 50089 - Lote 105/352 - Pérdida de Entrenamiento: 0.5961, Precisión de Entrenamiento: 0.8012\n",
      "Iteración 50124 - Lote 140/352 - Pérdida de Entrenamiento: 0.5923, Precisión de Entrenamiento: 0.8027\n",
      "Iteración 50159 - Lote 175/352 - Pérdida de Entrenamiento: 0.5936, Precisión de Entrenamiento: 0.8017\n",
      "Iteración 50194 - Lote 210/352 - Pérdida de Entrenamiento: 0.5940, Precisión de Entrenamiento: 0.8023\n",
      "Iteración 50229 - Lote 245/352 - Pérdida de Entrenamiento: 0.5948, Precisión de Entrenamiento: 0.8018\n",
      "Iteración 50264 - Lote 280/352 - Pérdida de Entrenamiento: 0.5940, Precisión de Entrenamiento: 0.8015\n",
      "Iteración 50299 - Lote 315/352 - Pérdida de Entrenamiento: 0.5905, Precisión de Entrenamiento: 0.8026\n",
      "Iteración 50334 - Lote 350/352 - Pérdida de Entrenamiento: 0.5909, Precisión de Entrenamiento: 0.8028\n",
      "Val loss: 0.5956, Val acc: 0.7958\n",
      "0.00625\n",
      "Epoch 144/400\n",
      "Iteración 50371 - Lote 35/352 - Pérdida de Entrenamiento: 0.5923, Precisión de Entrenamiento: 0.7996\n",
      "Iteración 50406 - Lote 70/352 - Pérdida de Entrenamiento: 0.5929, Precisión de Entrenamiento: 0.8021\n",
      "Iteración 50441 - Lote 105/352 - Pérdida de Entrenamiento: 0.5896, Precisión de Entrenamiento: 0.8041\n",
      "Iteración 50476 - Lote 140/352 - Pérdida de Entrenamiento: 0.5908, Precisión de Entrenamiento: 0.8028\n",
      "Iteración 50511 - Lote 175/352 - Pérdida de Entrenamiento: 0.5941, Precisión de Entrenamiento: 0.8010\n",
      "Iteración 50546 - Lote 210/352 - Pérdida de Entrenamiento: 0.5978, Precisión de Entrenamiento: 0.7990\n",
      "Iteración 50581 - Lote 245/352 - Pérdida de Entrenamiento: 0.5935, Precisión de Entrenamiento: 0.8007\n",
      "Iteración 50616 - Lote 280/352 - Pérdida de Entrenamiento: 0.5952, Precisión de Entrenamiento: 0.8007\n",
      "Iteración 50651 - Lote 315/352 - Pérdida de Entrenamiento: 0.5980, Precisión de Entrenamiento: 0.7992\n",
      "Iteración 50686 - Lote 350/352 - Pérdida de Entrenamiento: 0.5980, Precisión de Entrenamiento: 0.7996\n",
      "Val loss: 0.5928, Val acc: 0.7936\n",
      "0.003125\n",
      "Epoch 145/400\n",
      "Iteración 50723 - Lote 35/352 - Pérdida de Entrenamiento: 0.5637, Precisión de Entrenamiento: 0.8150\n",
      "Iteración 50758 - Lote 70/352 - Pérdida de Entrenamiento: 0.5634, Precisión de Entrenamiento: 0.8100\n",
      "Iteración 50793 - Lote 105/352 - Pérdida de Entrenamiento: 0.5530, Precisión de Entrenamiento: 0.8143\n",
      "Iteración 50828 - Lote 140/352 - Pérdida de Entrenamiento: 0.5531, Precisión de Entrenamiento: 0.8163\n",
      "Iteración 50863 - Lote 175/352 - Pérdida de Entrenamiento: 0.5457, Precisión de Entrenamiento: 0.8189\n",
      "Iteración 50898 - Lote 210/352 - Pérdida de Entrenamiento: 0.5455, Precisión de Entrenamiento: 0.8198\n",
      "Iteración 50933 - Lote 245/352 - Pérdida de Entrenamiento: 0.5431, Precisión de Entrenamiento: 0.8193\n",
      "Iteración 50968 - Lote 280/352 - Pérdida de Entrenamiento: 0.5455, Precisión de Entrenamiento: 0.8188\n",
      "Iteración 51003 - Lote 315/352 - Pérdida de Entrenamiento: 0.5459, Precisión de Entrenamiento: 0.8182\n",
      "Iteración 51038 - Lote 350/352 - Pérdida de Entrenamiento: 0.5451, Precisión de Entrenamiento: 0.8186\n",
      "Val loss: 0.5241, Val acc: 0.8240\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_145.pth\n",
      "Checkpoint del mejor modelo guardado en la época 145\n",
      "Gradientes para features.0.0.weight: min=-0.10530644655227661, max=0.1495378017425537, mean=0.00016657648666296154, std=0.028933126479387283\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.07477264851331711, max=0.0939573124051094, mean=-1.004009391181171e-05, std=0.0026093374472111464\n",
      "Gradientes para classifier.1.weight: min=-0.014297676272690296, max=0.019200609996914864, mean=-4.656612768993984e-12, std=0.0019791657105088234\n",
      "0.003125\n",
      "Epoch 146/400\n",
      "Iteración 51075 - Lote 35/352 - Pérdida de Entrenamiento: 0.5210, Precisión de Entrenamiento: 0.8225\n",
      "Iteración 51110 - Lote 70/352 - Pérdida de Entrenamiento: 0.5380, Precisión de Entrenamiento: 0.8193\n",
      "Iteración 51145 - Lote 105/352 - Pérdida de Entrenamiento: 0.5348, Precisión de Entrenamiento: 0.8215\n",
      "Iteración 51180 - Lote 140/352 - Pérdida de Entrenamiento: 0.5422, Precisión de Entrenamiento: 0.8185\n",
      "Iteración 51215 - Lote 175/352 - Pérdida de Entrenamiento: 0.5442, Precisión de Entrenamiento: 0.8183\n",
      "Iteración 51250 - Lote 210/352 - Pérdida de Entrenamiento: 0.5435, Precisión de Entrenamiento: 0.8187\n",
      "Iteración 51285 - Lote 245/352 - Pérdida de Entrenamiento: 0.5441, Precisión de Entrenamiento: 0.8179\n",
      "Iteración 51320 - Lote 280/352 - Pérdida de Entrenamiento: 0.5414, Precisión de Entrenamiento: 0.8188\n",
      "Iteración 51355 - Lote 315/352 - Pérdida de Entrenamiento: 0.5408, Precisión de Entrenamiento: 0.8188\n",
      "Iteración 51390 - Lote 350/352 - Pérdida de Entrenamiento: 0.5395, Precisión de Entrenamiento: 0.8194\n",
      "Val loss: 0.5349, Val acc: 0.8200\n",
      "0.003125\n",
      "Epoch 147/400\n",
      "Iteración 51427 - Lote 35/352 - Pérdida de Entrenamiento: 0.5272, Precisión de Entrenamiento: 0.8214\n",
      "Iteración 51462 - Lote 70/352 - Pérdida de Entrenamiento: 0.5279, Precisión de Entrenamiento: 0.8228\n",
      "Iteración 51497 - Lote 105/352 - Pérdida de Entrenamiento: 0.5303, Precisión de Entrenamiento: 0.8203\n",
      "Iteración 51532 - Lote 140/352 - Pérdida de Entrenamiento: 0.5321, Precisión de Entrenamiento: 0.8208\n",
      "Iteración 51567 - Lote 175/352 - Pérdida de Entrenamiento: 0.5329, Precisión de Entrenamiento: 0.8210\n",
      "Iteración 51602 - Lote 210/352 - Pérdida de Entrenamiento: 0.5358, Precisión de Entrenamiento: 0.8196\n",
      "Iteración 51637 - Lote 245/352 - Pérdida de Entrenamiento: 0.5376, Precisión de Entrenamiento: 0.8181\n",
      "Iteración 51672 - Lote 280/352 - Pérdida de Entrenamiento: 0.5399, Precisión de Entrenamiento: 0.8179\n",
      "Iteración 51707 - Lote 315/352 - Pérdida de Entrenamiento: 0.5400, Precisión de Entrenamiento: 0.8185\n",
      "Iteración 51742 - Lote 350/352 - Pérdida de Entrenamiento: 0.5402, Precisión de Entrenamiento: 0.8181\n",
      "Val loss: 0.5275, Val acc: 0.8214\n",
      "0.003125\n",
      "Epoch 148/400\n",
      "Iteración 51779 - Lote 35/352 - Pérdida de Entrenamiento: 0.5295, Precisión de Entrenamiento: 0.8176\n",
      "Iteración 51814 - Lote 70/352 - Pérdida de Entrenamiento: 0.5275, Precisión de Entrenamiento: 0.8222\n",
      "Iteración 51849 - Lote 105/352 - Pérdida de Entrenamiento: 0.5315, Precisión de Entrenamiento: 0.8226\n",
      "Iteración 51884 - Lote 140/352 - Pérdida de Entrenamiento: 0.5320, Precisión de Entrenamiento: 0.8215\n",
      "Iteración 51919 - Lote 175/352 - Pérdida de Entrenamiento: 0.5357, Precisión de Entrenamiento: 0.8195\n",
      "Iteración 51954 - Lote 210/352 - Pérdida de Entrenamiento: 0.5394, Precisión de Entrenamiento: 0.8182\n",
      "Iteración 51989 - Lote 245/352 - Pérdida de Entrenamiento: 0.5355, Precisión de Entrenamiento: 0.8195\n",
      "Iteración 52024 - Lote 280/352 - Pérdida de Entrenamiento: 0.5379, Precisión de Entrenamiento: 0.8186\n",
      "Iteración 52059 - Lote 315/352 - Pérdida de Entrenamiento: 0.5371, Precisión de Entrenamiento: 0.8183\n",
      "Iteración 52094 - Lote 350/352 - Pérdida de Entrenamiento: 0.5406, Precisión de Entrenamiento: 0.8171\n",
      "Val loss: 0.5417, Val acc: 0.8142\n",
      "0.003125\n",
      "Epoch 149/400\n",
      "Iteración 52131 - Lote 35/352 - Pérdida de Entrenamiento: 0.5193, Precisión de Entrenamiento: 0.8212\n",
      "Iteración 52166 - Lote 70/352 - Pérdida de Entrenamiento: 0.5305, Precisión de Entrenamiento: 0.8211\n",
      "Iteración 52201 - Lote 105/352 - Pérdida de Entrenamiento: 0.5361, Precisión de Entrenamiento: 0.8196\n",
      "Iteración 52236 - Lote 140/352 - Pérdida de Entrenamiento: 0.5396, Precisión de Entrenamiento: 0.8180\n",
      "Iteración 52271 - Lote 175/352 - Pérdida de Entrenamiento: 0.5403, Precisión de Entrenamiento: 0.8176\n",
      "Iteración 52306 - Lote 210/352 - Pérdida de Entrenamiento: 0.5394, Precisión de Entrenamiento: 0.8174\n",
      "Iteración 52341 - Lote 245/352 - Pérdida de Entrenamiento: 0.5368, Precisión de Entrenamiento: 0.8180\n",
      "Iteración 52376 - Lote 280/352 - Pérdida de Entrenamiento: 0.5354, Precisión de Entrenamiento: 0.8189\n",
      "Iteración 52411 - Lote 315/352 - Pérdida de Entrenamiento: 0.5345, Precisión de Entrenamiento: 0.8192\n",
      "Iteración 52446 - Lote 350/352 - Pérdida de Entrenamiento: 0.5354, Precisión de Entrenamiento: 0.8192\n",
      "Val loss: 0.5216, Val acc: 0.8234\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_149.pth\n",
      "Checkpoint del mejor modelo guardado en la época 149\n",
      "0.003125\n",
      "Epoch 150/400\n",
      "Iteración 52483 - Lote 35/352 - Pérdida de Entrenamiento: 0.5210, Precisión de Entrenamiento: 0.8221\n",
      "Iteración 52518 - Lote 70/352 - Pérdida de Entrenamiento: 0.5451, Precisión de Entrenamiento: 0.8173\n",
      "Iteración 52553 - Lote 105/352 - Pérdida de Entrenamiento: 0.5331, Precisión de Entrenamiento: 0.8213\n",
      "Iteración 52588 - Lote 140/352 - Pérdida de Entrenamiento: 0.5301, Precisión de Entrenamiento: 0.8208\n",
      "Iteración 52623 - Lote 175/352 - Pérdida de Entrenamiento: 0.5333, Precisión de Entrenamiento: 0.8199\n",
      "Iteración 52658 - Lote 210/352 - Pérdida de Entrenamiento: 0.5355, Precisión de Entrenamiento: 0.8184\n",
      "Iteración 52693 - Lote 245/352 - Pérdida de Entrenamiento: 0.5356, Precisión de Entrenamiento: 0.8185\n",
      "Iteración 52728 - Lote 280/352 - Pérdida de Entrenamiento: 0.5328, Precisión de Entrenamiento: 0.8196\n",
      "Iteración 52763 - Lote 315/352 - Pérdida de Entrenamiento: 0.5331, Precisión de Entrenamiento: 0.8197\n",
      "Iteración 52798 - Lote 350/352 - Pérdida de Entrenamiento: 0.5335, Precisión de Entrenamiento: 0.8195\n",
      "Val loss: 0.5344, Val acc: 0.8168\n",
      "Gradientes para features.0.0.weight: min=-0.18042878806591034, max=0.2182300090789795, mean=0.00845024362206459, std=0.06291339546442032\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.07808871567249298, max=0.14242224395275116, mean=1.0711635241023032e-06, std=0.003121335059404373\n",
      "Gradientes para classifier.1.weight: min=-0.023282786831259727, max=0.018851004540920258, mean=3.725290215195187e-11, std=0.002297561150044203\n",
      "0.003125\n",
      "Epoch 151/400\n",
      "Iteración 52835 - Lote 35/352 - Pérdida de Entrenamiento: 0.5300, Precisión de Entrenamiento: 0.8181\n",
      "Iteración 52870 - Lote 70/352 - Pérdida de Entrenamiento: 0.5282, Precisión de Entrenamiento: 0.8183\n",
      "Iteración 52905 - Lote 105/352 - Pérdida de Entrenamiento: 0.5246, Precisión de Entrenamiento: 0.8214\n",
      "Iteración 52940 - Lote 140/352 - Pérdida de Entrenamiento: 0.5259, Precisión de Entrenamiento: 0.8220\n",
      "Iteración 52975 - Lote 175/352 - Pérdida de Entrenamiento: 0.5297, Precisión de Entrenamiento: 0.8205\n",
      "Iteración 53010 - Lote 210/352 - Pérdida de Entrenamiento: 0.5320, Precisión de Entrenamiento: 0.8207\n",
      "Iteración 53045 - Lote 245/352 - Pérdida de Entrenamiento: 0.5312, Precisión de Entrenamiento: 0.8209\n",
      "Iteración 53080 - Lote 280/352 - Pérdida de Entrenamiento: 0.5326, Precisión de Entrenamiento: 0.8210\n",
      "Iteración 53115 - Lote 315/352 - Pérdida de Entrenamiento: 0.5355, Precisión de Entrenamiento: 0.8204\n",
      "Iteración 53150 - Lote 350/352 - Pérdida de Entrenamiento: 0.5366, Precisión de Entrenamiento: 0.8203\n",
      "Val loss: 0.5453, Val acc: 0.8124\n",
      "0.003125\n",
      "Epoch 152/400\n",
      "Iteración 53187 - Lote 35/352 - Pérdida de Entrenamiento: 0.5217, Precisión de Entrenamiento: 0.8232\n",
      "Iteración 53222 - Lote 70/352 - Pérdida de Entrenamiento: 0.5187, Precisión de Entrenamiento: 0.8244\n",
      "Iteración 53257 - Lote 105/352 - Pérdida de Entrenamiento: 0.5242, Precisión de Entrenamiento: 0.8217\n",
      "Iteración 53292 - Lote 140/352 - Pérdida de Entrenamiento: 0.5288, Precisión de Entrenamiento: 0.8203\n",
      "Iteración 53327 - Lote 175/352 - Pérdida de Entrenamiento: 0.5331, Precisión de Entrenamiento: 0.8187\n",
      "Iteración 53362 - Lote 210/352 - Pérdida de Entrenamiento: 0.5365, Precisión de Entrenamiento: 0.8171\n",
      "Iteración 53397 - Lote 245/352 - Pérdida de Entrenamiento: 0.5371, Precisión de Entrenamiento: 0.8179\n",
      "Iteración 53432 - Lote 280/352 - Pérdida de Entrenamiento: 0.5342, Precisión de Entrenamiento: 0.8191\n",
      "Iteración 53467 - Lote 315/352 - Pérdida de Entrenamiento: 0.5358, Precisión de Entrenamiento: 0.8188\n",
      "Iteración 53502 - Lote 350/352 - Pérdida de Entrenamiento: 0.5385, Precisión de Entrenamiento: 0.8179\n",
      "Val loss: 0.5280, Val acc: 0.8204\n",
      "0.003125\n",
      "Epoch 153/400\n",
      "Iteración 53539 - Lote 35/352 - Pérdida de Entrenamiento: 0.5232, Precisión de Entrenamiento: 0.8228\n",
      "Iteración 53574 - Lote 70/352 - Pérdida de Entrenamiento: 0.5231, Precisión de Entrenamiento: 0.8212\n",
      "Iteración 53609 - Lote 105/352 - Pérdida de Entrenamiento: 0.5256, Precisión de Entrenamiento: 0.8209\n",
      "Iteración 53644 - Lote 140/352 - Pérdida de Entrenamiento: 0.5283, Precisión de Entrenamiento: 0.8213\n",
      "Iteración 53679 - Lote 175/352 - Pérdida de Entrenamiento: 0.5309, Precisión de Entrenamiento: 0.8204\n",
      "Iteración 53714 - Lote 210/352 - Pérdida de Entrenamiento: 0.5349, Precisión de Entrenamiento: 0.8196\n",
      "Iteración 53749 - Lote 245/352 - Pérdida de Entrenamiento: 0.5371, Precisión de Entrenamiento: 0.8191\n",
      "Iteración 53784 - Lote 280/352 - Pérdida de Entrenamiento: 0.5409, Precisión de Entrenamiento: 0.8187\n",
      "Iteración 53819 - Lote 315/352 - Pérdida de Entrenamiento: 0.5411, Precisión de Entrenamiento: 0.8187\n",
      "Iteración 53854 - Lote 350/352 - Pérdida de Entrenamiento: 0.5401, Precisión de Entrenamiento: 0.8188\n",
      "Val loss: 0.5409, Val acc: 0.8142\n",
      "0.003125\n",
      "Epoch 154/400\n",
      "Iteración 53891 - Lote 35/352 - Pérdida de Entrenamiento: 0.5265, Precisión de Entrenamiento: 0.8241\n",
      "Iteración 53926 - Lote 70/352 - Pérdida de Entrenamiento: 0.5268, Precisión de Entrenamiento: 0.8252\n",
      "Iteración 53961 - Lote 105/352 - Pérdida de Entrenamiento: 0.5389, Precisión de Entrenamiento: 0.8212\n",
      "Iteración 53996 - Lote 140/352 - Pérdida de Entrenamiento: 0.5409, Precisión de Entrenamiento: 0.8193\n",
      "Iteración 54031 - Lote 175/352 - Pérdida de Entrenamiento: 0.5423, Precisión de Entrenamiento: 0.8183\n",
      "Iteración 54066 - Lote 210/352 - Pérdida de Entrenamiento: 0.5407, Precisión de Entrenamiento: 0.8189\n",
      "Iteración 54101 - Lote 245/352 - Pérdida de Entrenamiento: 0.5420, Precisión de Entrenamiento: 0.8184\n",
      "Iteración 54136 - Lote 280/352 - Pérdida de Entrenamiento: 0.5439, Precisión de Entrenamiento: 0.8177\n",
      "Iteración 54171 - Lote 315/352 - Pérdida de Entrenamiento: 0.5449, Precisión de Entrenamiento: 0.8176\n",
      "Iteración 54206 - Lote 350/352 - Pérdida de Entrenamiento: 0.5447, Precisión de Entrenamiento: 0.8179\n",
      "Val loss: 0.5420, Val acc: 0.8160\n",
      "0.003125\n",
      "Epoch 155/400\n",
      "Iteración 54243 - Lote 35/352 - Pérdida de Entrenamiento: 0.5262, Precisión de Entrenamiento: 0.8228\n",
      "Iteración 54278 - Lote 70/352 - Pérdida de Entrenamiento: 0.5359, Precisión de Entrenamiento: 0.8211\n",
      "Iteración 54313 - Lote 105/352 - Pérdida de Entrenamiento: 0.5401, Precisión de Entrenamiento: 0.8185\n",
      "Iteración 54348 - Lote 140/352 - Pérdida de Entrenamiento: 0.5397, Precisión de Entrenamiento: 0.8182\n",
      "Iteración 54383 - Lote 175/352 - Pérdida de Entrenamiento: 0.5360, Precisión de Entrenamiento: 0.8189\n",
      "Iteración 54418 - Lote 210/352 - Pérdida de Entrenamiento: 0.5366, Precisión de Entrenamiento: 0.8189\n",
      "Iteración 54453 - Lote 245/352 - Pérdida de Entrenamiento: 0.5365, Precisión de Entrenamiento: 0.8194\n",
      "Iteración 54488 - Lote 280/352 - Pérdida de Entrenamiento: 0.5391, Precisión de Entrenamiento: 0.8193\n",
      "Iteración 54523 - Lote 315/352 - Pérdida de Entrenamiento: 0.5418, Precisión de Entrenamiento: 0.8184\n",
      "Iteración 54558 - Lote 350/352 - Pérdida de Entrenamiento: 0.5396, Precisión de Entrenamiento: 0.8196\n",
      "Val loss: 0.5523, Val acc: 0.8146\n",
      "Gradientes para features.0.0.weight: min=-0.16842497885227203, max=0.10285460203886032, mean=-0.004537629429250956, std=0.036964915692806244\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.1020650565624237, max=0.08919444680213928, mean=-8.072299351624679e-06, std=0.0025283365976065397\n",
      "Gradientes para classifier.1.weight: min=-0.009836951270699501, max=0.00798166636377573, mean=6.984919153490976e-12, std=0.001365732285194099\n",
      "0.003125\n",
      "Epoch 156/400\n",
      "Iteración 54595 - Lote 35/352 - Pérdida de Entrenamiento: 0.5401, Precisión de Entrenamiento: 0.8214\n",
      "Iteración 54630 - Lote 70/352 - Pérdida de Entrenamiento: 0.5452, Precisión de Entrenamiento: 0.8199\n",
      "Iteración 54665 - Lote 105/352 - Pérdida de Entrenamiento: 0.5478, Precisión de Entrenamiento: 0.8187\n",
      "Iteración 54700 - Lote 140/352 - Pérdida de Entrenamiento: 0.5458, Precisión de Entrenamiento: 0.8190\n",
      "Iteración 54735 - Lote 175/352 - Pérdida de Entrenamiento: 0.5414, Precisión de Entrenamiento: 0.8194\n",
      "Iteración 54770 - Lote 210/352 - Pérdida de Entrenamiento: 0.5373, Precisión de Entrenamiento: 0.8202\n",
      "Iteración 54805 - Lote 245/352 - Pérdida de Entrenamiento: 0.5375, Precisión de Entrenamiento: 0.8197\n",
      "Iteración 54840 - Lote 280/352 - Pérdida de Entrenamiento: 0.5362, Precisión de Entrenamiento: 0.8216\n",
      "Iteración 54875 - Lote 315/352 - Pérdida de Entrenamiento: 0.5345, Precisión de Entrenamiento: 0.8221\n",
      "Iteración 54910 - Lote 350/352 - Pérdida de Entrenamiento: 0.5365, Precisión de Entrenamiento: 0.8211\n",
      "Val loss: 0.5469, Val acc: 0.8156\n",
      "0.003125\n",
      "Epoch 157/400\n",
      "Iteración 54947 - Lote 35/352 - Pérdida de Entrenamiento: 0.5099, Precisión de Entrenamiento: 0.8335\n",
      "Iteración 54982 - Lote 70/352 - Pérdida de Entrenamiento: 0.5179, Precisión de Entrenamiento: 0.8271\n",
      "Iteración 55017 - Lote 105/352 - Pérdida de Entrenamiento: 0.5240, Precisión de Entrenamiento: 0.8259\n",
      "Iteración 55052 - Lote 140/352 - Pérdida de Entrenamiento: 0.5279, Precisión de Entrenamiento: 0.8252\n",
      "Iteración 55087 - Lote 175/352 - Pérdida de Entrenamiento: 0.5304, Precisión de Entrenamiento: 0.8242\n",
      "Iteración 55122 - Lote 210/352 - Pérdida de Entrenamiento: 0.5323, Precisión de Entrenamiento: 0.8237\n",
      "Iteración 55157 - Lote 245/352 - Pérdida de Entrenamiento: 0.5328, Precisión de Entrenamiento: 0.8230\n",
      "Iteración 55192 - Lote 280/352 - Pérdida de Entrenamiento: 0.5318, Precisión de Entrenamiento: 0.8227\n",
      "Iteración 55227 - Lote 315/352 - Pérdida de Entrenamiento: 0.5359, Precisión de Entrenamiento: 0.8209\n",
      "Iteración 55262 - Lote 350/352 - Pérdida de Entrenamiento: 0.5366, Precisión de Entrenamiento: 0.8210\n",
      "Val loss: 0.5556, Val acc: 0.8130\n",
      "0.003125\n",
      "Epoch 158/400\n",
      "Iteración 55299 - Lote 35/352 - Pérdida de Entrenamiento: 0.5342, Precisión de Entrenamiento: 0.8217\n",
      "Iteración 55334 - Lote 70/352 - Pérdida de Entrenamiento: 0.5331, Precisión de Entrenamiento: 0.8208\n",
      "Iteración 55369 - Lote 105/352 - Pérdida de Entrenamiento: 0.5304, Precisión de Entrenamiento: 0.8205\n",
      "Iteración 55404 - Lote 140/352 - Pérdida de Entrenamiento: 0.5257, Precisión de Entrenamiento: 0.8225\n",
      "Iteración 55439 - Lote 175/352 - Pérdida de Entrenamiento: 0.5286, Precisión de Entrenamiento: 0.8221\n",
      "Iteración 55474 - Lote 210/352 - Pérdida de Entrenamiento: 0.5305, Precisión de Entrenamiento: 0.8223\n",
      "Iteración 55509 - Lote 245/352 - Pérdida de Entrenamiento: 0.5314, Precisión de Entrenamiento: 0.8216\n",
      "Iteración 55544 - Lote 280/352 - Pérdida de Entrenamiento: 0.5334, Precisión de Entrenamiento: 0.8207\n",
      "Iteración 55579 - Lote 315/352 - Pérdida de Entrenamiento: 0.5329, Precisión de Entrenamiento: 0.8214\n",
      "Iteración 55614 - Lote 350/352 - Pérdida de Entrenamiento: 0.5314, Precisión de Entrenamiento: 0.8223\n",
      "Val loss: 0.5284, Val acc: 0.8152\n",
      "0.0015625\n",
      "Epoch 159/400\n",
      "Iteración 55651 - Lote 35/352 - Pérdida de Entrenamiento: 0.4917, Precisión de Entrenamiento: 0.8366\n",
      "Iteración 55686 - Lote 70/352 - Pérdida de Entrenamiento: 0.4965, Precisión de Entrenamiento: 0.8368\n",
      "Iteración 55721 - Lote 105/352 - Pérdida de Entrenamiento: 0.5029, Precisión de Entrenamiento: 0.8335\n",
      "Iteración 55756 - Lote 140/352 - Pérdida de Entrenamiento: 0.5023, Precisión de Entrenamiento: 0.8323\n",
      "Iteración 55791 - Lote 175/352 - Pérdida de Entrenamiento: 0.4971, Precisión de Entrenamiento: 0.8351\n",
      "Iteración 55826 - Lote 210/352 - Pérdida de Entrenamiento: 0.4968, Precisión de Entrenamiento: 0.8348\n",
      "Iteración 55861 - Lote 245/352 - Pérdida de Entrenamiento: 0.4987, Precisión de Entrenamiento: 0.8346\n",
      "Iteración 55896 - Lote 280/352 - Pérdida de Entrenamiento: 0.5007, Precisión de Entrenamiento: 0.8340\n",
      "Iteración 55931 - Lote 315/352 - Pérdida de Entrenamiento: 0.5013, Precisión de Entrenamiento: 0.8338\n",
      "Iteración 55966 - Lote 350/352 - Pérdida de Entrenamiento: 0.5024, Precisión de Entrenamiento: 0.8336\n",
      "Val loss: 0.5165, Val acc: 0.8288\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_159.pth\n",
      "Checkpoint del mejor modelo guardado en la época 159\n",
      "0.0015625\n",
      "Epoch 160/400\n",
      "Iteración 56003 - Lote 35/352 - Pérdida de Entrenamiento: 0.4801, Precisión de Entrenamiento: 0.8404\n",
      "Iteración 56038 - Lote 70/352 - Pérdida de Entrenamiento: 0.4855, Precisión de Entrenamiento: 0.8364\n",
      "Iteración 56073 - Lote 105/352 - Pérdida de Entrenamiento: 0.4894, Precisión de Entrenamiento: 0.8361\n",
      "Iteración 56108 - Lote 140/352 - Pérdida de Entrenamiento: 0.4901, Precisión de Entrenamiento: 0.8353\n",
      "Iteración 56143 - Lote 175/352 - Pérdida de Entrenamiento: 0.4969, Precisión de Entrenamiento: 0.8345\n",
      "Iteración 56178 - Lote 210/352 - Pérdida de Entrenamiento: 0.4959, Precisión de Entrenamiento: 0.8338\n",
      "Iteración 56213 - Lote 245/352 - Pérdida de Entrenamiento: 0.4967, Precisión de Entrenamiento: 0.8336\n",
      "Iteración 56248 - Lote 280/352 - Pérdida de Entrenamiento: 0.4994, Precisión de Entrenamiento: 0.8325\n",
      "Iteración 56283 - Lote 315/352 - Pérdida de Entrenamiento: 0.5017, Precisión de Entrenamiento: 0.8316\n",
      "Iteración 56318 - Lote 350/352 - Pérdida de Entrenamiento: 0.5020, Precisión de Entrenamiento: 0.8319\n",
      "Val loss: 0.5219, Val acc: 0.8242\n",
      "Gradientes para features.0.0.weight: min=-0.12993308901786804, max=0.19223034381866455, mean=0.0013653360074386, std=0.042680028825998306\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.08158442378044128, max=0.08456345647573471, mean=1.1717830602719914e-05, std=0.002681160345673561\n",
      "Gradientes para classifier.1.weight: min=-0.012283273041248322, max=0.014702307991683483, mean=1.3969838306981952e-11, std=0.0019456855952739716\n",
      "0.0015625\n",
      "Epoch 161/400\n",
      "Iteración 56355 - Lote 35/352 - Pérdida de Entrenamiento: 0.4968, Precisión de Entrenamiento: 0.8344\n",
      "Iteración 56390 - Lote 70/352 - Pérdida de Entrenamiento: 0.4977, Precisión de Entrenamiento: 0.8343\n",
      "Iteración 56425 - Lote 105/352 - Pérdida de Entrenamiento: 0.4939, Precisión de Entrenamiento: 0.8336\n",
      "Iteración 56460 - Lote 140/352 - Pérdida de Entrenamiento: 0.4968, Precisión de Entrenamiento: 0.8316\n",
      "Iteración 56495 - Lote 175/352 - Pérdida de Entrenamiento: 0.5008, Precisión de Entrenamiento: 0.8308\n",
      "Iteración 56530 - Lote 210/352 - Pérdida de Entrenamiento: 0.4967, Precisión de Entrenamiento: 0.8317\n",
      "Iteración 56565 - Lote 245/352 - Pérdida de Entrenamiento: 0.4952, Precisión de Entrenamiento: 0.8324\n",
      "Iteración 56600 - Lote 280/352 - Pérdida de Entrenamiento: 0.4967, Precisión de Entrenamiento: 0.8320\n",
      "Iteración 56635 - Lote 315/352 - Pérdida de Entrenamiento: 0.4973, Precisión de Entrenamiento: 0.8320\n",
      "Iteración 56670 - Lote 350/352 - Pérdida de Entrenamiento: 0.4972, Precisión de Entrenamiento: 0.8327\n",
      "Val loss: 0.5041, Val acc: 0.8306\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_161.pth\n",
      "Checkpoint del mejor modelo guardado en la época 161\n",
      "0.0015625\n",
      "Epoch 162/400\n",
      "Iteración 56707 - Lote 35/352 - Pérdida de Entrenamiento: 0.4793, Precisión de Entrenamiento: 0.8337\n",
      "Iteración 56742 - Lote 70/352 - Pérdida de Entrenamiento: 0.4876, Precisión de Entrenamiento: 0.8311\n",
      "Iteración 56777 - Lote 105/352 - Pérdida de Entrenamiento: 0.4903, Precisión de Entrenamiento: 0.8315\n",
      "Iteración 56812 - Lote 140/352 - Pérdida de Entrenamiento: 0.4903, Precisión de Entrenamiento: 0.8332\n",
      "Iteración 56847 - Lote 175/352 - Pérdida de Entrenamiento: 0.4906, Precisión de Entrenamiento: 0.8322\n",
      "Iteración 56882 - Lote 210/352 - Pérdida de Entrenamiento: 0.4942, Precisión de Entrenamiento: 0.8317\n",
      "Iteración 56917 - Lote 245/352 - Pérdida de Entrenamiento: 0.4955, Precisión de Entrenamiento: 0.8322\n",
      "Iteración 56952 - Lote 280/352 - Pérdida de Entrenamiento: 0.4967, Precisión de Entrenamiento: 0.8321\n",
      "Iteración 56987 - Lote 315/352 - Pérdida de Entrenamiento: 0.4959, Precisión de Entrenamiento: 0.8325\n",
      "Iteración 57022 - Lote 350/352 - Pérdida de Entrenamiento: 0.4958, Precisión de Entrenamiento: 0.8320\n",
      "Val loss: 0.5026, Val acc: 0.8304\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_162.pth\n",
      "Checkpoint del mejor modelo guardado en la época 162\n",
      "0.0015625\n",
      "Epoch 163/400\n",
      "Iteración 57059 - Lote 35/352 - Pérdida de Entrenamiento: 0.4963, Precisión de Entrenamiento: 0.8326\n",
      "Iteración 57094 - Lote 70/352 - Pérdida de Entrenamiento: 0.4836, Precisión de Entrenamiento: 0.8408\n",
      "Iteración 57129 - Lote 105/352 - Pérdida de Entrenamiento: 0.4837, Precisión de Entrenamiento: 0.8397\n",
      "Iteración 57164 - Lote 140/352 - Pérdida de Entrenamiento: 0.4815, Precisión de Entrenamiento: 0.8405\n",
      "Iteración 57199 - Lote 175/352 - Pérdida de Entrenamiento: 0.4821, Precisión de Entrenamiento: 0.8394\n",
      "Iteración 57234 - Lote 210/352 - Pérdida de Entrenamiento: 0.4859, Precisión de Entrenamiento: 0.8378\n",
      "Iteración 57269 - Lote 245/352 - Pérdida de Entrenamiento: 0.4872, Precisión de Entrenamiento: 0.8373\n",
      "Iteración 57304 - Lote 280/352 - Pérdida de Entrenamiento: 0.4884, Precisión de Entrenamiento: 0.8370\n",
      "Iteración 57339 - Lote 315/352 - Pérdida de Entrenamiento: 0.4899, Precisión de Entrenamiento: 0.8362\n",
      "Iteración 57374 - Lote 350/352 - Pérdida de Entrenamiento: 0.4919, Precisión de Entrenamiento: 0.8354\n",
      "Val loss: 0.5069, Val acc: 0.8238\n",
      "0.0015625\n",
      "Epoch 164/400\n",
      "Iteración 57411 - Lote 35/352 - Pérdida de Entrenamiento: 0.4991, Precisión de Entrenamiento: 0.8281\n",
      "Iteración 57446 - Lote 70/352 - Pérdida de Entrenamiento: 0.4938, Precisión de Entrenamiento: 0.8317\n",
      "Iteración 57481 - Lote 105/352 - Pérdida de Entrenamiento: 0.4937, Precisión de Entrenamiento: 0.8325\n",
      "Iteración 57516 - Lote 140/352 - Pérdida de Entrenamiento: 0.5004, Precisión de Entrenamiento: 0.8321\n",
      "Iteración 57551 - Lote 175/352 - Pérdida de Entrenamiento: 0.4995, Precisión de Entrenamiento: 0.8334\n",
      "Iteración 57586 - Lote 210/352 - Pérdida de Entrenamiento: 0.5002, Precisión de Entrenamiento: 0.8337\n",
      "Iteración 57621 - Lote 245/352 - Pérdida de Entrenamiento: 0.4984, Precisión de Entrenamiento: 0.8342\n",
      "Iteración 57656 - Lote 280/352 - Pérdida de Entrenamiento: 0.4982, Precisión de Entrenamiento: 0.8340\n",
      "Iteración 57691 - Lote 315/352 - Pérdida de Entrenamiento: 0.4975, Precisión de Entrenamiento: 0.8345\n",
      "Iteración 57726 - Lote 350/352 - Pérdida de Entrenamiento: 0.4943, Precisión de Entrenamiento: 0.8347\n",
      "Val loss: 0.5122, Val acc: 0.8260\n",
      "0.0015625\n",
      "Epoch 165/400\n",
      "Iteración 57763 - Lote 35/352 - Pérdida de Entrenamiento: 0.4770, Precisión de Entrenamiento: 0.8446\n",
      "Iteración 57798 - Lote 70/352 - Pérdida de Entrenamiento: 0.4913, Precisión de Entrenamiento: 0.8394\n",
      "Iteración 57833 - Lote 105/352 - Pérdida de Entrenamiento: 0.4840, Precisión de Entrenamiento: 0.8391\n",
      "Iteración 57868 - Lote 140/352 - Pérdida de Entrenamiento: 0.4873, Precisión de Entrenamiento: 0.8371\n",
      "Iteración 57903 - Lote 175/352 - Pérdida de Entrenamiento: 0.4886, Precisión de Entrenamiento: 0.8373\n",
      "Iteración 57938 - Lote 210/352 - Pérdida de Entrenamiento: 0.4904, Precisión de Entrenamiento: 0.8366\n",
      "Iteración 57973 - Lote 245/352 - Pérdida de Entrenamiento: 0.4901, Precisión de Entrenamiento: 0.8363\n",
      "Iteración 58008 - Lote 280/352 - Pérdida de Entrenamiento: 0.4919, Precisión de Entrenamiento: 0.8349\n",
      "Iteración 58043 - Lote 315/352 - Pérdida de Entrenamiento: 0.4937, Precisión de Entrenamiento: 0.8344\n",
      "Iteración 58078 - Lote 350/352 - Pérdida de Entrenamiento: 0.4936, Precisión de Entrenamiento: 0.8343\n",
      "Val loss: 0.5052, Val acc: 0.8284\n",
      "Gradientes para features.0.0.weight: min=-0.2038213610649109, max=0.1449967324733734, mean=-0.005631009582430124, std=0.042549047619104385\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.1952672302722931, max=0.0922727882862091, mean=-6.486051006504567e-06, std=0.0037330547347664833\n",
      "Gradientes para classifier.1.weight: min=-0.019106121733784676, max=0.011720571666955948, mean=1.8626451075975936e-11, std=0.0019507966935634613\n",
      "0.0015625\n",
      "Epoch 166/400\n",
      "Iteración 58115 - Lote 35/352 - Pérdida de Entrenamiento: 0.5028, Precisión de Entrenamiento: 0.8310\n",
      "Iteración 58150 - Lote 70/352 - Pérdida de Entrenamiento: 0.4919, Precisión de Entrenamiento: 0.8325\n",
      "Iteración 58185 - Lote 105/352 - Pérdida de Entrenamiento: 0.4945, Precisión de Entrenamiento: 0.8330\n",
      "Iteración 58220 - Lote 140/352 - Pérdida de Entrenamiento: 0.4928, Precisión de Entrenamiento: 0.8339\n",
      "Iteración 58255 - Lote 175/352 - Pérdida de Entrenamiento: 0.4907, Precisión de Entrenamiento: 0.8346\n",
      "Iteración 58290 - Lote 210/352 - Pérdida de Entrenamiento: 0.4908, Precisión de Entrenamiento: 0.8349\n",
      "Iteración 58325 - Lote 245/352 - Pérdida de Entrenamiento: 0.4932, Precisión de Entrenamiento: 0.8339\n",
      "Iteración 58360 - Lote 280/352 - Pérdida de Entrenamiento: 0.4908, Precisión de Entrenamiento: 0.8354\n",
      "Iteración 58395 - Lote 315/352 - Pérdida de Entrenamiento: 0.4913, Precisión de Entrenamiento: 0.8351\n",
      "Iteración 58430 - Lote 350/352 - Pérdida de Entrenamiento: 0.4929, Precisión de Entrenamiento: 0.8345\n",
      "Val loss: 0.5003, Val acc: 0.8270\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_166.pth\n",
      "Checkpoint del mejor modelo guardado en la época 166\n",
      "0.0015625\n",
      "Epoch 167/400\n",
      "Iteración 58467 - Lote 35/352 - Pérdida de Entrenamiento: 0.4778, Precisión de Entrenamiento: 0.8400\n",
      "Iteración 58502 - Lote 70/352 - Pérdida de Entrenamiento: 0.4941, Precisión de Entrenamiento: 0.8373\n",
      "Iteración 58537 - Lote 105/352 - Pérdida de Entrenamiento: 0.4931, Precisión de Entrenamiento: 0.8376\n",
      "Iteración 58572 - Lote 140/352 - Pérdida de Entrenamiento: 0.4914, Precisión de Entrenamiento: 0.8374\n",
      "Iteración 58607 - Lote 175/352 - Pérdida de Entrenamiento: 0.4951, Precisión de Entrenamiento: 0.8359\n",
      "Iteración 58642 - Lote 210/352 - Pérdida de Entrenamiento: 0.4941, Precisión de Entrenamiento: 0.8357\n",
      "Iteración 58677 - Lote 245/352 - Pérdida de Entrenamiento: 0.4971, Precisión de Entrenamiento: 0.8347\n",
      "Iteración 58712 - Lote 280/352 - Pérdida de Entrenamiento: 0.4993, Precisión de Entrenamiento: 0.8338\n",
      "Iteración 58747 - Lote 315/352 - Pérdida de Entrenamiento: 0.4972, Precisión de Entrenamiento: 0.8344\n",
      "Iteración 58782 - Lote 350/352 - Pérdida de Entrenamiento: 0.4950, Precisión de Entrenamiento: 0.8346\n",
      "Val loss: 0.5078, Val acc: 0.8314\n",
      "0.0015625\n",
      "Epoch 168/400\n",
      "Iteración 58819 - Lote 35/352 - Pérdida de Entrenamiento: 0.4968, Precisión de Entrenamiento: 0.8297\n",
      "Iteración 58854 - Lote 70/352 - Pérdida de Entrenamiento: 0.4986, Precisión de Entrenamiento: 0.8292\n",
      "Iteración 58889 - Lote 105/352 - Pérdida de Entrenamiento: 0.4984, Precisión de Entrenamiento: 0.8283\n",
      "Iteración 58924 - Lote 140/352 - Pérdida de Entrenamiento: 0.4883, Precisión de Entrenamiento: 0.8329\n",
      "Iteración 58959 - Lote 175/352 - Pérdida de Entrenamiento: 0.4868, Precisión de Entrenamiento: 0.8346\n",
      "Iteración 58994 - Lote 210/352 - Pérdida de Entrenamiento: 0.4883, Precisión de Entrenamiento: 0.8345\n",
      "Iteración 59029 - Lote 245/352 - Pérdida de Entrenamiento: 0.4891, Precisión de Entrenamiento: 0.8352\n",
      "Iteración 59064 - Lote 280/352 - Pérdida de Entrenamiento: 0.4890, Precisión de Entrenamiento: 0.8350\n",
      "Iteración 59099 - Lote 315/352 - Pérdida de Entrenamiento: 0.4876, Precisión de Entrenamiento: 0.8357\n",
      "Iteración 59134 - Lote 350/352 - Pérdida de Entrenamiento: 0.4891, Precisión de Entrenamiento: 0.8354\n",
      "Val loss: 0.5113, Val acc: 0.8288\n",
      "0.0015625\n",
      "Epoch 169/400\n",
      "Iteración 59171 - Lote 35/352 - Pérdida de Entrenamiento: 0.4932, Precisión de Entrenamiento: 0.8317\n",
      "Iteración 59206 - Lote 70/352 - Pérdida de Entrenamiento: 0.4977, Precisión de Entrenamiento: 0.8328\n",
      "Iteración 59241 - Lote 105/352 - Pérdida de Entrenamiento: 0.4952, Precisión de Entrenamiento: 0.8325\n",
      "Iteración 59276 - Lote 140/352 - Pérdida de Entrenamiento: 0.4943, Precisión de Entrenamiento: 0.8320\n",
      "Iteración 59311 - Lote 175/352 - Pérdida de Entrenamiento: 0.4947, Precisión de Entrenamiento: 0.8332\n",
      "Iteración 59346 - Lote 210/352 - Pérdida de Entrenamiento: 0.5003, Precisión de Entrenamiento: 0.8315\n",
      "Iteración 59381 - Lote 245/352 - Pérdida de Entrenamiento: 0.4983, Precisión de Entrenamiento: 0.8324\n",
      "Iteración 59416 - Lote 280/352 - Pérdida de Entrenamiento: 0.4969, Precisión de Entrenamiento: 0.8329\n",
      "Iteración 59451 - Lote 315/352 - Pérdida de Entrenamiento: 0.4958, Precisión de Entrenamiento: 0.8333\n",
      "Iteración 59486 - Lote 350/352 - Pérdida de Entrenamiento: 0.4953, Precisión de Entrenamiento: 0.8336\n",
      "Val loss: 0.5154, Val acc: 0.8236\n",
      "0.0015625\n",
      "Epoch 170/400\n",
      "Iteración 59523 - Lote 35/352 - Pérdida de Entrenamiento: 0.4880, Precisión de Entrenamiento: 0.8324\n",
      "Iteración 59558 - Lote 70/352 - Pérdida de Entrenamiento: 0.5090, Precisión de Entrenamiento: 0.8278\n",
      "Iteración 59593 - Lote 105/352 - Pérdida de Entrenamiento: 0.5026, Precisión de Entrenamiento: 0.8320\n",
      "Iteración 59628 - Lote 140/352 - Pérdida de Entrenamiento: 0.5024, Precisión de Entrenamiento: 0.8316\n",
      "Iteración 59663 - Lote 175/352 - Pérdida de Entrenamiento: 0.5016, Precisión de Entrenamiento: 0.8314\n",
      "Iteración 59698 - Lote 210/352 - Pérdida de Entrenamiento: 0.5013, Precisión de Entrenamiento: 0.8308\n",
      "Iteración 59733 - Lote 245/352 - Pérdida de Entrenamiento: 0.5014, Precisión de Entrenamiento: 0.8312\n",
      "Iteración 59768 - Lote 280/352 - Pérdida de Entrenamiento: 0.4981, Precisión de Entrenamiento: 0.8328\n",
      "Iteración 59803 - Lote 315/352 - Pérdida de Entrenamiento: 0.4986, Precisión de Entrenamiento: 0.8324\n",
      "Iteración 59838 - Lote 350/352 - Pérdida de Entrenamiento: 0.4996, Precisión de Entrenamiento: 0.8322\n",
      "Val loss: 0.5200, Val acc: 0.8226\n",
      "Gradientes para features.0.0.weight: min=-0.20866748690605164, max=0.13862918317317963, mean=-0.007997197099030018, std=0.04419602453708649\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.07697971165180206, max=0.1695048213005066, mean=4.350921244622441e-06, std=0.0033448105677962303\n",
      "Gradientes para classifier.1.weight: min=-0.012844541110098362, max=0.012634174898266792, mean=2.7939676613963904e-11, std=0.0017803809605538845\n",
      "0.0015625\n",
      "Epoch 171/400\n",
      "Iteración 59875 - Lote 35/352 - Pérdida de Entrenamiento: 0.4886, Precisión de Entrenamiento: 0.8386\n",
      "Iteración 59910 - Lote 70/352 - Pérdida de Entrenamiento: 0.4895, Precisión de Entrenamiento: 0.8388\n",
      "Iteración 59945 - Lote 105/352 - Pérdida de Entrenamiento: 0.4921, Precisión de Entrenamiento: 0.8374\n",
      "Iteración 59980 - Lote 140/352 - Pérdida de Entrenamiento: 0.4887, Precisión de Entrenamiento: 0.8371\n",
      "Iteración 60015 - Lote 175/352 - Pérdida de Entrenamiento: 0.4880, Precisión de Entrenamiento: 0.8377\n",
      "Iteración 60050 - Lote 210/352 - Pérdida de Entrenamiento: 0.4904, Precisión de Entrenamiento: 0.8362\n",
      "Iteración 60085 - Lote 245/352 - Pérdida de Entrenamiento: 0.4930, Precisión de Entrenamiento: 0.8351\n",
      "Iteración 60120 - Lote 280/352 - Pérdida de Entrenamiento: 0.4945, Precisión de Entrenamiento: 0.8344\n",
      "Iteración 60155 - Lote 315/352 - Pérdida de Entrenamiento: 0.4929, Precisión de Entrenamiento: 0.8348\n",
      "Iteración 60190 - Lote 350/352 - Pérdida de Entrenamiento: 0.4920, Precisión de Entrenamiento: 0.8351\n",
      "Val loss: 0.4971, Val acc: 0.8282\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_171.pth\n",
      "Checkpoint del mejor modelo guardado en la época 171\n",
      "0.0015625\n",
      "Epoch 172/400\n",
      "Iteración 60227 - Lote 35/352 - Pérdida de Entrenamiento: 0.4809, Precisión de Entrenamiento: 0.8397\n",
      "Iteración 60262 - Lote 70/352 - Pérdida de Entrenamiento: 0.4895, Precisión de Entrenamiento: 0.8350\n",
      "Iteración 60297 - Lote 105/352 - Pérdida de Entrenamiento: 0.4884, Precisión de Entrenamiento: 0.8352\n",
      "Iteración 60332 - Lote 140/352 - Pérdida de Entrenamiento: 0.4966, Precisión de Entrenamiento: 0.8311\n",
      "Iteración 60367 - Lote 175/352 - Pérdida de Entrenamiento: 0.4939, Precisión de Entrenamiento: 0.8324\n",
      "Iteración 60402 - Lote 210/352 - Pérdida de Entrenamiento: 0.4920, Precisión de Entrenamiento: 0.8340\n",
      "Iteración 60437 - Lote 245/352 - Pérdida de Entrenamiento: 0.4936, Precisión de Entrenamiento: 0.8331\n",
      "Iteración 60472 - Lote 280/352 - Pérdida de Entrenamiento: 0.4897, Precisión de Entrenamiento: 0.8344\n",
      "Iteración 60507 - Lote 315/352 - Pérdida de Entrenamiento: 0.4898, Precisión de Entrenamiento: 0.8346\n",
      "Iteración 60542 - Lote 350/352 - Pérdida de Entrenamiento: 0.4913, Precisión de Entrenamiento: 0.8342\n",
      "Val loss: 0.5030, Val acc: 0.8290\n",
      "0.0015625\n",
      "Epoch 173/400\n",
      "Iteración 60579 - Lote 35/352 - Pérdida de Entrenamiento: 0.4948, Precisión de Entrenamiento: 0.8339\n",
      "Iteración 60614 - Lote 70/352 - Pérdida de Entrenamiento: 0.4917, Precisión de Entrenamiento: 0.8360\n",
      "Iteración 60649 - Lote 105/352 - Pérdida de Entrenamiento: 0.4923, Precisión de Entrenamiento: 0.8347\n",
      "Iteración 60684 - Lote 140/352 - Pérdida de Entrenamiento: 0.4938, Precisión de Entrenamiento: 0.8349\n",
      "Iteración 60719 - Lote 175/352 - Pérdida de Entrenamiento: 0.4926, Precisión de Entrenamiento: 0.8354\n",
      "Iteración 60754 - Lote 210/352 - Pérdida de Entrenamiento: 0.4906, Precisión de Entrenamiento: 0.8362\n",
      "Iteración 60789 - Lote 245/352 - Pérdida de Entrenamiento: 0.4887, Precisión de Entrenamiento: 0.8371\n",
      "Iteración 60824 - Lote 280/352 - Pérdida de Entrenamiento: 0.4895, Precisión de Entrenamiento: 0.8365\n",
      "Iteración 60859 - Lote 315/352 - Pérdida de Entrenamiento: 0.4904, Precisión de Entrenamiento: 0.8359\n",
      "Iteración 60894 - Lote 350/352 - Pérdida de Entrenamiento: 0.4901, Precisión de Entrenamiento: 0.8357\n",
      "Val loss: 0.5062, Val acc: 0.8252\n",
      "0.0015625\n",
      "Epoch 174/400\n",
      "Iteración 60931 - Lote 35/352 - Pérdida de Entrenamiento: 0.5177, Precisión de Entrenamiento: 0.8243\n",
      "Iteración 60966 - Lote 70/352 - Pérdida de Entrenamiento: 0.5009, Precisión de Entrenamiento: 0.8320\n",
      "Iteración 61001 - Lote 105/352 - Pérdida de Entrenamiento: 0.5021, Precisión de Entrenamiento: 0.8313\n",
      "Iteración 61036 - Lote 140/352 - Pérdida de Entrenamiento: 0.4964, Precisión de Entrenamiento: 0.8334\n",
      "Iteración 61071 - Lote 175/352 - Pérdida de Entrenamiento: 0.4931, Precisión de Entrenamiento: 0.8352\n",
      "Iteración 61106 - Lote 210/352 - Pérdida de Entrenamiento: 0.4952, Precisión de Entrenamiento: 0.8340\n",
      "Iteración 61141 - Lote 245/352 - Pérdida de Entrenamiento: 0.4941, Precisión de Entrenamiento: 0.8345\n",
      "Iteración 61176 - Lote 280/352 - Pérdida de Entrenamiento: 0.4925, Precisión de Entrenamiento: 0.8354\n",
      "Iteración 61211 - Lote 315/352 - Pérdida de Entrenamiento: 0.4900, Precisión de Entrenamiento: 0.8358\n",
      "Iteración 61246 - Lote 350/352 - Pérdida de Entrenamiento: 0.4908, Precisión de Entrenamiento: 0.8354\n",
      "Val loss: 0.5049, Val acc: 0.8266\n",
      "0.0015625\n",
      "Epoch 175/400\n",
      "Iteración 61283 - Lote 35/352 - Pérdida de Entrenamiento: 0.4737, Precisión de Entrenamiento: 0.8422\n",
      "Iteración 61318 - Lote 70/352 - Pérdida de Entrenamiento: 0.4870, Precisión de Entrenamiento: 0.8390\n",
      "Iteración 61353 - Lote 105/352 - Pérdida de Entrenamiento: 0.4828, Precisión de Entrenamiento: 0.8382\n",
      "Iteración 61388 - Lote 140/352 - Pérdida de Entrenamiento: 0.4908, Precisión de Entrenamiento: 0.8358\n",
      "Iteración 61423 - Lote 175/352 - Pérdida de Entrenamiento: 0.4926, Precisión de Entrenamiento: 0.8361\n",
      "Iteración 61458 - Lote 210/352 - Pérdida de Entrenamiento: 0.4899, Precisión de Entrenamiento: 0.8372\n",
      "Iteración 61493 - Lote 245/352 - Pérdida de Entrenamiento: 0.4928, Precisión de Entrenamiento: 0.8356\n",
      "Iteración 61528 - Lote 280/352 - Pérdida de Entrenamiento: 0.4923, Precisión de Entrenamiento: 0.8353\n",
      "Iteración 61563 - Lote 315/352 - Pérdida de Entrenamiento: 0.4916, Precisión de Entrenamiento: 0.8357\n",
      "Iteración 61598 - Lote 350/352 - Pérdida de Entrenamiento: 0.4918, Precisión de Entrenamiento: 0.8357\n",
      "Val loss: 0.5017, Val acc: 0.8272\n",
      "Gradientes para features.0.0.weight: min=-0.16956576704978943, max=0.12691548466682434, mean=-0.002023084321990609, std=0.043589040637016296\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.16320328414440155, max=0.10172771662473679, mean=-3.280762257418246e-07, std=0.0039197723381221294\n",
      "Gradientes para classifier.1.weight: min=-0.01632225327193737, max=0.014444666914641857, mean=4.656612768993984e-12, std=0.002477115485817194\n",
      "0.0015625\n",
      "Epoch 176/400\n",
      "Iteración 61635 - Lote 35/352 - Pérdida de Entrenamiento: 0.4870, Precisión de Entrenamiento: 0.8344\n",
      "Iteración 61670 - Lote 70/352 - Pérdida de Entrenamiento: 0.4735, Precisión de Entrenamiento: 0.8419\n",
      "Iteración 61705 - Lote 105/352 - Pérdida de Entrenamiento: 0.4798, Precisión de Entrenamiento: 0.8414\n",
      "Iteración 61740 - Lote 140/352 - Pérdida de Entrenamiento: 0.4856, Precisión de Entrenamiento: 0.8390\n",
      "Iteración 61775 - Lote 175/352 - Pérdida de Entrenamiento: 0.4912, Precisión de Entrenamiento: 0.8361\n",
      "Iteración 61810 - Lote 210/352 - Pérdida de Entrenamiento: 0.4929, Precisión de Entrenamiento: 0.8358\n",
      "Iteración 61845 - Lote 245/352 - Pérdida de Entrenamiento: 0.4917, Precisión de Entrenamiento: 0.8350\n",
      "Iteración 61880 - Lote 280/352 - Pérdida de Entrenamiento: 0.4949, Precisión de Entrenamiento: 0.8343\n",
      "Iteración 61915 - Lote 315/352 - Pérdida de Entrenamiento: 0.4932, Precisión de Entrenamiento: 0.8341\n",
      "Iteración 61950 - Lote 350/352 - Pérdida de Entrenamiento: 0.4929, Precisión de Entrenamiento: 0.8344\n",
      "Val loss: 0.5183, Val acc: 0.8240\n",
      "0.0015625\n",
      "Epoch 177/400\n",
      "Iteración 61987 - Lote 35/352 - Pérdida de Entrenamiento: 0.4900, Precisión de Entrenamiento: 0.8350\n",
      "Iteración 62022 - Lote 70/352 - Pérdida de Entrenamiento: 0.4872, Precisión de Entrenamiento: 0.8321\n",
      "Iteración 62057 - Lote 105/352 - Pérdida de Entrenamiento: 0.4861, Precisión de Entrenamiento: 0.8325\n",
      "Iteración 62092 - Lote 140/352 - Pérdida de Entrenamiento: 0.4897, Precisión de Entrenamiento: 0.8329\n",
      "Iteración 62127 - Lote 175/352 - Pérdida de Entrenamiento: 0.4923, Precisión de Entrenamiento: 0.8326\n",
      "Iteración 62162 - Lote 210/352 - Pérdida de Entrenamiento: 0.4914, Precisión de Entrenamiento: 0.8338\n",
      "Iteración 62197 - Lote 245/352 - Pérdida de Entrenamiento: 0.4917, Precisión de Entrenamiento: 0.8340\n",
      "Iteración 62232 - Lote 280/352 - Pérdida de Entrenamiento: 0.4935, Precisión de Entrenamiento: 0.8330\n",
      "Iteración 62267 - Lote 315/352 - Pérdida de Entrenamiento: 0.4924, Precisión de Entrenamiento: 0.8338\n",
      "Iteración 62302 - Lote 350/352 - Pérdida de Entrenamiento: 0.4914, Precisión de Entrenamiento: 0.8339\n",
      "Val loss: 0.5017, Val acc: 0.8270\n",
      "0.0015625\n",
      "Epoch 178/400\n",
      "Iteración 62339 - Lote 35/352 - Pérdida de Entrenamiento: 0.5053, Precisión de Entrenamiento: 0.8306\n",
      "Iteración 62374 - Lote 70/352 - Pérdida de Entrenamiento: 0.4930, Precisión de Entrenamiento: 0.8350\n",
      "Iteración 62409 - Lote 105/352 - Pérdida de Entrenamiento: 0.4879, Precisión de Entrenamiento: 0.8362\n",
      "Iteración 62444 - Lote 140/352 - Pérdida de Entrenamiento: 0.4882, Precisión de Entrenamiento: 0.8358\n",
      "Iteración 62479 - Lote 175/352 - Pérdida de Entrenamiento: 0.4927, Precisión de Entrenamiento: 0.8340\n",
      "Iteración 62514 - Lote 210/352 - Pérdida de Entrenamiento: 0.4904, Precisión de Entrenamiento: 0.8351\n",
      "Iteración 62549 - Lote 245/352 - Pérdida de Entrenamiento: 0.4890, Precisión de Entrenamiento: 0.8360\n",
      "Iteración 62584 - Lote 280/352 - Pérdida de Entrenamiento: 0.4896, Precisión de Entrenamiento: 0.8353\n",
      "Iteración 62619 - Lote 315/352 - Pérdida de Entrenamiento: 0.4913, Precisión de Entrenamiento: 0.8342\n",
      "Iteración 62654 - Lote 350/352 - Pérdida de Entrenamiento: 0.4925, Precisión de Entrenamiento: 0.8341\n",
      "Val loss: 0.4989, Val acc: 0.8278\n",
      "0.0015625\n",
      "Epoch 179/400\n",
      "Iteración 62691 - Lote 35/352 - Pérdida de Entrenamiento: 0.4638, Precisión de Entrenamiento: 0.8411\n",
      "Iteración 62726 - Lote 70/352 - Pérdida de Entrenamiento: 0.4717, Precisión de Entrenamiento: 0.8367\n",
      "Iteración 62761 - Lote 105/352 - Pérdida de Entrenamiento: 0.4813, Precisión de Entrenamiento: 0.8356\n",
      "Iteración 62796 - Lote 140/352 - Pérdida de Entrenamiento: 0.4794, Precisión de Entrenamiento: 0.8361\n",
      "Iteración 62831 - Lote 175/352 - Pérdida de Entrenamiento: 0.4809, Precisión de Entrenamiento: 0.8362\n",
      "Iteración 62866 - Lote 210/352 - Pérdida de Entrenamiento: 0.4800, Precisión de Entrenamiento: 0.8366\n",
      "Iteración 62901 - Lote 245/352 - Pérdida de Entrenamiento: 0.4845, Precisión de Entrenamiento: 0.8346\n",
      "Iteración 62936 - Lote 280/352 - Pérdida de Entrenamiento: 0.4860, Precisión de Entrenamiento: 0.8341\n",
      "Iteración 62971 - Lote 315/352 - Pérdida de Entrenamiento: 0.4895, Precisión de Entrenamiento: 0.8331\n",
      "Iteración 63006 - Lote 350/352 - Pérdida de Entrenamiento: 0.4888, Precisión de Entrenamiento: 0.8337\n",
      "Val loss: 0.5118, Val acc: 0.8244\n",
      "0.0015625\n",
      "Epoch 180/400\n",
      "Iteración 63043 - Lote 35/352 - Pérdida de Entrenamiento: 0.4658, Precisión de Entrenamiento: 0.8467\n",
      "Iteración 63078 - Lote 70/352 - Pérdida de Entrenamiento: 0.4771, Precisión de Entrenamiento: 0.8408\n",
      "Iteración 63113 - Lote 105/352 - Pérdida de Entrenamiento: 0.4891, Precisión de Entrenamiento: 0.8364\n",
      "Iteración 63148 - Lote 140/352 - Pérdida de Entrenamiento: 0.4830, Precisión de Entrenamiento: 0.8389\n",
      "Iteración 63183 - Lote 175/352 - Pérdida de Entrenamiento: 0.4844, Precisión de Entrenamiento: 0.8379\n",
      "Iteración 63218 - Lote 210/352 - Pérdida de Entrenamiento: 0.4905, Precisión de Entrenamiento: 0.8348\n",
      "Iteración 63253 - Lote 245/352 - Pérdida de Entrenamiento: 0.4909, Precisión de Entrenamiento: 0.8349\n",
      "Iteración 63288 - Lote 280/352 - Pérdida de Entrenamiento: 0.4908, Precisión de Entrenamiento: 0.8340\n",
      "Iteración 63323 - Lote 315/352 - Pérdida de Entrenamiento: 0.4948, Precisión de Entrenamiento: 0.8333\n",
      "Iteración 63358 - Lote 350/352 - Pérdida de Entrenamiento: 0.4962, Precisión de Entrenamiento: 0.8333\n",
      "Val loss: 0.5113, Val acc: 0.8210\n",
      "Gradientes para features.0.0.weight: min=-0.17813056707382202, max=0.16352945566177368, mean=-0.0049098483286798, std=0.049942634999752045\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.10132639855146408, max=0.06873805075883865, mean=-8.695887117937673e-06, std=0.0029261927120387554\n",
      "Gradientes para classifier.1.weight: min=-0.010897445492446423, max=0.012744496576488018, mean=-1.8626451075975936e-11, std=0.0014599865535274148\n",
      "0.00078125\n",
      "Epoch 181/400\n",
      "Iteración 63395 - Lote 35/352 - Pérdida de Entrenamiento: 0.4634, Precisión de Entrenamiento: 0.8496\n",
      "Iteración 63430 - Lote 70/352 - Pérdida de Entrenamiento: 0.4726, Precisión de Entrenamiento: 0.8424\n",
      "Iteración 63465 - Lote 105/352 - Pérdida de Entrenamiento: 0.4708, Precisión de Entrenamiento: 0.8417\n",
      "Iteración 63500 - Lote 140/352 - Pérdida de Entrenamiento: 0.4644, Precisión de Entrenamiento: 0.8455\n",
      "Iteración 63535 - Lote 175/352 - Pérdida de Entrenamiento: 0.4640, Precisión de Entrenamiento: 0.8448\n",
      "Iteración 63570 - Lote 210/352 - Pérdida de Entrenamiento: 0.4651, Precisión de Entrenamiento: 0.8443\n",
      "Iteración 63605 - Lote 245/352 - Pérdida de Entrenamiento: 0.4653, Precisión de Entrenamiento: 0.8434\n",
      "Iteración 63640 - Lote 280/352 - Pérdida de Entrenamiento: 0.4697, Precisión de Entrenamiento: 0.8419\n",
      "Iteración 63675 - Lote 315/352 - Pérdida de Entrenamiento: 0.4738, Precisión de Entrenamiento: 0.8408\n",
      "Iteración 63710 - Lote 350/352 - Pérdida de Entrenamiento: 0.4718, Precisión de Entrenamiento: 0.8421\n",
      "Val loss: 0.4904, Val acc: 0.8306\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_181.pth\n",
      "Checkpoint del mejor modelo guardado en la época 181\n",
      "0.00078125\n",
      "Epoch 182/400\n",
      "Iteración 63747 - Lote 35/352 - Pérdida de Entrenamiento: 0.4665, Precisión de Entrenamiento: 0.8462\n",
      "Iteración 63782 - Lote 70/352 - Pérdida de Entrenamiento: 0.4698, Precisión de Entrenamiento: 0.8402\n",
      "Iteración 63817 - Lote 105/352 - Pérdida de Entrenamiento: 0.4732, Precisión de Entrenamiento: 0.8377\n",
      "Iteración 63852 - Lote 140/352 - Pérdida de Entrenamiento: 0.4695, Precisión de Entrenamiento: 0.8400\n",
      "Iteración 63887 - Lote 175/352 - Pérdida de Entrenamiento: 0.4698, Precisión de Entrenamiento: 0.8407\n",
      "Iteración 63922 - Lote 210/352 - Pérdida de Entrenamiento: 0.4694, Precisión de Entrenamiento: 0.8417\n",
      "Iteración 63957 - Lote 245/352 - Pérdida de Entrenamiento: 0.4686, Precisión de Entrenamiento: 0.8417\n",
      "Iteración 63992 - Lote 280/352 - Pérdida de Entrenamiento: 0.4642, Precisión de Entrenamiento: 0.8435\n",
      "Iteración 64027 - Lote 315/352 - Pérdida de Entrenamiento: 0.4634, Precisión de Entrenamiento: 0.8436\n",
      "Iteración 64062 - Lote 350/352 - Pérdida de Entrenamiento: 0.4626, Precisión de Entrenamiento: 0.8442\n",
      "Val loss: 0.4875, Val acc: 0.8358\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_182.pth\n",
      "Checkpoint del mejor modelo guardado en la época 182\n",
      "0.00078125\n",
      "Epoch 183/400\n",
      "Iteración 64099 - Lote 35/352 - Pérdida de Entrenamiento: 0.4714, Precisión de Entrenamiento: 0.8444\n",
      "Iteración 64134 - Lote 70/352 - Pérdida de Entrenamiento: 0.4710, Precisión de Entrenamiento: 0.8442\n",
      "Iteración 64169 - Lote 105/352 - Pérdida de Entrenamiento: 0.4726, Precisión de Entrenamiento: 0.8428\n",
      "Iteración 64204 - Lote 140/352 - Pérdida de Entrenamiento: 0.4682, Precisión de Entrenamiento: 0.8430\n",
      "Iteración 64239 - Lote 175/352 - Pérdida de Entrenamiento: 0.4704, Precisión de Entrenamiento: 0.8418\n",
      "Iteración 64274 - Lote 210/352 - Pérdida de Entrenamiento: 0.4693, Precisión de Entrenamiento: 0.8427\n",
      "Iteración 64309 - Lote 245/352 - Pérdida de Entrenamiento: 0.4682, Precisión de Entrenamiento: 0.8424\n",
      "Iteración 64344 - Lote 280/352 - Pérdida de Entrenamiento: 0.4686, Precisión de Entrenamiento: 0.8428\n",
      "Iteración 64379 - Lote 315/352 - Pérdida de Entrenamiento: 0.4667, Precisión de Entrenamiento: 0.8432\n",
      "Iteración 64414 - Lote 350/352 - Pérdida de Entrenamiento: 0.4650, Precisión de Entrenamiento: 0.8441\n",
      "Val loss: 0.4966, Val acc: 0.8284\n",
      "0.00078125\n",
      "Epoch 184/400\n",
      "Iteración 64451 - Lote 35/352 - Pérdida de Entrenamiento: 0.4465, Precisión de Entrenamiento: 0.8516\n",
      "Iteración 64486 - Lote 70/352 - Pérdida de Entrenamiento: 0.4599, Precisión de Entrenamiento: 0.8456\n",
      "Iteración 64521 - Lote 105/352 - Pérdida de Entrenamiento: 0.4524, Precisión de Entrenamiento: 0.8472\n",
      "Iteración 64556 - Lote 140/352 - Pérdida de Entrenamiento: 0.4534, Precisión de Entrenamiento: 0.8470\n",
      "Iteración 64591 - Lote 175/352 - Pérdida de Entrenamiento: 0.4572, Precisión de Entrenamiento: 0.8456\n",
      "Iteración 64626 - Lote 210/352 - Pérdida de Entrenamiento: 0.4569, Precisión de Entrenamiento: 0.8459\n",
      "Iteración 64661 - Lote 245/352 - Pérdida de Entrenamiento: 0.4548, Precisión de Entrenamiento: 0.8471\n",
      "Iteración 64696 - Lote 280/352 - Pérdida de Entrenamiento: 0.4578, Precisión de Entrenamiento: 0.8464\n",
      "Iteración 64731 - Lote 315/352 - Pérdida de Entrenamiento: 0.4612, Precisión de Entrenamiento: 0.8455\n",
      "Iteración 64766 - Lote 350/352 - Pérdida de Entrenamiento: 0.4627, Precisión de Entrenamiento: 0.8446\n",
      "Val loss: 0.4927, Val acc: 0.8322\n",
      "0.00078125\n",
      "Epoch 185/400\n",
      "Iteración 64803 - Lote 35/352 - Pérdida de Entrenamiento: 0.4421, Precisión de Entrenamiento: 0.8504\n",
      "Iteración 64838 - Lote 70/352 - Pérdida de Entrenamiento: 0.4573, Precisión de Entrenamiento: 0.8468\n",
      "Iteración 64873 - Lote 105/352 - Pérdida de Entrenamiento: 0.4567, Precisión de Entrenamiento: 0.8459\n",
      "Iteración 64908 - Lote 140/352 - Pérdida de Entrenamiento: 0.4566, Precisión de Entrenamiento: 0.8451\n",
      "Iteración 64943 - Lote 175/352 - Pérdida de Entrenamiento: 0.4577, Precisión de Entrenamiento: 0.8454\n",
      "Iteración 64978 - Lote 210/352 - Pérdida de Entrenamiento: 0.4573, Precisión de Entrenamiento: 0.8462\n",
      "Iteración 65013 - Lote 245/352 - Pérdida de Entrenamiento: 0.4576, Precisión de Entrenamiento: 0.8460\n",
      "Iteración 65048 - Lote 280/352 - Pérdida de Entrenamiento: 0.4571, Precisión de Entrenamiento: 0.8458\n",
      "Iteración 65083 - Lote 315/352 - Pérdida de Entrenamiento: 0.4585, Precisión de Entrenamiento: 0.8458\n",
      "Iteración 65118 - Lote 350/352 - Pérdida de Entrenamiento: 0.4600, Precisión de Entrenamiento: 0.8450\n",
      "Val loss: 0.4924, Val acc: 0.8306\n",
      "Gradientes para features.0.0.weight: min=-0.1454801857471466, max=0.12554025650024414, mean=-0.0006448489730246365, std=0.033968620002269745\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.09755279868841171, max=0.0648576095700264, mean=-1.1062296835007146e-05, std=0.002540531102567911\n",
      "Gradientes para classifier.1.weight: min=-0.01170251052826643, max=0.015645882114768028, mean=4.656612768993984e-12, std=0.0016337241977453232\n",
      "0.00078125\n",
      "Epoch 186/400\n",
      "Iteración 65155 - Lote 35/352 - Pérdida de Entrenamiento: 0.4444, Precisión de Entrenamiento: 0.8511\n",
      "Iteración 65190 - Lote 70/352 - Pérdida de Entrenamiento: 0.4497, Precisión de Entrenamiento: 0.8483\n",
      "Iteración 65225 - Lote 105/352 - Pérdida de Entrenamiento: 0.4546, Precisión de Entrenamiento: 0.8478\n",
      "Iteración 65260 - Lote 140/352 - Pérdida de Entrenamiento: 0.4633, Precisión de Entrenamiento: 0.8454\n",
      "Iteración 65295 - Lote 175/352 - Pérdida de Entrenamiento: 0.4616, Precisión de Entrenamiento: 0.8442\n",
      "Iteración 65330 - Lote 210/352 - Pérdida de Entrenamiento: 0.4615, Precisión de Entrenamiento: 0.8439\n",
      "Iteración 65365 - Lote 245/352 - Pérdida de Entrenamiento: 0.4613, Precisión de Entrenamiento: 0.8444\n",
      "Iteración 65400 - Lote 280/352 - Pérdida de Entrenamiento: 0.4627, Precisión de Entrenamiento: 0.8444\n",
      "Iteración 65435 - Lote 315/352 - Pérdida de Entrenamiento: 0.4629, Precisión de Entrenamiento: 0.8445\n",
      "Iteración 65470 - Lote 350/352 - Pérdida de Entrenamiento: 0.4611, Precisión de Entrenamiento: 0.8448\n",
      "Val loss: 0.4842, Val acc: 0.8354\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_186.pth\n",
      "Checkpoint del mejor modelo guardado en la época 186\n",
      "0.00078125\n",
      "Epoch 187/400\n",
      "Iteración 65507 - Lote 35/352 - Pérdida de Entrenamiento: 0.4572, Precisión de Entrenamiento: 0.8513\n",
      "Iteración 65542 - Lote 70/352 - Pérdida de Entrenamiento: 0.4634, Precisión de Entrenamiento: 0.8491\n",
      "Iteración 65577 - Lote 105/352 - Pérdida de Entrenamiento: 0.4706, Precisión de Entrenamiento: 0.8440\n",
      "Iteración 65612 - Lote 140/352 - Pérdida de Entrenamiento: 0.4666, Precisión de Entrenamiento: 0.8439\n",
      "Iteración 65647 - Lote 175/352 - Pérdida de Entrenamiento: 0.4680, Precisión de Entrenamiento: 0.8431\n",
      "Iteración 65682 - Lote 210/352 - Pérdida de Entrenamiento: 0.4670, Precisión de Entrenamiento: 0.8441\n",
      "Iteración 65717 - Lote 245/352 - Pérdida de Entrenamiento: 0.4653, Precisión de Entrenamiento: 0.8441\n",
      "Iteración 65752 - Lote 280/352 - Pérdida de Entrenamiento: 0.4675, Precisión de Entrenamiento: 0.8439\n",
      "Iteración 65787 - Lote 315/352 - Pérdida de Entrenamiento: 0.4653, Precisión de Entrenamiento: 0.8442\n",
      "Iteración 65822 - Lote 350/352 - Pérdida de Entrenamiento: 0.4665, Precisión de Entrenamiento: 0.8435\n",
      "Val loss: 0.4954, Val acc: 0.8282\n",
      "0.00078125\n",
      "Epoch 188/400\n",
      "Iteración 65859 - Lote 35/352 - Pérdida de Entrenamiento: 0.4607, Precisión de Entrenamiento: 0.8467\n",
      "Iteración 65894 - Lote 70/352 - Pérdida de Entrenamiento: 0.4428, Precisión de Entrenamiento: 0.8502\n",
      "Iteración 65929 - Lote 105/352 - Pérdida de Entrenamiento: 0.4449, Precisión de Entrenamiento: 0.8490\n",
      "Iteración 65964 - Lote 140/352 - Pérdida de Entrenamiento: 0.4505, Precisión de Entrenamiento: 0.8476\n",
      "Iteración 65999 - Lote 175/352 - Pérdida de Entrenamiento: 0.4512, Precisión de Entrenamiento: 0.8482\n",
      "Iteración 66034 - Lote 210/352 - Pérdida de Entrenamiento: 0.4541, Precisión de Entrenamiento: 0.8465\n",
      "Iteración 66069 - Lote 245/352 - Pérdida de Entrenamiento: 0.4582, Precisión de Entrenamiento: 0.8448\n",
      "Iteración 66104 - Lote 280/352 - Pérdida de Entrenamiento: 0.4603, Precisión de Entrenamiento: 0.8448\n",
      "Iteración 66139 - Lote 315/352 - Pérdida de Entrenamiento: 0.4596, Precisión de Entrenamiento: 0.8451\n",
      "Iteración 66174 - Lote 350/352 - Pérdida de Entrenamiento: 0.4620, Precisión de Entrenamiento: 0.8449\n",
      "Val loss: 0.4866, Val acc: 0.8316\n",
      "0.00078125\n",
      "Epoch 189/400\n",
      "Iteración 66211 - Lote 35/352 - Pérdida de Entrenamiento: 0.4602, Precisión de Entrenamiento: 0.8402\n",
      "Iteración 66246 - Lote 70/352 - Pérdida de Entrenamiento: 0.4597, Precisión de Entrenamiento: 0.8438\n",
      "Iteración 66281 - Lote 105/352 - Pérdida de Entrenamiento: 0.4617, Precisión de Entrenamiento: 0.8429\n",
      "Iteración 66316 - Lote 140/352 - Pérdida de Entrenamiento: 0.4633, Precisión de Entrenamiento: 0.8416\n",
      "Iteración 66351 - Lote 175/352 - Pérdida de Entrenamiento: 0.4631, Precisión de Entrenamiento: 0.8429\n",
      "Iteración 66386 - Lote 210/352 - Pérdida de Entrenamiento: 0.4622, Precisión de Entrenamiento: 0.8436\n",
      "Iteración 66421 - Lote 245/352 - Pérdida de Entrenamiento: 0.4663, Precisión de Entrenamiento: 0.8427\n",
      "Iteración 66456 - Lote 280/352 - Pérdida de Entrenamiento: 0.4670, Precisión de Entrenamiento: 0.8424\n",
      "Iteración 66491 - Lote 315/352 - Pérdida de Entrenamiento: 0.4683, Precisión de Entrenamiento: 0.8423\n",
      "Iteración 66526 - Lote 350/352 - Pérdida de Entrenamiento: 0.4670, Precisión de Entrenamiento: 0.8430\n",
      "Val loss: 0.4915, Val acc: 0.8302\n",
      "0.00078125\n",
      "Epoch 190/400\n",
      "Iteración 66563 - Lote 35/352 - Pérdida de Entrenamiento: 0.4495, Precisión de Entrenamiento: 0.8518\n",
      "Iteración 66598 - Lote 70/352 - Pérdida de Entrenamiento: 0.4605, Precisión de Entrenamiento: 0.8450\n",
      "Iteración 66633 - Lote 105/352 - Pérdida de Entrenamiento: 0.4596, Precisión de Entrenamiento: 0.8467\n",
      "Iteración 66668 - Lote 140/352 - Pérdida de Entrenamiento: 0.4591, Precisión de Entrenamiento: 0.8480\n",
      "Iteración 66703 - Lote 175/352 - Pérdida de Entrenamiento: 0.4586, Precisión de Entrenamiento: 0.8471\n",
      "Iteración 66738 - Lote 210/352 - Pérdida de Entrenamiento: 0.4598, Precisión de Entrenamiento: 0.8456\n",
      "Iteración 66773 - Lote 245/352 - Pérdida de Entrenamiento: 0.4607, Precisión de Entrenamiento: 0.8457\n",
      "Iteración 66808 - Lote 280/352 - Pérdida de Entrenamiento: 0.4592, Precisión de Entrenamiento: 0.8467\n",
      "Iteración 66843 - Lote 315/352 - Pérdida de Entrenamiento: 0.4611, Precisión de Entrenamiento: 0.8454\n",
      "Iteración 66878 - Lote 350/352 - Pérdida de Entrenamiento: 0.4607, Precisión de Entrenamiento: 0.8461\n",
      "Val loss: 0.4956, Val acc: 0.8292\n",
      "Gradientes para features.0.0.weight: min=-0.22402071952819824, max=0.24187886714935303, mean=-0.002410172950476408, std=0.06312385201454163\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.1126747578382492, max=0.13795985281467438, mean=-2.901097332141944e-06, std=0.004169730469584465\n",
      "Gradientes para classifier.1.weight: min=-0.01312315370887518, max=0.01707143522799015, mean=3.4924597935859225e-11, std=0.0022150888107717037\n",
      "0.00078125\n",
      "Epoch 191/400\n",
      "Iteración 66915 - Lote 35/352 - Pérdida de Entrenamiento: 0.4855, Precisión de Entrenamiento: 0.8377\n",
      "Iteración 66950 - Lote 70/352 - Pérdida de Entrenamiento: 0.4636, Precisión de Entrenamiento: 0.8462\n",
      "Iteración 66985 - Lote 105/352 - Pérdida de Entrenamiento: 0.4614, Precisión de Entrenamiento: 0.8454\n",
      "Iteración 67020 - Lote 140/352 - Pérdida de Entrenamiento: 0.4543, Precisión de Entrenamiento: 0.8474\n",
      "Iteración 67055 - Lote 175/352 - Pérdida de Entrenamiento: 0.4525, Precisión de Entrenamiento: 0.8476\n",
      "Iteración 67090 - Lote 210/352 - Pérdida de Entrenamiento: 0.4569, Precisión de Entrenamiento: 0.8455\n",
      "Iteración 67125 - Lote 245/352 - Pérdida de Entrenamiento: 0.4609, Precisión de Entrenamiento: 0.8447\n",
      "Iteración 67160 - Lote 280/352 - Pérdida de Entrenamiento: 0.4588, Precisión de Entrenamiento: 0.8455\n",
      "Iteración 67195 - Lote 315/352 - Pérdida de Entrenamiento: 0.4595, Precisión de Entrenamiento: 0.8450\n",
      "Iteración 67230 - Lote 350/352 - Pérdida de Entrenamiento: 0.4597, Precisión de Entrenamiento: 0.8453\n",
      "Val loss: 0.4851, Val acc: 0.8336\n",
      "0.00078125\n",
      "Epoch 192/400\n",
      "Iteración 67267 - Lote 35/352 - Pérdida de Entrenamiento: 0.4643, Precisión de Entrenamiento: 0.8460\n",
      "Iteración 67302 - Lote 70/352 - Pérdida de Entrenamiento: 0.4602, Precisión de Entrenamiento: 0.8455\n",
      "Iteración 67337 - Lote 105/352 - Pérdida de Entrenamiento: 0.4584, Precisión de Entrenamiento: 0.8490\n",
      "Iteración 67372 - Lote 140/352 - Pérdida de Entrenamiento: 0.4586, Precisión de Entrenamiento: 0.8487\n",
      "Iteración 67407 - Lote 175/352 - Pérdida de Entrenamiento: 0.4612, Precisión de Entrenamiento: 0.8478\n",
      "Iteración 67442 - Lote 210/352 - Pérdida de Entrenamiento: 0.4597, Precisión de Entrenamiento: 0.8477\n",
      "Iteración 67477 - Lote 245/352 - Pérdida de Entrenamiento: 0.4611, Precisión de Entrenamiento: 0.8474\n",
      "Iteración 67512 - Lote 280/352 - Pérdida de Entrenamiento: 0.4620, Precisión de Entrenamiento: 0.8465\n",
      "Iteración 67547 - Lote 315/352 - Pérdida de Entrenamiento: 0.4631, Precisión de Entrenamiento: 0.8455\n",
      "Iteración 67582 - Lote 350/352 - Pérdida de Entrenamiento: 0.4635, Precisión de Entrenamiento: 0.8453\n",
      "Val loss: 0.4991, Val acc: 0.8252\n",
      "0.00078125\n",
      "Epoch 193/400\n",
      "Iteración 67619 - Lote 35/352 - Pérdida de Entrenamiento: 0.4609, Precisión de Entrenamiento: 0.8451\n",
      "Iteración 67654 - Lote 70/352 - Pérdida de Entrenamiento: 0.4598, Precisión de Entrenamiento: 0.8484\n",
      "Iteración 67689 - Lote 105/352 - Pérdida de Entrenamiento: 0.4600, Precisión de Entrenamiento: 0.8473\n",
      "Iteración 67724 - Lote 140/352 - Pérdida de Entrenamiento: 0.4584, Precisión de Entrenamiento: 0.8463\n",
      "Iteración 67759 - Lote 175/352 - Pérdida de Entrenamiento: 0.4591, Precisión de Entrenamiento: 0.8461\n",
      "Iteración 67794 - Lote 210/352 - Pérdida de Entrenamiento: 0.4590, Precisión de Entrenamiento: 0.8451\n",
      "Iteración 67829 - Lote 245/352 - Pérdida de Entrenamiento: 0.4585, Precisión de Entrenamiento: 0.8456\n",
      "Iteración 67864 - Lote 280/352 - Pérdida de Entrenamiento: 0.4598, Precisión de Entrenamiento: 0.8443\n",
      "Iteración 67899 - Lote 315/352 - Pérdida de Entrenamiento: 0.4610, Precisión de Entrenamiento: 0.8440\n",
      "Iteración 67934 - Lote 350/352 - Pérdida de Entrenamiento: 0.4596, Precisión de Entrenamiento: 0.8446\n",
      "Val loss: 0.4861, Val acc: 0.8348\n",
      "0.00078125\n",
      "Epoch 194/400\n",
      "Iteración 67971 - Lote 35/352 - Pérdida de Entrenamiento: 0.4548, Precisión de Entrenamiento: 0.8453\n",
      "Iteración 68006 - Lote 70/352 - Pérdida de Entrenamiento: 0.4653, Precisión de Entrenamiento: 0.8445\n",
      "Iteración 68041 - Lote 105/352 - Pérdida de Entrenamiento: 0.4620, Precisión de Entrenamiento: 0.8444\n",
      "Iteración 68076 - Lote 140/352 - Pérdida de Entrenamiento: 0.4627, Precisión de Entrenamiento: 0.8447\n",
      "Iteración 68111 - Lote 175/352 - Pérdida de Entrenamiento: 0.4625, Precisión de Entrenamiento: 0.8456\n",
      "Iteración 68146 - Lote 210/352 - Pérdida de Entrenamiento: 0.4602, Precisión de Entrenamiento: 0.8465\n",
      "Iteración 68181 - Lote 245/352 - Pérdida de Entrenamiento: 0.4649, Precisión de Entrenamiento: 0.8456\n",
      "Iteración 68216 - Lote 280/352 - Pérdida de Entrenamiento: 0.4634, Precisión de Entrenamiento: 0.8458\n",
      "Iteración 68251 - Lote 315/352 - Pérdida de Entrenamiento: 0.4633, Precisión de Entrenamiento: 0.8458\n",
      "Iteración 68286 - Lote 350/352 - Pérdida de Entrenamiento: 0.4632, Precisión de Entrenamiento: 0.8458\n",
      "Val loss: 0.4846, Val acc: 0.8350\n",
      "0.00078125\n",
      "Epoch 195/400\n",
      "Iteración 68323 - Lote 35/352 - Pérdida de Entrenamiento: 0.4629, Precisión de Entrenamiento: 0.8444\n",
      "Iteración 68358 - Lote 70/352 - Pérdida de Entrenamiento: 0.4535, Precisión de Entrenamiento: 0.8446\n",
      "Iteración 68393 - Lote 105/352 - Pérdida de Entrenamiento: 0.4637, Precisión de Entrenamiento: 0.8423\n",
      "Iteración 68428 - Lote 140/352 - Pérdida de Entrenamiento: 0.4605, Precisión de Entrenamiento: 0.8433\n",
      "Iteración 68463 - Lote 175/352 - Pérdida de Entrenamiento: 0.4644, Precisión de Entrenamiento: 0.8433\n",
      "Iteración 68498 - Lote 210/352 - Pérdida de Entrenamiento: 0.4607, Precisión de Entrenamiento: 0.8451\n",
      "Iteración 68533 - Lote 245/352 - Pérdida de Entrenamiento: 0.4605, Precisión de Entrenamiento: 0.8445\n",
      "Iteración 68568 - Lote 280/352 - Pérdida de Entrenamiento: 0.4622, Precisión de Entrenamiento: 0.8448\n",
      "Iteración 68603 - Lote 315/352 - Pérdida de Entrenamiento: 0.4616, Precisión de Entrenamiento: 0.8453\n",
      "Iteración 68638 - Lote 350/352 - Pérdida de Entrenamiento: 0.4622, Precisión de Entrenamiento: 0.8447\n",
      "Val loss: 0.4882, Val acc: 0.8366\n",
      "Gradientes para features.0.0.weight: min=-0.12467655539512634, max=0.10344803333282471, mean=0.0028989817947149277, std=0.03164247050881386\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.10956370085477829, max=0.09052968770265579, mean=-5.576166017817741e-07, std=0.0037403972819447517\n",
      "Gradientes para classifier.1.weight: min=-0.01160302385687828, max=0.01473449170589447, mean=-3.2596290250319626e-11, std=0.0016736724646762013\n",
      "0.000390625\n",
      "Epoch 196/400\n",
      "Iteración 68675 - Lote 35/352 - Pérdida de Entrenamiento: 0.4534, Precisión de Entrenamiento: 0.8504\n",
      "Iteración 68710 - Lote 70/352 - Pérdida de Entrenamiento: 0.4495, Precisión de Entrenamiento: 0.8516\n",
      "Iteración 68745 - Lote 105/352 - Pérdida de Entrenamiento: 0.4450, Precisión de Entrenamiento: 0.8516\n",
      "Iteración 68780 - Lote 140/352 - Pérdida de Entrenamiento: 0.4452, Precisión de Entrenamiento: 0.8515\n",
      "Iteración 68815 - Lote 175/352 - Pérdida de Entrenamiento: 0.4421, Precisión de Entrenamiento: 0.8523\n",
      "Iteración 68850 - Lote 210/352 - Pérdida de Entrenamiento: 0.4422, Precisión de Entrenamiento: 0.8525\n",
      "Iteración 68885 - Lote 245/352 - Pérdida de Entrenamiento: 0.4401, Precisión de Entrenamiento: 0.8529\n",
      "Iteración 68920 - Lote 280/352 - Pérdida de Entrenamiento: 0.4457, Precisión de Entrenamiento: 0.8516\n",
      "Iteración 68955 - Lote 315/352 - Pérdida de Entrenamiento: 0.4453, Precisión de Entrenamiento: 0.8516\n",
      "Iteración 68990 - Lote 350/352 - Pérdida de Entrenamiento: 0.4468, Precisión de Entrenamiento: 0.8508\n",
      "Val loss: 0.4770, Val acc: 0.8370\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_196.pth\n",
      "Checkpoint del mejor modelo guardado en la época 196\n",
      "0.000390625\n",
      "Epoch 197/400\n",
      "Iteración 69027 - Lote 35/352 - Pérdida de Entrenamiento: 0.4395, Precisión de Entrenamiento: 0.8509\n",
      "Iteración 69062 - Lote 70/352 - Pérdida de Entrenamiento: 0.4443, Precisión de Entrenamiento: 0.8513\n",
      "Iteración 69097 - Lote 105/352 - Pérdida de Entrenamiento: 0.4385, Precisión de Entrenamiento: 0.8518\n",
      "Iteración 69132 - Lote 140/352 - Pérdida de Entrenamiento: 0.4402, Precisión de Entrenamiento: 0.8526\n",
      "Iteración 69167 - Lote 175/352 - Pérdida de Entrenamiento: 0.4400, Precisión de Entrenamiento: 0.8526\n",
      "Iteración 69202 - Lote 210/352 - Pérdida de Entrenamiento: 0.4460, Precisión de Entrenamiento: 0.8507\n",
      "Iteración 69237 - Lote 245/352 - Pérdida de Entrenamiento: 0.4481, Precisión de Entrenamiento: 0.8509\n",
      "Iteración 69272 - Lote 280/352 - Pérdida de Entrenamiento: 0.4471, Precisión de Entrenamiento: 0.8509\n",
      "Iteración 69307 - Lote 315/352 - Pérdida de Entrenamiento: 0.4449, Precisión de Entrenamiento: 0.8520\n",
      "Iteración 69342 - Lote 350/352 - Pérdida de Entrenamiento: 0.4468, Precisión de Entrenamiento: 0.8509\n",
      "Val loss: 0.4776, Val acc: 0.8342\n",
      "0.000390625\n",
      "Epoch 198/400\n",
      "Iteración 69379 - Lote 35/352 - Pérdida de Entrenamiento: 0.4376, Precisión de Entrenamiento: 0.8491\n",
      "Iteración 69414 - Lote 70/352 - Pérdida de Entrenamiento: 0.4406, Precisión de Entrenamiento: 0.8480\n",
      "Iteración 69449 - Lote 105/352 - Pérdida de Entrenamiento: 0.4451, Precisión de Entrenamiento: 0.8485\n",
      "Iteración 69484 - Lote 140/352 - Pérdida de Entrenamiento: 0.4437, Precisión de Entrenamiento: 0.8496\n",
      "Iteración 69519 - Lote 175/352 - Pérdida de Entrenamiento: 0.4407, Precisión de Entrenamiento: 0.8509\n",
      "Iteración 69554 - Lote 210/352 - Pérdida de Entrenamiento: 0.4435, Precisión de Entrenamiento: 0.8499\n",
      "Iteración 69589 - Lote 245/352 - Pérdida de Entrenamiento: 0.4460, Precisión de Entrenamiento: 0.8495\n",
      "Iteración 69624 - Lote 280/352 - Pérdida de Entrenamiento: 0.4457, Precisión de Entrenamiento: 0.8497\n",
      "Iteración 69659 - Lote 315/352 - Pérdida de Entrenamiento: 0.4459, Precisión de Entrenamiento: 0.8505\n",
      "Iteración 69694 - Lote 350/352 - Pérdida de Entrenamiento: 0.4452, Precisión de Entrenamiento: 0.8505\n",
      "Val loss: 0.4828, Val acc: 0.8360\n",
      "0.000390625\n",
      "Epoch 199/400\n",
      "Iteración 69731 - Lote 35/352 - Pérdida de Entrenamiento: 0.4423, Precisión de Entrenamiento: 0.8536\n",
      "Iteración 69766 - Lote 70/352 - Pérdida de Entrenamiento: 0.4461, Precisión de Entrenamiento: 0.8527\n",
      "Iteración 69801 - Lote 105/352 - Pérdida de Entrenamiento: 0.4454, Precisión de Entrenamiento: 0.8521\n",
      "Iteración 69836 - Lote 140/352 - Pérdida de Entrenamiento: 0.4514, Precisión de Entrenamiento: 0.8503\n",
      "Iteración 69871 - Lote 175/352 - Pérdida de Entrenamiento: 0.4489, Precisión de Entrenamiento: 0.8506\n",
      "Iteración 69906 - Lote 210/352 - Pérdida de Entrenamiento: 0.4522, Precisión de Entrenamiento: 0.8491\n",
      "Iteración 69941 - Lote 245/352 - Pérdida de Entrenamiento: 0.4479, Precisión de Entrenamiento: 0.8506\n",
      "Iteración 69976 - Lote 280/352 - Pérdida de Entrenamiento: 0.4470, Precisión de Entrenamiento: 0.8505\n",
      "Iteración 70011 - Lote 315/352 - Pérdida de Entrenamiento: 0.4463, Precisión de Entrenamiento: 0.8503\n",
      "Iteración 70046 - Lote 350/352 - Pérdida de Entrenamiento: 0.4462, Precisión de Entrenamiento: 0.8503\n",
      "Val loss: 0.4806, Val acc: 0.8376\n",
      "0.000390625\n",
      "Epoch 200/400\n",
      "Iteración 70083 - Lote 35/352 - Pérdida de Entrenamiento: 0.4354, Precisión de Entrenamiento: 0.8538\n",
      "Iteración 70118 - Lote 70/352 - Pérdida de Entrenamiento: 0.4349, Precisión de Entrenamiento: 0.8531\n",
      "Iteración 70153 - Lote 105/352 - Pérdida de Entrenamiento: 0.4346, Precisión de Entrenamiento: 0.8557\n",
      "Iteración 70188 - Lote 140/352 - Pérdida de Entrenamiento: 0.4344, Precisión de Entrenamiento: 0.8555\n",
      "Iteración 70223 - Lote 175/352 - Pérdida de Entrenamiento: 0.4369, Precisión de Entrenamiento: 0.8541\n",
      "Iteración 70258 - Lote 210/352 - Pérdida de Entrenamiento: 0.4389, Precisión de Entrenamiento: 0.8523\n",
      "Iteración 70293 - Lote 245/352 - Pérdida de Entrenamiento: 0.4390, Precisión de Entrenamiento: 0.8523\n",
      "Iteración 70328 - Lote 280/352 - Pérdida de Entrenamiento: 0.4376, Precisión de Entrenamiento: 0.8526\n",
      "Iteración 70363 - Lote 315/352 - Pérdida de Entrenamiento: 0.4371, Precisión de Entrenamiento: 0.8526\n",
      "Iteración 70398 - Lote 350/352 - Pérdida de Entrenamiento: 0.4382, Precisión de Entrenamiento: 0.8522\n",
      "Val loss: 0.4762, Val acc: 0.8334\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_200.pth\n",
      "Checkpoint del mejor modelo guardado en la época 200\n",
      "Gradientes para features.0.0.weight: min=-0.14221838116645813, max=0.24131515622138977, mean=0.011457238346338272, std=0.050066012889146805\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.1289624124765396, max=0.11887549608945847, mean=3.6278029256209265e-06, std=0.004239045549184084\n",
      "Gradientes para classifier.1.weight: min=-0.02065345272421837, max=0.018531054258346558, mean=-2.3283064712331658e-11, std=0.0026833785232156515\n",
      "0.000390625\n",
      "Epoch 201/400\n",
      "Iteración 70435 - Lote 35/352 - Pérdida de Entrenamiento: 0.4281, Precisión de Entrenamiento: 0.8533\n",
      "Iteración 70470 - Lote 70/352 - Pérdida de Entrenamiento: 0.4242, Precisión de Entrenamiento: 0.8570\n",
      "Iteración 70505 - Lote 105/352 - Pérdida de Entrenamiento: 0.4365, Precisión de Entrenamiento: 0.8522\n",
      "Iteración 70540 - Lote 140/352 - Pérdida de Entrenamiento: 0.4383, Precisión de Entrenamiento: 0.8522\n",
      "Iteración 70575 - Lote 175/352 - Pérdida de Entrenamiento: 0.4351, Precisión de Entrenamiento: 0.8534\n",
      "Iteración 70610 - Lote 210/352 - Pérdida de Entrenamiento: 0.4361, Precisión de Entrenamiento: 0.8531\n",
      "Iteración 70645 - Lote 245/352 - Pérdida de Entrenamiento: 0.4414, Precisión de Entrenamiento: 0.8514\n",
      "Iteración 70680 - Lote 280/352 - Pérdida de Entrenamiento: 0.4408, Precisión de Entrenamiento: 0.8517\n",
      "Iteración 70715 - Lote 315/352 - Pérdida de Entrenamiento: 0.4391, Precisión de Entrenamiento: 0.8526\n",
      "Iteración 70750 - Lote 350/352 - Pérdida de Entrenamiento: 0.4395, Precisión de Entrenamiento: 0.8524\n",
      "Val loss: 0.4805, Val acc: 0.8366\n",
      "0.000390625\n",
      "Epoch 202/400\n",
      "Iteración 70787 - Lote 35/352 - Pérdida de Entrenamiento: 0.4263, Precisión de Entrenamiento: 0.8558\n",
      "Iteración 70822 - Lote 70/352 - Pérdida de Entrenamiento: 0.4257, Precisión de Entrenamiento: 0.8581\n",
      "Iteración 70857 - Lote 105/352 - Pérdida de Entrenamiento: 0.4264, Precisión de Entrenamiento: 0.8582\n",
      "Iteración 70892 - Lote 140/352 - Pérdida de Entrenamiento: 0.4267, Precisión de Entrenamiento: 0.8570\n",
      "Iteración 70927 - Lote 175/352 - Pérdida de Entrenamiento: 0.4301, Precisión de Entrenamiento: 0.8567\n",
      "Iteración 70962 - Lote 210/352 - Pérdida de Entrenamiento: 0.4341, Precisión de Entrenamiento: 0.8551\n",
      "Iteración 70997 - Lote 245/352 - Pérdida de Entrenamiento: 0.4396, Precisión de Entrenamiento: 0.8532\n",
      "Iteración 71032 - Lote 280/352 - Pérdida de Entrenamiento: 0.4387, Precisión de Entrenamiento: 0.8533\n",
      "Iteración 71067 - Lote 315/352 - Pérdida de Entrenamiento: 0.4373, Precisión de Entrenamiento: 0.8537\n",
      "Iteración 71102 - Lote 350/352 - Pérdida de Entrenamiento: 0.4384, Precisión de Entrenamiento: 0.8533\n",
      "Val loss: 0.4760, Val acc: 0.8354\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_202.pth\n",
      "Checkpoint del mejor modelo guardado en la época 202\n",
      "0.000390625\n",
      "Epoch 203/400\n",
      "Iteración 71139 - Lote 35/352 - Pérdida de Entrenamiento: 0.4369, Precisión de Entrenamiento: 0.8598\n",
      "Iteración 71174 - Lote 70/352 - Pérdida de Entrenamiento: 0.4337, Precisión de Entrenamiento: 0.8581\n",
      "Iteración 71209 - Lote 105/352 - Pérdida de Entrenamiento: 0.4349, Precisión de Entrenamiento: 0.8560\n",
      "Iteración 71244 - Lote 140/352 - Pérdida de Entrenamiento: 0.4403, Precisión de Entrenamiento: 0.8542\n",
      "Iteración 71279 - Lote 175/352 - Pérdida de Entrenamiento: 0.4399, Precisión de Entrenamiento: 0.8534\n",
      "Iteración 71314 - Lote 210/352 - Pérdida de Entrenamiento: 0.4396, Precisión de Entrenamiento: 0.8534\n",
      "Iteración 71349 - Lote 245/352 - Pérdida de Entrenamiento: 0.4449, Precisión de Entrenamiento: 0.8515\n",
      "Iteración 71384 - Lote 280/352 - Pérdida de Entrenamiento: 0.4442, Precisión de Entrenamiento: 0.8515\n",
      "Iteración 71419 - Lote 315/352 - Pérdida de Entrenamiento: 0.4447, Precisión de Entrenamiento: 0.8507\n",
      "Iteración 71454 - Lote 350/352 - Pérdida de Entrenamiento: 0.4429, Precisión de Entrenamiento: 0.8513\n",
      "Val loss: 0.4763, Val acc: 0.8362\n",
      "0.000390625\n",
      "Epoch 204/400\n",
      "Iteración 71491 - Lote 35/352 - Pérdida de Entrenamiento: 0.4320, Precisión de Entrenamiento: 0.8562\n",
      "Iteración 71526 - Lote 70/352 - Pérdida de Entrenamiento: 0.4375, Precisión de Entrenamiento: 0.8518\n",
      "Iteración 71561 - Lote 105/352 - Pérdida de Entrenamiento: 0.4391, Precisión de Entrenamiento: 0.8529\n",
      "Iteración 71596 - Lote 140/352 - Pérdida de Entrenamiento: 0.4414, Precisión de Entrenamiento: 0.8515\n",
      "Iteración 71631 - Lote 175/352 - Pérdida de Entrenamiento: 0.4422, Precisión de Entrenamiento: 0.8507\n",
      "Iteración 71666 - Lote 210/352 - Pérdida de Entrenamiento: 0.4438, Precisión de Entrenamiento: 0.8504\n",
      "Iteración 71701 - Lote 245/352 - Pérdida de Entrenamiento: 0.4492, Precisión de Entrenamiento: 0.8484\n",
      "Iteración 71736 - Lote 280/352 - Pérdida de Entrenamiento: 0.4457, Precisión de Entrenamiento: 0.8499\n",
      "Iteración 71771 - Lote 315/352 - Pérdida de Entrenamiento: 0.4419, Precisión de Entrenamiento: 0.8511\n",
      "Iteración 71806 - Lote 350/352 - Pérdida de Entrenamiento: 0.4417, Precisión de Entrenamiento: 0.8510\n",
      "Val loss: 0.4793, Val acc: 0.8334\n",
      "0.000390625\n",
      "Epoch 205/400\n",
      "Iteración 71843 - Lote 35/352 - Pérdida de Entrenamiento: 0.4340, Precisión de Entrenamiento: 0.8529\n",
      "Iteración 71878 - Lote 70/352 - Pérdida de Entrenamiento: 0.4489, Precisión de Entrenamiento: 0.8526\n",
      "Iteración 71913 - Lote 105/352 - Pérdida de Entrenamiento: 0.4440, Precisión de Entrenamiento: 0.8531\n",
      "Iteración 71948 - Lote 140/352 - Pérdida de Entrenamiento: 0.4506, Precisión de Entrenamiento: 0.8497\n",
      "Iteración 71983 - Lote 175/352 - Pérdida de Entrenamiento: 0.4478, Precisión de Entrenamiento: 0.8507\n",
      "Iteración 72018 - Lote 210/352 - Pérdida de Entrenamiento: 0.4485, Precisión de Entrenamiento: 0.8487\n",
      "Iteración 72053 - Lote 245/352 - Pérdida de Entrenamiento: 0.4457, Precisión de Entrenamiento: 0.8501\n",
      "Iteración 72088 - Lote 280/352 - Pérdida de Entrenamiento: 0.4448, Precisión de Entrenamiento: 0.8499\n",
      "Iteración 72123 - Lote 315/352 - Pérdida de Entrenamiento: 0.4433, Precisión de Entrenamiento: 0.8499\n",
      "Iteración 72158 - Lote 350/352 - Pérdida de Entrenamiento: 0.4433, Precisión de Entrenamiento: 0.8511\n",
      "Val loss: 0.4756, Val acc: 0.8366\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_205.pth\n",
      "Checkpoint del mejor modelo guardado en la época 205\n",
      "Gradientes para features.0.0.weight: min=-0.3088531494140625, max=0.1605505347251892, mean=-0.0246302280575037, std=0.06759636104106903\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.2845222055912018, max=0.19625453650951385, mean=9.136767403106205e-06, std=0.005468566901981831\n",
      "Gradientes para classifier.1.weight: min=-0.02308269962668419, max=0.014999504201114178, mean=9.313225537987968e-12, std=0.0026327469386160374\n",
      "0.000390625\n",
      "Epoch 206/400\n",
      "Iteración 72195 - Lote 35/352 - Pérdida de Entrenamiento: 0.4532, Precisión de Entrenamiento: 0.8453\n",
      "Iteración 72230 - Lote 70/352 - Pérdida de Entrenamiento: 0.4408, Precisión de Entrenamiento: 0.8513\n",
      "Iteración 72265 - Lote 105/352 - Pérdida de Entrenamiento: 0.4413, Precisión de Entrenamiento: 0.8510\n",
      "Iteración 72300 - Lote 140/352 - Pérdida de Entrenamiento: 0.4364, Precisión de Entrenamiento: 0.8523\n",
      "Iteración 72335 - Lote 175/352 - Pérdida de Entrenamiento: 0.4389, Precisión de Entrenamiento: 0.8527\n",
      "Iteración 72370 - Lote 210/352 - Pérdida de Entrenamiento: 0.4396, Precisión de Entrenamiento: 0.8529\n",
      "Iteración 72405 - Lote 245/352 - Pérdida de Entrenamiento: 0.4391, Precisión de Entrenamiento: 0.8527\n",
      "Iteración 72440 - Lote 280/352 - Pérdida de Entrenamiento: 0.4386, Precisión de Entrenamiento: 0.8527\n",
      "Iteración 72475 - Lote 315/352 - Pérdida de Entrenamiento: 0.4391, Precisión de Entrenamiento: 0.8530\n",
      "Iteración 72510 - Lote 350/352 - Pérdida de Entrenamiento: 0.4410, Precisión de Entrenamiento: 0.8525\n",
      "Val loss: 0.4737, Val acc: 0.8368\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_206.pth\n",
      "Checkpoint del mejor modelo guardado en la época 206\n",
      "0.000390625\n",
      "Epoch 207/400\n",
      "Iteración 72547 - Lote 35/352 - Pérdida de Entrenamiento: 0.4211, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 72582 - Lote 70/352 - Pérdida de Entrenamiento: 0.4222, Precisión de Entrenamiento: 0.8615\n",
      "Iteración 72617 - Lote 105/352 - Pérdida de Entrenamiento: 0.4352, Precisión de Entrenamiento: 0.8554\n",
      "Iteración 72652 - Lote 140/352 - Pérdida de Entrenamiento: 0.4345, Precisión de Entrenamiento: 0.8554\n",
      "Iteración 72687 - Lote 175/352 - Pérdida de Entrenamiento: 0.4354, Precisión de Entrenamiento: 0.8551\n",
      "Iteración 72722 - Lote 210/352 - Pérdida de Entrenamiento: 0.4360, Precisión de Entrenamiento: 0.8549\n",
      "Iteración 72757 - Lote 245/352 - Pérdida de Entrenamiento: 0.4372, Precisión de Entrenamiento: 0.8539\n",
      "Iteración 72792 - Lote 280/352 - Pérdida de Entrenamiento: 0.4385, Precisión de Entrenamiento: 0.8533\n",
      "Iteración 72827 - Lote 315/352 - Pérdida de Entrenamiento: 0.4391, Precisión de Entrenamiento: 0.8531\n",
      "Iteración 72862 - Lote 350/352 - Pérdida de Entrenamiento: 0.4383, Precisión de Entrenamiento: 0.8535\n",
      "Val loss: 0.4784, Val acc: 0.8328\n",
      "0.000390625\n",
      "Epoch 208/400\n",
      "Iteración 72899 - Lote 35/352 - Pérdida de Entrenamiento: 0.4393, Precisión de Entrenamiento: 0.8496\n",
      "Iteración 72934 - Lote 70/352 - Pérdida de Entrenamiento: 0.4363, Precisión de Entrenamiento: 0.8528\n",
      "Iteración 72969 - Lote 105/352 - Pérdida de Entrenamiento: 0.4412, Precisión de Entrenamiento: 0.8528\n",
      "Iteración 73004 - Lote 140/352 - Pérdida de Entrenamiento: 0.4360, Precisión de Entrenamiento: 0.8538\n",
      "Iteración 73039 - Lote 175/352 - Pérdida de Entrenamiento: 0.4406, Precisión de Entrenamiento: 0.8509\n",
      "Iteración 73074 - Lote 210/352 - Pérdida de Entrenamiento: 0.4395, Precisión de Entrenamiento: 0.8516\n",
      "Iteración 73109 - Lote 245/352 - Pérdida de Entrenamiento: 0.4394, Precisión de Entrenamiento: 0.8521\n",
      "Iteración 73144 - Lote 280/352 - Pérdida de Entrenamiento: 0.4381, Precisión de Entrenamiento: 0.8531\n",
      "Iteración 73179 - Lote 315/352 - Pérdida de Entrenamiento: 0.4389, Precisión de Entrenamiento: 0.8529\n",
      "Iteración 73214 - Lote 350/352 - Pérdida de Entrenamiento: 0.4381, Precisión de Entrenamiento: 0.8530\n",
      "Val loss: 0.4762, Val acc: 0.8372\n",
      "0.000390625\n",
      "Epoch 209/400\n",
      "Iteración 73251 - Lote 35/352 - Pérdida de Entrenamiento: 0.4129, Precisión de Entrenamiento: 0.8616\n",
      "Iteración 73286 - Lote 70/352 - Pérdida de Entrenamiento: 0.4107, Precisión de Entrenamiento: 0.8615\n",
      "Iteración 73321 - Lote 105/352 - Pérdida de Entrenamiento: 0.4212, Precisión de Entrenamiento: 0.8592\n",
      "Iteración 73356 - Lote 140/352 - Pérdida de Entrenamiento: 0.4260, Precisión de Entrenamiento: 0.8575\n",
      "Iteración 73391 - Lote 175/352 - Pérdida de Entrenamiento: 0.4282, Precisión de Entrenamiento: 0.8567\n",
      "Iteración 73426 - Lote 210/352 - Pérdida de Entrenamiento: 0.4308, Precisión de Entrenamiento: 0.8554\n",
      "Iteración 73461 - Lote 245/352 - Pérdida de Entrenamiento: 0.4304, Precisión de Entrenamiento: 0.8559\n",
      "Iteración 73496 - Lote 280/352 - Pérdida de Entrenamiento: 0.4326, Precisión de Entrenamiento: 0.8549\n",
      "Iteración 73531 - Lote 315/352 - Pérdida de Entrenamiento: 0.4339, Precisión de Entrenamiento: 0.8542\n",
      "Iteración 73566 - Lote 350/352 - Pérdida de Entrenamiento: 0.4344, Precisión de Entrenamiento: 0.8542\n",
      "Val loss: 0.4826, Val acc: 0.8356\n",
      "0.000390625\n",
      "Epoch 210/400\n",
      "Iteración 73603 - Lote 35/352 - Pérdida de Entrenamiento: 0.4344, Precisión de Entrenamiento: 0.8551\n",
      "Iteración 73638 - Lote 70/352 - Pérdida de Entrenamiento: 0.4356, Precisión de Entrenamiento: 0.8558\n",
      "Iteración 73673 - Lote 105/352 - Pérdida de Entrenamiento: 0.4282, Precisión de Entrenamiento: 0.8580\n",
      "Iteración 73708 - Lote 140/352 - Pérdida de Entrenamiento: 0.4340, Precisión de Entrenamiento: 0.8561\n",
      "Iteración 73743 - Lote 175/352 - Pérdida de Entrenamiento: 0.4383, Precisión de Entrenamiento: 0.8543\n",
      "Iteración 73778 - Lote 210/352 - Pérdida de Entrenamiento: 0.4407, Precisión de Entrenamiento: 0.8530\n",
      "Iteración 73813 - Lote 245/352 - Pérdida de Entrenamiento: 0.4441, Precisión de Entrenamiento: 0.8518\n",
      "Iteración 73848 - Lote 280/352 - Pérdida de Entrenamiento: 0.4463, Precisión de Entrenamiento: 0.8508\n",
      "Iteración 73883 - Lote 315/352 - Pérdida de Entrenamiento: 0.4440, Precisión de Entrenamiento: 0.8515\n",
      "Iteración 73918 - Lote 350/352 - Pérdida de Entrenamiento: 0.4401, Precisión de Entrenamiento: 0.8532\n",
      "Val loss: 0.4740, Val acc: 0.8394\n",
      "Gradientes para features.0.0.weight: min=-0.15832564234733582, max=0.1266561895608902, mean=-0.0029941846150904894, std=0.04308968782424927\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.12604834139347076, max=0.16954095661640167, mean=1.8741625353868585e-06, std=0.004400715697556734\n",
      "Gradientes para classifier.1.weight: min=-0.020908014848828316, max=0.018717315047979355, mean=1.8626451075975936e-11, std=0.0018517228309065104\n",
      "0.000390625\n",
      "Epoch 211/400\n",
      "Iteración 73955 - Lote 35/352 - Pérdida de Entrenamiento: 0.4221, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 73990 - Lote 70/352 - Pérdida de Entrenamiento: 0.4424, Precisión de Entrenamiento: 0.8544\n",
      "Iteración 74025 - Lote 105/352 - Pérdida de Entrenamiento: 0.4383, Precisión de Entrenamiento: 0.8536\n",
      "Iteración 74060 - Lote 140/352 - Pérdida de Entrenamiento: 0.4370, Precisión de Entrenamiento: 0.8533\n",
      "Iteración 74095 - Lote 175/352 - Pérdida de Entrenamiento: 0.4377, Precisión de Entrenamiento: 0.8530\n",
      "Iteración 74130 - Lote 210/352 - Pérdida de Entrenamiento: 0.4368, Precisión de Entrenamiento: 0.8532\n",
      "Iteración 74165 - Lote 245/352 - Pérdida de Entrenamiento: 0.4353, Precisión de Entrenamiento: 0.8536\n",
      "Iteración 74200 - Lote 280/352 - Pérdida de Entrenamiento: 0.4372, Precisión de Entrenamiento: 0.8529\n",
      "Iteración 74235 - Lote 315/352 - Pérdida de Entrenamiento: 0.4384, Precisión de Entrenamiento: 0.8525\n",
      "Iteración 74270 - Lote 350/352 - Pérdida de Entrenamiento: 0.4378, Precisión de Entrenamiento: 0.8525\n",
      "Val loss: 0.4760, Val acc: 0.8366\n",
      "0.000390625\n",
      "Epoch 212/400\n",
      "Iteración 74307 - Lote 35/352 - Pérdida de Entrenamiento: 0.4244, Precisión de Entrenamiento: 0.8603\n",
      "Iteración 74342 - Lote 70/352 - Pérdida de Entrenamiento: 0.4310, Precisión de Entrenamiento: 0.8592\n",
      "Iteración 74377 - Lote 105/352 - Pérdida de Entrenamiento: 0.4323, Precisión de Entrenamiento: 0.8562\n",
      "Iteración 74412 - Lote 140/352 - Pérdida de Entrenamiento: 0.4303, Precisión de Entrenamiento: 0.8572\n",
      "Iteración 74447 - Lote 175/352 - Pérdida de Entrenamiento: 0.4338, Precisión de Entrenamiento: 0.8566\n",
      "Iteración 74482 - Lote 210/352 - Pérdida de Entrenamiento: 0.4327, Precisión de Entrenamiento: 0.8565\n",
      "Iteración 74517 - Lote 245/352 - Pérdida de Entrenamiento: 0.4346, Precisión de Entrenamiento: 0.8561\n",
      "Iteración 74552 - Lote 280/352 - Pérdida de Entrenamiento: 0.4352, Precisión de Entrenamiento: 0.8559\n",
      "Iteración 74587 - Lote 315/352 - Pérdida de Entrenamiento: 0.4359, Precisión de Entrenamiento: 0.8556\n",
      "Iteración 74622 - Lote 350/352 - Pérdida de Entrenamiento: 0.4382, Precisión de Entrenamiento: 0.8552\n",
      "Val loss: 0.4799, Val acc: 0.8348\n",
      "0.000390625\n",
      "Epoch 213/400\n",
      "Iteración 74659 - Lote 35/352 - Pérdida de Entrenamiento: 0.4409, Precisión de Entrenamiento: 0.8560\n",
      "Iteración 74694 - Lote 70/352 - Pérdida de Entrenamiento: 0.4387, Precisión de Entrenamiento: 0.8519\n",
      "Iteración 74729 - Lote 105/352 - Pérdida de Entrenamiento: 0.4397, Precisión de Entrenamiento: 0.8524\n",
      "Iteración 74764 - Lote 140/352 - Pérdida de Entrenamiento: 0.4402, Precisión de Entrenamiento: 0.8512\n",
      "Iteración 74799 - Lote 175/352 - Pérdida de Entrenamiento: 0.4412, Precisión de Entrenamiento: 0.8515\n",
      "Iteración 74834 - Lote 210/352 - Pérdida de Entrenamiento: 0.4436, Precisión de Entrenamiento: 0.8503\n",
      "Iteración 74869 - Lote 245/352 - Pérdida de Entrenamiento: 0.4415, Precisión de Entrenamiento: 0.8515\n",
      "Iteración 74904 - Lote 280/352 - Pérdida de Entrenamiento: 0.4403, Precisión de Entrenamiento: 0.8523\n",
      "Iteración 74939 - Lote 315/352 - Pérdida de Entrenamiento: 0.4373, Precisión de Entrenamiento: 0.8531\n",
      "Iteración 74974 - Lote 350/352 - Pérdida de Entrenamiento: 0.4371, Precisión de Entrenamiento: 0.8531\n",
      "Val loss: 0.4743, Val acc: 0.8390\n",
      "0.000390625\n",
      "Epoch 214/400\n",
      "Iteración 75011 - Lote 35/352 - Pérdida de Entrenamiento: 0.4245, Precisión de Entrenamiento: 0.8554\n",
      "Iteración 75046 - Lote 70/352 - Pérdida de Entrenamiento: 0.4347, Precisión de Entrenamiento: 0.8536\n",
      "Iteración 75081 - Lote 105/352 - Pérdida de Entrenamiento: 0.4350, Precisión de Entrenamiento: 0.8536\n",
      "Iteración 75116 - Lote 140/352 - Pérdida de Entrenamiento: 0.4327, Precisión de Entrenamiento: 0.8551\n",
      "Iteración 75151 - Lote 175/352 - Pérdida de Entrenamiento: 0.4331, Precisión de Entrenamiento: 0.8547\n",
      "Iteración 75186 - Lote 210/352 - Pérdida de Entrenamiento: 0.4368, Precisión de Entrenamiento: 0.8539\n",
      "Iteración 75221 - Lote 245/352 - Pérdida de Entrenamiento: 0.4377, Precisión de Entrenamiento: 0.8540\n",
      "Iteración 75256 - Lote 280/352 - Pérdida de Entrenamiento: 0.4385, Precisión de Entrenamiento: 0.8530\n",
      "Iteración 75291 - Lote 315/352 - Pérdida de Entrenamiento: 0.4409, Precisión de Entrenamiento: 0.8515\n",
      "Iteración 75326 - Lote 350/352 - Pérdida de Entrenamiento: 0.4406, Precisión de Entrenamiento: 0.8519\n",
      "Val loss: 0.4803, Val acc: 0.8334\n",
      "0.000390625\n",
      "Epoch 215/400\n",
      "Iteración 75363 - Lote 35/352 - Pérdida de Entrenamiento: 0.4344, Precisión de Entrenamiento: 0.8592\n",
      "Iteración 75398 - Lote 70/352 - Pérdida de Entrenamiento: 0.4305, Precisión de Entrenamiento: 0.8569\n",
      "Iteración 75433 - Lote 105/352 - Pérdida de Entrenamiento: 0.4314, Precisión de Entrenamiento: 0.8565\n",
      "Iteración 75468 - Lote 140/352 - Pérdida de Entrenamiento: 0.4345, Precisión de Entrenamiento: 0.8530\n",
      "Iteración 75503 - Lote 175/352 - Pérdida de Entrenamiento: 0.4316, Precisión de Entrenamiento: 0.8539\n",
      "Iteración 75538 - Lote 210/352 - Pérdida de Entrenamiento: 0.4329, Precisión de Entrenamiento: 0.8541\n",
      "Iteración 75573 - Lote 245/352 - Pérdida de Entrenamiento: 0.4353, Precisión de Entrenamiento: 0.8531\n",
      "Iteración 75608 - Lote 280/352 - Pérdida de Entrenamiento: 0.4385, Precisión de Entrenamiento: 0.8522\n",
      "Iteración 75643 - Lote 315/352 - Pérdida de Entrenamiento: 0.4399, Precisión de Entrenamiento: 0.8523\n",
      "Iteración 75678 - Lote 350/352 - Pérdida de Entrenamiento: 0.4391, Precisión de Entrenamiento: 0.8527\n",
      "Val loss: 0.4820, Val acc: 0.8370\n",
      "Gradientes para features.0.0.weight: min=-0.22814033925533295, max=0.2174573689699173, mean=0.007131822872906923, std=0.056028131395578384\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.3144472539424896, max=0.24898560345172882, mean=2.7932649118156405e-06, std=0.004894528072327375\n",
      "Gradientes para classifier.1.weight: min=-0.013171836733818054, max=0.015561996959149837, mean=2.3283064712331658e-11, std=0.0021344353444874287\n",
      "0.0001953125\n",
      "Epoch 216/400\n",
      "Iteración 75715 - Lote 35/352 - Pérdida de Entrenamiento: 0.4311, Precisión de Entrenamiento: 0.8558\n",
      "Iteración 75750 - Lote 70/352 - Pérdida de Entrenamiento: 0.4304, Precisión de Entrenamiento: 0.8570\n",
      "Iteración 75785 - Lote 105/352 - Pérdida de Entrenamiento: 0.4336, Precisión de Entrenamiento: 0.8562\n",
      "Iteración 75820 - Lote 140/352 - Pérdida de Entrenamiento: 0.4373, Precisión de Entrenamiento: 0.8538\n",
      "Iteración 75855 - Lote 175/352 - Pérdida de Entrenamiento: 0.4355, Precisión de Entrenamiento: 0.8535\n",
      "Iteración 75890 - Lote 210/352 - Pérdida de Entrenamiento: 0.4326, Precisión de Entrenamiento: 0.8540\n",
      "Iteración 75925 - Lote 245/352 - Pérdida de Entrenamiento: 0.4301, Precisión de Entrenamiento: 0.8556\n",
      "Iteración 75960 - Lote 280/352 - Pérdida de Entrenamiento: 0.4311, Precisión de Entrenamiento: 0.8550\n",
      "Iteración 75995 - Lote 315/352 - Pérdida de Entrenamiento: 0.4313, Precisión de Entrenamiento: 0.8554\n",
      "Iteración 76030 - Lote 350/352 - Pérdida de Entrenamiento: 0.4326, Precisión de Entrenamiento: 0.8546\n",
      "Val loss: 0.4753, Val acc: 0.8386\n",
      "0.0001953125\n",
      "Epoch 217/400\n",
      "Iteración 76067 - Lote 35/352 - Pérdida de Entrenamiento: 0.4357, Precisión de Entrenamiento: 0.8496\n",
      "Iteración 76102 - Lote 70/352 - Pérdida de Entrenamiento: 0.4328, Precisión de Entrenamiento: 0.8533\n",
      "Iteración 76137 - Lote 105/352 - Pérdida de Entrenamiento: 0.4378, Precisión de Entrenamiento: 0.8526\n",
      "Iteración 76172 - Lote 140/352 - Pérdida de Entrenamiento: 0.4335, Precisión de Entrenamiento: 0.8542\n",
      "Iteración 76207 - Lote 175/352 - Pérdida de Entrenamiento: 0.4327, Precisión de Entrenamiento: 0.8542\n",
      "Iteración 76242 - Lote 210/352 - Pérdida de Entrenamiento: 0.4323, Precisión de Entrenamiento: 0.8548\n",
      "Iteración 76277 - Lote 245/352 - Pérdida de Entrenamiento: 0.4326, Precisión de Entrenamiento: 0.8548\n",
      "Iteración 76312 - Lote 280/352 - Pérdida de Entrenamiento: 0.4316, Precisión de Entrenamiento: 0.8561\n",
      "Iteración 76347 - Lote 315/352 - Pérdida de Entrenamiento: 0.4316, Precisión de Entrenamiento: 0.8566\n",
      "Iteración 76382 - Lote 350/352 - Pérdida de Entrenamiento: 0.4308, Precisión de Entrenamiento: 0.8567\n",
      "Val loss: 0.4773, Val acc: 0.8374\n",
      "0.0001953125\n",
      "Epoch 218/400\n",
      "Iteración 76419 - Lote 35/352 - Pérdida de Entrenamiento: 0.4322, Precisión de Entrenamiento: 0.8587\n",
      "Iteración 76454 - Lote 70/352 - Pérdida de Entrenamiento: 0.4344, Precisión de Entrenamiento: 0.8545\n",
      "Iteración 76489 - Lote 105/352 - Pérdida de Entrenamiento: 0.4363, Precisión de Entrenamiento: 0.8556\n",
      "Iteración 76524 - Lote 140/352 - Pérdida de Entrenamiento: 0.4302, Precisión de Entrenamiento: 0.8561\n",
      "Iteración 76559 - Lote 175/352 - Pérdida de Entrenamiento: 0.4282, Precisión de Entrenamiento: 0.8571\n",
      "Iteración 76594 - Lote 210/352 - Pérdida de Entrenamiento: 0.4290, Precisión de Entrenamiento: 0.8572\n",
      "Iteración 76629 - Lote 245/352 - Pérdida de Entrenamiento: 0.4314, Precisión de Entrenamiento: 0.8560\n",
      "Iteración 76664 - Lote 280/352 - Pérdida de Entrenamiento: 0.4312, Precisión de Entrenamiento: 0.8562\n",
      "Iteración 76699 - Lote 315/352 - Pérdida de Entrenamiento: 0.4293, Precisión de Entrenamiento: 0.8570\n",
      "Iteración 76734 - Lote 350/352 - Pérdida de Entrenamiento: 0.4304, Precisión de Entrenamiento: 0.8565\n",
      "Val loss: 0.4751, Val acc: 0.8364\n",
      "0.0001953125\n",
      "Epoch 219/400\n",
      "Iteración 76771 - Lote 35/352 - Pérdida de Entrenamiento: 0.4367, Precisión de Entrenamiento: 0.8607\n",
      "Iteración 76806 - Lote 70/352 - Pérdida de Entrenamiento: 0.4296, Precisión de Entrenamiento: 0.8559\n",
      "Iteración 76841 - Lote 105/352 - Pérdida de Entrenamiento: 0.4299, Precisión de Entrenamiento: 0.8551\n",
      "Iteración 76876 - Lote 140/352 - Pérdida de Entrenamiento: 0.4334, Precisión de Entrenamiento: 0.8536\n",
      "Iteración 76911 - Lote 175/352 - Pérdida de Entrenamiento: 0.4314, Precisión de Entrenamiento: 0.8535\n",
      "Iteración 76946 - Lote 210/352 - Pérdida de Entrenamiento: 0.4307, Precisión de Entrenamiento: 0.8537\n",
      "Iteración 76981 - Lote 245/352 - Pérdida de Entrenamiento: 0.4331, Precisión de Entrenamiento: 0.8535\n",
      "Iteración 77016 - Lote 280/352 - Pérdida de Entrenamiento: 0.4345, Precisión de Entrenamiento: 0.8532\n",
      "Iteración 77051 - Lote 315/352 - Pérdida de Entrenamiento: 0.4363, Precisión de Entrenamiento: 0.8525\n",
      "Iteración 77086 - Lote 350/352 - Pérdida de Entrenamiento: 0.4345, Precisión de Entrenamiento: 0.8532\n",
      "Val loss: 0.4728, Val acc: 0.8398\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_219.pth\n",
      "Checkpoint del mejor modelo guardado en la época 219\n",
      "0.0001953125\n",
      "Epoch 220/400\n",
      "Iteración 77123 - Lote 35/352 - Pérdida de Entrenamiento: 0.4250, Precisión de Entrenamiento: 0.8547\n",
      "Iteración 77158 - Lote 70/352 - Pérdida de Entrenamiento: 0.4207, Precisión de Entrenamiento: 0.8585\n",
      "Iteración 77193 - Lote 105/352 - Pérdida de Entrenamiento: 0.4212, Precisión de Entrenamiento: 0.8576\n",
      "Iteración 77228 - Lote 140/352 - Pérdida de Entrenamiento: 0.4216, Precisión de Entrenamiento: 0.8583\n",
      "Iteración 77263 - Lote 175/352 - Pérdida de Entrenamiento: 0.4242, Precisión de Entrenamiento: 0.8577\n",
      "Iteración 77298 - Lote 210/352 - Pérdida de Entrenamiento: 0.4255, Precisión de Entrenamiento: 0.8583\n",
      "Iteración 77333 - Lote 245/352 - Pérdida de Entrenamiento: 0.4256, Precisión de Entrenamiento: 0.8584\n",
      "Iteración 77368 - Lote 280/352 - Pérdida de Entrenamiento: 0.4225, Precisión de Entrenamiento: 0.8600\n",
      "Iteración 77403 - Lote 315/352 - Pérdida de Entrenamiento: 0.4240, Precisión de Entrenamiento: 0.8594\n",
      "Iteración 77438 - Lote 350/352 - Pérdida de Entrenamiento: 0.4247, Precisión de Entrenamiento: 0.8594\n",
      "Val loss: 0.4758, Val acc: 0.8370\n",
      "Gradientes para features.0.0.weight: min=-0.2666178047657013, max=0.2222435176372528, mean=0.0008407182758674026, std=0.0585457906126976\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.1033221110701561, max=0.10782135277986526, mean=1.1604586688918062e-05, std=0.003229614347219467\n",
      "Gradientes para classifier.1.weight: min=-0.011536410078406334, max=0.010016015730798244, mean=4.656612768993984e-12, std=0.001272539608180523\n",
      "0.0001953125\n",
      "Epoch 221/400\n",
      "Iteración 77475 - Lote 35/352 - Pérdida de Entrenamiento: 0.4197, Precisión de Entrenamiento: 0.8603\n",
      "Iteración 77510 - Lote 70/352 - Pérdida de Entrenamiento: 0.4206, Precisión de Entrenamiento: 0.8599\n",
      "Iteración 77545 - Lote 105/352 - Pérdida de Entrenamiento: 0.4269, Precisión de Entrenamiento: 0.8592\n",
      "Iteración 77580 - Lote 140/352 - Pérdida de Entrenamiento: 0.4273, Precisión de Entrenamiento: 0.8585\n",
      "Iteración 77615 - Lote 175/352 - Pérdida de Entrenamiento: 0.4289, Precisión de Entrenamiento: 0.8575\n",
      "Iteración 77650 - Lote 210/352 - Pérdida de Entrenamiento: 0.4293, Precisión de Entrenamiento: 0.8571\n",
      "Iteración 77685 - Lote 245/352 - Pérdida de Entrenamiento: 0.4327, Precisión de Entrenamiento: 0.8561\n",
      "Iteración 77720 - Lote 280/352 - Pérdida de Entrenamiento: 0.4326, Precisión de Entrenamiento: 0.8557\n",
      "Iteración 77755 - Lote 315/352 - Pérdida de Entrenamiento: 0.4314, Precisión de Entrenamiento: 0.8562\n",
      "Iteración 77790 - Lote 350/352 - Pérdida de Entrenamiento: 0.4319, Precisión de Entrenamiento: 0.8560\n",
      "Val loss: 0.4780, Val acc: 0.8382\n",
      "0.0001953125\n",
      "Epoch 222/400\n",
      "Iteración 77827 - Lote 35/352 - Pérdida de Entrenamiento: 0.4153, Precisión de Entrenamiento: 0.8629\n",
      "Iteración 77862 - Lote 70/352 - Pérdida de Entrenamiento: 0.4302, Precisión de Entrenamiento: 0.8587\n",
      "Iteración 77897 - Lote 105/352 - Pérdida de Entrenamiento: 0.4346, Precisión de Entrenamiento: 0.8580\n",
      "Iteración 77932 - Lote 140/352 - Pérdida de Entrenamiento: 0.4320, Precisión de Entrenamiento: 0.8566\n",
      "Iteración 77967 - Lote 175/352 - Pérdida de Entrenamiento: 0.4334, Precisión de Entrenamiento: 0.8564\n",
      "Iteración 78002 - Lote 210/352 - Pérdida de Entrenamiento: 0.4322, Precisión de Entrenamiento: 0.8565\n",
      "Iteración 78037 - Lote 245/352 - Pérdida de Entrenamiento: 0.4310, Precisión de Entrenamiento: 0.8561\n",
      "Iteración 78072 - Lote 280/352 - Pérdida de Entrenamiento: 0.4301, Precisión de Entrenamiento: 0.8565\n",
      "Iteración 78107 - Lote 315/352 - Pérdida de Entrenamiento: 0.4303, Precisión de Entrenamiento: 0.8564\n",
      "Iteración 78142 - Lote 350/352 - Pérdida de Entrenamiento: 0.4300, Precisión de Entrenamiento: 0.8566\n",
      "Val loss: 0.4756, Val acc: 0.8390\n",
      "0.0001953125\n",
      "Epoch 223/400\n",
      "Iteración 78179 - Lote 35/352 - Pérdida de Entrenamiento: 0.4337, Precisión de Entrenamiento: 0.8502\n",
      "Iteración 78214 - Lote 70/352 - Pérdida de Entrenamiento: 0.4268, Precisión de Entrenamiento: 0.8546\n",
      "Iteración 78249 - Lote 105/352 - Pérdida de Entrenamiento: 0.4348, Precisión de Entrenamiento: 0.8528\n",
      "Iteración 78284 - Lote 140/352 - Pérdida de Entrenamiento: 0.4353, Precisión de Entrenamiento: 0.8533\n",
      "Iteración 78319 - Lote 175/352 - Pérdida de Entrenamiento: 0.4323, Precisión de Entrenamiento: 0.8549\n",
      "Iteración 78354 - Lote 210/352 - Pérdida de Entrenamiento: 0.4334, Precisión de Entrenamiento: 0.8545\n",
      "Iteración 78389 - Lote 245/352 - Pérdida de Entrenamiento: 0.4342, Precisión de Entrenamiento: 0.8546\n",
      "Iteración 78424 - Lote 280/352 - Pérdida de Entrenamiento: 0.4316, Precisión de Entrenamiento: 0.8555\n",
      "Iteración 78459 - Lote 315/352 - Pérdida de Entrenamiento: 0.4317, Precisión de Entrenamiento: 0.8553\n",
      "Iteración 78494 - Lote 350/352 - Pérdida de Entrenamiento: 0.4325, Precisión de Entrenamiento: 0.8551\n",
      "Val loss: 0.4692, Val acc: 0.8386\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_223.pth\n",
      "Checkpoint del mejor modelo guardado en la época 223\n",
      "0.0001953125\n",
      "Epoch 224/400\n",
      "Iteración 78531 - Lote 35/352 - Pérdida de Entrenamiento: 0.4327, Precisión de Entrenamiento: 0.8594\n",
      "Iteración 78566 - Lote 70/352 - Pérdida de Entrenamiento: 0.4318, Precisión de Entrenamiento: 0.8556\n",
      "Iteración 78601 - Lote 105/352 - Pérdida de Entrenamiento: 0.4273, Precisión de Entrenamiento: 0.8578\n",
      "Iteración 78636 - Lote 140/352 - Pérdida de Entrenamiento: 0.4304, Precisión de Entrenamiento: 0.8561\n",
      "Iteración 78671 - Lote 175/352 - Pérdida de Entrenamiento: 0.4309, Precisión de Entrenamiento: 0.8574\n",
      "Iteración 78706 - Lote 210/352 - Pérdida de Entrenamiento: 0.4311, Precisión de Entrenamiento: 0.8570\n",
      "Iteración 78741 - Lote 245/352 - Pérdida de Entrenamiento: 0.4326, Precisión de Entrenamiento: 0.8559\n",
      "Iteración 78776 - Lote 280/352 - Pérdida de Entrenamiento: 0.4295, Precisión de Entrenamiento: 0.8571\n",
      "Iteración 78811 - Lote 315/352 - Pérdida de Entrenamiento: 0.4274, Precisión de Entrenamiento: 0.8581\n",
      "Iteración 78846 - Lote 350/352 - Pérdida de Entrenamiento: 0.4282, Precisión de Entrenamiento: 0.8571\n",
      "Val loss: 0.4741, Val acc: 0.8370\n",
      "0.0001953125\n",
      "Epoch 225/400\n",
      "Iteración 78883 - Lote 35/352 - Pérdida de Entrenamiento: 0.4356, Precisión de Entrenamiento: 0.8554\n",
      "Iteración 78918 - Lote 70/352 - Pérdida de Entrenamiento: 0.4257, Precisión de Entrenamiento: 0.8566\n",
      "Iteración 78953 - Lote 105/352 - Pérdida de Entrenamiento: 0.4228, Precisión de Entrenamiento: 0.8588\n",
      "Iteración 78988 - Lote 140/352 - Pérdida de Entrenamiento: 0.4296, Precisión de Entrenamiento: 0.8565\n",
      "Iteración 79023 - Lote 175/352 - Pérdida de Entrenamiento: 0.4296, Precisión de Entrenamiento: 0.8562\n",
      "Iteración 79058 - Lote 210/352 - Pérdida de Entrenamiento: 0.4270, Precisión de Entrenamiento: 0.8568\n",
      "Iteración 79093 - Lote 245/352 - Pérdida de Entrenamiento: 0.4265, Precisión de Entrenamiento: 0.8571\n",
      "Iteración 79128 - Lote 280/352 - Pérdida de Entrenamiento: 0.4287, Precisión de Entrenamiento: 0.8567\n",
      "Iteración 79163 - Lote 315/352 - Pérdida de Entrenamiento: 0.4284, Precisión de Entrenamiento: 0.8562\n",
      "Iteración 79198 - Lote 350/352 - Pérdida de Entrenamiento: 0.4261, Precisión de Entrenamiento: 0.8564\n",
      "Val loss: 0.4714, Val acc: 0.8398\n",
      "Gradientes para features.0.0.weight: min=-0.1522141993045807, max=0.11171159893274307, mean=0.0019777731504291296, std=0.037226829677820206\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.16397428512573242, max=0.08216538280248642, mean=1.1729133575499873e-06, std=0.00342188379727304\n",
      "Gradientes para classifier.1.weight: min=-0.008797917515039444, max=0.01080812606960535, mean=0.0, std=0.0013799353037029505\n",
      "0.0001953125\n",
      "Epoch 226/400\n",
      "Iteración 79235 - Lote 35/352 - Pérdida de Entrenamiento: 0.4216, Precisión de Entrenamiento: 0.8650\n",
      "Iteración 79270 - Lote 70/352 - Pérdida de Entrenamiento: 0.4210, Precisión de Entrenamiento: 0.8625\n",
      "Iteración 79305 - Lote 105/352 - Pérdida de Entrenamiento: 0.4249, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 79340 - Lote 140/352 - Pérdida de Entrenamiento: 0.4278, Precisión de Entrenamiento: 0.8595\n",
      "Iteración 79375 - Lote 175/352 - Pérdida de Entrenamiento: 0.4280, Precisión de Entrenamiento: 0.8586\n",
      "Iteración 79410 - Lote 210/352 - Pérdida de Entrenamiento: 0.4294, Precisión de Entrenamiento: 0.8578\n",
      "Iteración 79445 - Lote 245/352 - Pérdida de Entrenamiento: 0.4262, Precisión de Entrenamiento: 0.8586\n",
      "Iteración 79480 - Lote 280/352 - Pérdida de Entrenamiento: 0.4244, Precisión de Entrenamiento: 0.8593\n",
      "Iteración 79515 - Lote 315/352 - Pérdida de Entrenamiento: 0.4256, Precisión de Entrenamiento: 0.8581\n",
      "Iteración 79550 - Lote 350/352 - Pérdida de Entrenamiento: 0.4236, Precisión de Entrenamiento: 0.8583\n",
      "Val loss: 0.4724, Val acc: 0.8388\n",
      "0.0001953125\n",
      "Epoch 227/400\n",
      "Iteración 79587 - Lote 35/352 - Pérdida de Entrenamiento: 0.4255, Precisión de Entrenamiento: 0.8663\n",
      "Iteración 79622 - Lote 70/352 - Pérdida de Entrenamiento: 0.4207, Precisión de Entrenamiento: 0.8615\n",
      "Iteración 79657 - Lote 105/352 - Pérdida de Entrenamiento: 0.4198, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 79692 - Lote 140/352 - Pérdida de Entrenamiento: 0.4168, Precisión de Entrenamiento: 0.8624\n",
      "Iteración 79727 - Lote 175/352 - Pérdida de Entrenamiento: 0.4207, Precisión de Entrenamiento: 0.8611\n",
      "Iteración 79762 - Lote 210/352 - Pérdida de Entrenamiento: 0.4249, Precisión de Entrenamiento: 0.8588\n",
      "Iteración 79797 - Lote 245/352 - Pérdida de Entrenamiento: 0.4249, Precisión de Entrenamiento: 0.8596\n",
      "Iteración 79832 - Lote 280/352 - Pérdida de Entrenamiento: 0.4253, Precisión de Entrenamiento: 0.8590\n",
      "Iteración 79867 - Lote 315/352 - Pérdida de Entrenamiento: 0.4264, Precisión de Entrenamiento: 0.8590\n",
      "Iteración 79902 - Lote 350/352 - Pérdida de Entrenamiento: 0.4268, Precisión de Entrenamiento: 0.8586\n",
      "Val loss: 0.4743, Val acc: 0.8386\n",
      "0.0001953125\n",
      "Epoch 228/400\n",
      "Iteración 79939 - Lote 35/352 - Pérdida de Entrenamiento: 0.4066, Precisión de Entrenamiento: 0.8679\n",
      "Iteración 79974 - Lote 70/352 - Pérdida de Entrenamiento: 0.4185, Precisión de Entrenamiento: 0.8634\n",
      "Iteración 80009 - Lote 105/352 - Pérdida de Entrenamiento: 0.4237, Precisión de Entrenamiento: 0.8617\n",
      "Iteración 80044 - Lote 140/352 - Pérdida de Entrenamiento: 0.4231, Precisión de Entrenamiento: 0.8609\n",
      "Iteración 80079 - Lote 175/352 - Pérdida de Entrenamiento: 0.4218, Precisión de Entrenamiento: 0.8619\n",
      "Iteración 80114 - Lote 210/352 - Pérdida de Entrenamiento: 0.4250, Precisión de Entrenamiento: 0.8602\n",
      "Iteración 80149 - Lote 245/352 - Pérdida de Entrenamiento: 0.4213, Precisión de Entrenamiento: 0.8607\n",
      "Iteración 80184 - Lote 280/352 - Pérdida de Entrenamiento: 0.4190, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 80219 - Lote 315/352 - Pérdida de Entrenamiento: 0.4185, Precisión de Entrenamiento: 0.8619\n",
      "Iteración 80254 - Lote 350/352 - Pérdida de Entrenamiento: 0.4209, Precisión de Entrenamiento: 0.8613\n",
      "Val loss: 0.4779, Val acc: 0.8372\n",
      "0.0001953125\n",
      "Epoch 229/400\n",
      "Iteración 80291 - Lote 35/352 - Pérdida de Entrenamiento: 0.4230, Precisión de Entrenamiento: 0.8623\n",
      "Iteración 80326 - Lote 70/352 - Pérdida de Entrenamiento: 0.4159, Precisión de Entrenamiento: 0.8632\n",
      "Iteración 80361 - Lote 105/352 - Pérdida de Entrenamiento: 0.4290, Precisión de Entrenamiento: 0.8579\n",
      "Iteración 80396 - Lote 140/352 - Pérdida de Entrenamiento: 0.4313, Precisión de Entrenamiento: 0.8574\n",
      "Iteración 80431 - Lote 175/352 - Pérdida de Entrenamiento: 0.4274, Precisión de Entrenamiento: 0.8583\n",
      "Iteración 80466 - Lote 210/352 - Pérdida de Entrenamiento: 0.4256, Precisión de Entrenamiento: 0.8592\n",
      "Iteración 80501 - Lote 245/352 - Pérdida de Entrenamiento: 0.4260, Precisión de Entrenamiento: 0.8584\n",
      "Iteración 80536 - Lote 280/352 - Pérdida de Entrenamiento: 0.4243, Precisión de Entrenamiento: 0.8592\n",
      "Iteración 80571 - Lote 315/352 - Pérdida de Entrenamiento: 0.4260, Precisión de Entrenamiento: 0.8587\n",
      "Iteración 80606 - Lote 350/352 - Pérdida de Entrenamiento: 0.4268, Precisión de Entrenamiento: 0.8581\n",
      "Val loss: 0.4744, Val acc: 0.8330\n",
      "0.0001953125\n",
      "Epoch 230/400\n",
      "Iteración 80643 - Lote 35/352 - Pérdida de Entrenamiento: 0.4179, Precisión de Entrenamiento: 0.8594\n",
      "Iteración 80678 - Lote 70/352 - Pérdida de Entrenamiento: 0.4262, Precisión de Entrenamiento: 0.8547\n",
      "Iteración 80713 - Lote 105/352 - Pérdida de Entrenamiento: 0.4241, Precisión de Entrenamiento: 0.8588\n",
      "Iteración 80748 - Lote 140/352 - Pérdida de Entrenamiento: 0.4265, Precisión de Entrenamiento: 0.8588\n",
      "Iteración 80783 - Lote 175/352 - Pérdida de Entrenamiento: 0.4255, Precisión de Entrenamiento: 0.8576\n",
      "Iteración 80818 - Lote 210/352 - Pérdida de Entrenamiento: 0.4263, Precisión de Entrenamiento: 0.8584\n",
      "Iteración 80853 - Lote 245/352 - Pérdida de Entrenamiento: 0.4271, Precisión de Entrenamiento: 0.8572\n",
      "Iteración 80888 - Lote 280/352 - Pérdida de Entrenamiento: 0.4241, Precisión de Entrenamiento: 0.8578\n",
      "Iteración 80923 - Lote 315/352 - Pérdida de Entrenamiento: 0.4239, Precisión de Entrenamiento: 0.8582\n",
      "Iteración 80958 - Lote 350/352 - Pérdida de Entrenamiento: 0.4237, Precisión de Entrenamiento: 0.8580\n",
      "Val loss: 0.4771, Val acc: 0.8382\n",
      "Gradientes para features.0.0.weight: min=-0.24714165925979614, max=0.20732614398002625, mean=-0.002764759585261345, std=0.050811585038900375\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.16433025896549225, max=0.11999868601560593, mean=1.3836727703164797e-05, std=0.0032305894419550896\n",
      "Gradientes para classifier.1.weight: min=-0.01445235125720501, max=0.009968304075300694, mean=0.0, std=0.0015964880585670471\n",
      "0.0001953125\n",
      "Epoch 231/400\n",
      "Iteración 80995 - Lote 35/352 - Pérdida de Entrenamiento: 0.4121, Precisión de Entrenamiento: 0.8679\n",
      "Iteración 81030 - Lote 70/352 - Pérdida de Entrenamiento: 0.4145, Precisión de Entrenamiento: 0.8637\n",
      "Iteración 81065 - Lote 105/352 - Pérdida de Entrenamiento: 0.4202, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 81100 - Lote 140/352 - Pérdida de Entrenamiento: 0.4180, Precisión de Entrenamiento: 0.8629\n",
      "Iteración 81135 - Lote 175/352 - Pérdida de Entrenamiento: 0.4248, Precisión de Entrenamiento: 0.8596\n",
      "Iteración 81170 - Lote 210/352 - Pérdida de Entrenamiento: 0.4270, Precisión de Entrenamiento: 0.8588\n",
      "Iteración 81205 - Lote 245/352 - Pérdida de Entrenamiento: 0.4253, Precisión de Entrenamiento: 0.8592\n",
      "Iteración 81240 - Lote 280/352 - Pérdida de Entrenamiento: 0.4248, Precisión de Entrenamiento: 0.8596\n",
      "Iteración 81275 - Lote 315/352 - Pérdida de Entrenamiento: 0.4246, Precisión de Entrenamiento: 0.8597\n",
      "Iteración 81310 - Lote 350/352 - Pérdida de Entrenamiento: 0.4242, Precisión de Entrenamiento: 0.8600\n",
      "Val loss: 0.4772, Val acc: 0.8368\n",
      "0.0001953125\n",
      "Epoch 232/400\n",
      "Iteración 81347 - Lote 35/352 - Pérdida de Entrenamiento: 0.4228, Precisión de Entrenamiento: 0.8596\n",
      "Iteración 81382 - Lote 70/352 - Pérdida de Entrenamiento: 0.4272, Precisión de Entrenamiento: 0.8592\n",
      "Iteración 81417 - Lote 105/352 - Pérdida de Entrenamiento: 0.4268, Precisión de Entrenamiento: 0.8583\n",
      "Iteración 81452 - Lote 140/352 - Pérdida de Entrenamiento: 0.4248, Precisión de Entrenamiento: 0.8582\n",
      "Iteración 81487 - Lote 175/352 - Pérdida de Entrenamiento: 0.4205, Precisión de Entrenamiento: 0.8599\n",
      "Iteración 81522 - Lote 210/352 - Pérdida de Entrenamiento: 0.4254, Precisión de Entrenamiento: 0.8591\n",
      "Iteración 81557 - Lote 245/352 - Pérdida de Entrenamiento: 0.4259, Precisión de Entrenamiento: 0.8584\n",
      "Iteración 81592 - Lote 280/352 - Pérdida de Entrenamiento: 0.4242, Precisión de Entrenamiento: 0.8589\n",
      "Iteración 81627 - Lote 315/352 - Pérdida de Entrenamiento: 0.4238, Precisión de Entrenamiento: 0.8588\n",
      "Iteración 81662 - Lote 350/352 - Pérdida de Entrenamiento: 0.4246, Precisión de Entrenamiento: 0.8583\n",
      "Val loss: 0.4760, Val acc: 0.8378\n",
      "9.765625e-05\n",
      "Epoch 233/400\n",
      "Iteración 81699 - Lote 35/352 - Pérdida de Entrenamiento: 0.4003, Precisión de Entrenamiento: 0.8661\n",
      "Iteración 81734 - Lote 70/352 - Pérdida de Entrenamiento: 0.4065, Precisión de Entrenamiento: 0.8638\n",
      "Iteración 81769 - Lote 105/352 - Pérdida de Entrenamiento: 0.4187, Precisión de Entrenamiento: 0.8606\n",
      "Iteración 81804 - Lote 140/352 - Pérdida de Entrenamiento: 0.4183, Precisión de Entrenamiento: 0.8598\n",
      "Iteración 81839 - Lote 175/352 - Pérdida de Entrenamiento: 0.4216, Precisión de Entrenamiento: 0.8588\n",
      "Iteración 81874 - Lote 210/352 - Pérdida de Entrenamiento: 0.4206, Precisión de Entrenamiento: 0.8584\n",
      "Iteración 81909 - Lote 245/352 - Pérdida de Entrenamiento: 0.4226, Precisión de Entrenamiento: 0.8583\n",
      "Iteración 81944 - Lote 280/352 - Pérdida de Entrenamiento: 0.4203, Precisión de Entrenamiento: 0.8593\n",
      "Iteración 81979 - Lote 315/352 - Pérdida de Entrenamiento: 0.4194, Precisión de Entrenamiento: 0.8596\n",
      "Iteración 82014 - Lote 350/352 - Pérdida de Entrenamiento: 0.4191, Precisión de Entrenamiento: 0.8596\n",
      "Val loss: 0.4746, Val acc: 0.8392\n",
      "9.765625e-05\n",
      "Epoch 234/400\n",
      "Iteración 82051 - Lote 35/352 - Pérdida de Entrenamiento: 0.4186, Precisión de Entrenamiento: 0.8607\n",
      "Iteración 82086 - Lote 70/352 - Pérdida de Entrenamiento: 0.4155, Precisión de Entrenamiento: 0.8628\n",
      "Iteración 82121 - Lote 105/352 - Pérdida de Entrenamiento: 0.4124, Precisión de Entrenamiento: 0.8644\n",
      "Iteración 82156 - Lote 140/352 - Pérdida de Entrenamiento: 0.4150, Precisión de Entrenamiento: 0.8628\n",
      "Iteración 82191 - Lote 175/352 - Pérdida de Entrenamiento: 0.4173, Precisión de Entrenamiento: 0.8604\n",
      "Iteración 82226 - Lote 210/352 - Pérdida de Entrenamiento: 0.4200, Precisión de Entrenamiento: 0.8592\n",
      "Iteración 82261 - Lote 245/352 - Pérdida de Entrenamiento: 0.4216, Precisión de Entrenamiento: 0.8586\n",
      "Iteración 82296 - Lote 280/352 - Pérdida de Entrenamiento: 0.4219, Precisión de Entrenamiento: 0.8582\n",
      "Iteración 82331 - Lote 315/352 - Pérdida de Entrenamiento: 0.4240, Precisión de Entrenamiento: 0.8576\n",
      "Iteración 82366 - Lote 350/352 - Pérdida de Entrenamiento: 0.4233, Precisión de Entrenamiento: 0.8580\n",
      "Val loss: 0.4738, Val acc: 0.8394\n",
      "9.765625e-05\n",
      "Epoch 235/400\n",
      "Iteración 82403 - Lote 35/352 - Pérdida de Entrenamiento: 0.4239, Precisión de Entrenamiento: 0.8518\n",
      "Iteración 82438 - Lote 70/352 - Pérdida de Entrenamiento: 0.4300, Precisión de Entrenamiento: 0.8527\n",
      "Iteración 82473 - Lote 105/352 - Pérdida de Entrenamiento: 0.4227, Precisión de Entrenamiento: 0.8580\n",
      "Iteración 82508 - Lote 140/352 - Pérdida de Entrenamiento: 0.4214, Precisión de Entrenamiento: 0.8592\n",
      "Iteración 82543 - Lote 175/352 - Pérdida de Entrenamiento: 0.4225, Precisión de Entrenamiento: 0.8576\n",
      "Iteración 82578 - Lote 210/352 - Pérdida de Entrenamiento: 0.4229, Precisión de Entrenamiento: 0.8578\n",
      "Iteración 82613 - Lote 245/352 - Pérdida de Entrenamiento: 0.4205, Precisión de Entrenamiento: 0.8596\n",
      "Iteración 82648 - Lote 280/352 - Pérdida de Entrenamiento: 0.4190, Precisión de Entrenamiento: 0.8602\n",
      "Iteración 82683 - Lote 315/352 - Pérdida de Entrenamiento: 0.4169, Precisión de Entrenamiento: 0.8609\n",
      "Iteración 82718 - Lote 350/352 - Pérdida de Entrenamiento: 0.4181, Precisión de Entrenamiento: 0.8608\n",
      "Val loss: 0.4718, Val acc: 0.8390\n",
      "Gradientes para features.0.0.weight: min=-0.18195214867591858, max=0.27185818552970886, mean=-0.0019357121782377362, std=0.060664694756269455\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.09748885035514832, max=0.15510275959968567, mean=6.86468547428376e-06, std=0.003732549026608467\n",
      "Gradientes para classifier.1.weight: min=-0.015547037124633789, max=0.01793752796947956, mean=0.0, std=0.0019886346999555826\n",
      "9.765625e-05\n",
      "Epoch 236/400\n",
      "Iteración 82755 - Lote 35/352 - Pérdida de Entrenamiento: 0.3905, Precisión de Entrenamiento: 0.8627\n",
      "Iteración 82790 - Lote 70/352 - Pérdida de Entrenamiento: 0.4152, Precisión de Entrenamiento: 0.8578\n",
      "Iteración 82825 - Lote 105/352 - Pérdida de Entrenamiento: 0.4181, Precisión de Entrenamiento: 0.8582\n",
      "Iteración 82860 - Lote 140/352 - Pérdida de Entrenamiento: 0.4197, Precisión de Entrenamiento: 0.8579\n",
      "Iteración 82895 - Lote 175/352 - Pérdida de Entrenamiento: 0.4237, Precisión de Entrenamiento: 0.8569\n",
      "Iteración 82930 - Lote 210/352 - Pérdida de Entrenamiento: 0.4213, Precisión de Entrenamiento: 0.8586\n",
      "Iteración 82965 - Lote 245/352 - Pérdida de Entrenamiento: 0.4189, Precisión de Entrenamiento: 0.8601\n",
      "Iteración 83000 - Lote 280/352 - Pérdida de Entrenamiento: 0.4171, Precisión de Entrenamiento: 0.8606\n",
      "Iteración 83035 - Lote 315/352 - Pérdida de Entrenamiento: 0.4184, Precisión de Entrenamiento: 0.8604\n",
      "Iteración 83070 - Lote 350/352 - Pérdida de Entrenamiento: 0.4180, Precisión de Entrenamiento: 0.8605\n",
      "Val loss: 0.4723, Val acc: 0.8388\n",
      "9.765625e-05\n",
      "Epoch 237/400\n",
      "Iteración 83107 - Lote 35/352 - Pérdida de Entrenamiento: 0.4142, Precisión de Entrenamiento: 0.8603\n",
      "Iteración 83142 - Lote 70/352 - Pérdida de Entrenamiento: 0.4301, Precisión de Entrenamiento: 0.8542\n",
      "Iteración 83177 - Lote 105/352 - Pérdida de Entrenamiento: 0.4231, Precisión de Entrenamiento: 0.8575\n",
      "Iteración 83212 - Lote 140/352 - Pérdida de Entrenamiento: 0.4213, Precisión de Entrenamiento: 0.8577\n",
      "Iteración 83247 - Lote 175/352 - Pérdida de Entrenamiento: 0.4199, Precisión de Entrenamiento: 0.8591\n",
      "Iteración 83282 - Lote 210/352 - Pérdida de Entrenamiento: 0.4219, Precisión de Entrenamiento: 0.8580\n",
      "Iteración 83317 - Lote 245/352 - Pérdida de Entrenamiento: 0.4189, Precisión de Entrenamiento: 0.8593\n",
      "Iteración 83352 - Lote 280/352 - Pérdida de Entrenamiento: 0.4184, Precisión de Entrenamiento: 0.8591\n",
      "Iteración 83387 - Lote 315/352 - Pérdida de Entrenamiento: 0.4210, Precisión de Entrenamiento: 0.8577\n",
      "Iteración 83422 - Lote 350/352 - Pérdida de Entrenamiento: 0.4207, Precisión de Entrenamiento: 0.8579\n",
      "Val loss: 0.4721, Val acc: 0.8386\n",
      "9.765625e-05\n",
      "Epoch 238/400\n",
      "Iteración 83459 - Lote 35/352 - Pérdida de Entrenamiento: 0.4205, Precisión de Entrenamiento: 0.8571\n",
      "Iteración 83494 - Lote 70/352 - Pérdida de Entrenamiento: 0.4303, Precisión de Entrenamiento: 0.8548\n",
      "Iteración 83529 - Lote 105/352 - Pérdida de Entrenamiento: 0.4251, Precisión de Entrenamiento: 0.8565\n",
      "Iteración 83564 - Lote 140/352 - Pérdida de Entrenamiento: 0.4286, Precisión de Entrenamiento: 0.8561\n",
      "Iteración 83599 - Lote 175/352 - Pérdida de Entrenamiento: 0.4256, Precisión de Entrenamiento: 0.8573\n",
      "Iteración 83634 - Lote 210/352 - Pérdida de Entrenamiento: 0.4275, Precisión de Entrenamiento: 0.8555\n",
      "Iteración 83669 - Lote 245/352 - Pérdida de Entrenamiento: 0.4272, Precisión de Entrenamiento: 0.8559\n",
      "Iteración 83704 - Lote 280/352 - Pérdida de Entrenamiento: 0.4267, Precisión de Entrenamiento: 0.8566\n",
      "Iteración 83739 - Lote 315/352 - Pérdida de Entrenamiento: 0.4269, Precisión de Entrenamiento: 0.8573\n",
      "Iteración 83774 - Lote 350/352 - Pérdida de Entrenamiento: 0.4266, Precisión de Entrenamiento: 0.8581\n",
      "Val loss: 0.4713, Val acc: 0.8406\n",
      "9.765625e-05\n",
      "Epoch 239/400\n",
      "Iteración 83811 - Lote 35/352 - Pérdida de Entrenamiento: 0.4088, Precisión de Entrenamiento: 0.8634\n",
      "Iteración 83846 - Lote 70/352 - Pérdida de Entrenamiento: 0.4274, Precisión de Entrenamiento: 0.8570\n",
      "Iteración 83881 - Lote 105/352 - Pérdida de Entrenamiento: 0.4245, Precisión de Entrenamiento: 0.8569\n",
      "Iteración 83916 - Lote 140/352 - Pérdida de Entrenamiento: 0.4209, Precisión de Entrenamiento: 0.8581\n",
      "Iteración 83951 - Lote 175/352 - Pérdida de Entrenamiento: 0.4231, Precisión de Entrenamiento: 0.8593\n",
      "Iteración 83986 - Lote 210/352 - Pérdida de Entrenamiento: 0.4233, Precisión de Entrenamiento: 0.8602\n",
      "Iteración 84021 - Lote 245/352 - Pérdida de Entrenamiento: 0.4201, Precisión de Entrenamiento: 0.8614\n",
      "Iteración 84056 - Lote 280/352 - Pérdida de Entrenamiento: 0.4198, Precisión de Entrenamiento: 0.8610\n",
      "Iteración 84091 - Lote 315/352 - Pérdida de Entrenamiento: 0.4206, Precisión de Entrenamiento: 0.8610\n",
      "Iteración 84126 - Lote 350/352 - Pérdida de Entrenamiento: 0.4195, Precisión de Entrenamiento: 0.8614\n",
      "Val loss: 0.4761, Val acc: 0.8406\n",
      "9.765625e-05\n",
      "Epoch 240/400\n",
      "Iteración 84163 - Lote 35/352 - Pérdida de Entrenamiento: 0.4272, Precisión de Entrenamiento: 0.8594\n",
      "Iteración 84198 - Lote 70/352 - Pérdida de Entrenamiento: 0.4317, Precisión de Entrenamiento: 0.8578\n",
      "Iteración 84233 - Lote 105/352 - Pérdida de Entrenamiento: 0.4208, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 84268 - Lote 140/352 - Pérdida de Entrenamiento: 0.4177, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 84303 - Lote 175/352 - Pérdida de Entrenamiento: 0.4166, Precisión de Entrenamiento: 0.8617\n",
      "Iteración 84338 - Lote 210/352 - Pérdida de Entrenamiento: 0.4169, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 84373 - Lote 245/352 - Pérdida de Entrenamiento: 0.4148, Precisión de Entrenamiento: 0.8616\n",
      "Iteración 84408 - Lote 280/352 - Pérdida de Entrenamiento: 0.4183, Precisión de Entrenamiento: 0.8609\n",
      "Iteración 84443 - Lote 315/352 - Pérdida de Entrenamiento: 0.4177, Precisión de Entrenamiento: 0.8611\n",
      "Iteración 84478 - Lote 350/352 - Pérdida de Entrenamiento: 0.4181, Precisión de Entrenamiento: 0.8610\n",
      "Val loss: 0.4704, Val acc: 0.8412\n",
      "Gradientes para features.0.0.weight: min=-0.21754387021064758, max=0.22222435474395752, mean=0.004783590789884329, std=0.06156026944518089\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.16011612117290497, max=0.21587812900543213, mean=1.984433538382291e-06, std=0.005683469120413065\n",
      "Gradientes para classifier.1.weight: min=-0.026956120505928993, max=0.01427745632827282, mean=4.6566129424663316e-11, std=0.002604203997179866\n",
      "9.765625e-05\n",
      "Epoch 241/400\n",
      "Iteración 84515 - Lote 35/352 - Pérdida de Entrenamiento: 0.3938, Precisión de Entrenamiento: 0.8734\n",
      "Iteración 84550 - Lote 70/352 - Pérdida de Entrenamiento: 0.4060, Precisión de Entrenamiento: 0.8662\n",
      "Iteración 84585 - Lote 105/352 - Pérdida de Entrenamiento: 0.4055, Precisión de Entrenamiento: 0.8671\n",
      "Iteración 84620 - Lote 140/352 - Pérdida de Entrenamiento: 0.4130, Precisión de Entrenamiento: 0.8634\n",
      "Iteración 84655 - Lote 175/352 - Pérdida de Entrenamiento: 0.4167, Precisión de Entrenamiento: 0.8622\n",
      "Iteración 84690 - Lote 210/352 - Pérdida de Entrenamiento: 0.4169, Precisión de Entrenamiento: 0.8619\n",
      "Iteración 84725 - Lote 245/352 - Pérdida de Entrenamiento: 0.4147, Precisión de Entrenamiento: 0.8626\n",
      "Iteración 84760 - Lote 280/352 - Pérdida de Entrenamiento: 0.4168, Precisión de Entrenamiento: 0.8620\n",
      "Iteración 84795 - Lote 315/352 - Pérdida de Entrenamiento: 0.4166, Precisión de Entrenamiento: 0.8620\n",
      "Iteración 84830 - Lote 350/352 - Pérdida de Entrenamiento: 0.4181, Precisión de Entrenamiento: 0.8614\n",
      "Val loss: 0.4713, Val acc: 0.8406\n",
      "4.8828125e-05\n",
      "Epoch 242/400\n",
      "Iteración 84867 - Lote 35/352 - Pérdida de Entrenamiento: 0.4157, Precisión de Entrenamiento: 0.8634\n",
      "Iteración 84902 - Lote 70/352 - Pérdida de Entrenamiento: 0.4218, Precisión de Entrenamiento: 0.8595\n",
      "Iteración 84937 - Lote 105/352 - Pérdida de Entrenamiento: 0.4154, Precisión de Entrenamiento: 0.8610\n",
      "Iteración 84972 - Lote 140/352 - Pérdida de Entrenamiento: 0.4196, Precisión de Entrenamiento: 0.8599\n",
      "Iteración 85007 - Lote 175/352 - Pérdida de Entrenamiento: 0.4187, Precisión de Entrenamiento: 0.8595\n",
      "Iteración 85042 - Lote 210/352 - Pérdida de Entrenamiento: 0.4175, Precisión de Entrenamiento: 0.8605\n",
      "Iteración 85077 - Lote 245/352 - Pérdida de Entrenamiento: 0.4167, Precisión de Entrenamiento: 0.8611\n",
      "Iteración 85112 - Lote 280/352 - Pérdida de Entrenamiento: 0.4142, Precisión de Entrenamiento: 0.8616\n",
      "Iteración 85147 - Lote 315/352 - Pérdida de Entrenamiento: 0.4153, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 85182 - Lote 350/352 - Pérdida de Entrenamiento: 0.4140, Precisión de Entrenamiento: 0.8618\n",
      "Val loss: 0.4706, Val acc: 0.8398\n",
      "4.8828125e-05\n",
      "Epoch 243/400\n",
      "Iteración 85219 - Lote 35/352 - Pérdida de Entrenamiento: 0.4220, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 85254 - Lote 70/352 - Pérdida de Entrenamiento: 0.4202, Precisión de Entrenamiento: 0.8616\n",
      "Iteración 85289 - Lote 105/352 - Pérdida de Entrenamiento: 0.4118, Precisión de Entrenamiento: 0.8643\n",
      "Iteración 85324 - Lote 140/352 - Pérdida de Entrenamiento: 0.4168, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 85359 - Lote 175/352 - Pérdida de Entrenamiento: 0.4188, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 85394 - Lote 210/352 - Pérdida de Entrenamiento: 0.4174, Precisión de Entrenamiento: 0.8605\n",
      "Iteración 85429 - Lote 245/352 - Pérdida de Entrenamiento: 0.4178, Precisión de Entrenamiento: 0.8607\n",
      "Iteración 85464 - Lote 280/352 - Pérdida de Entrenamiento: 0.4173, Precisión de Entrenamiento: 0.8606\n",
      "Iteración 85499 - Lote 315/352 - Pérdida de Entrenamiento: 0.4177, Precisión de Entrenamiento: 0.8609\n",
      "Iteración 85534 - Lote 350/352 - Pérdida de Entrenamiento: 0.4178, Precisión de Entrenamiento: 0.8611\n",
      "Val loss: 0.4738, Val acc: 0.8406\n",
      "4.8828125e-05\n",
      "Epoch 244/400\n",
      "Iteración 85571 - Lote 35/352 - Pérdida de Entrenamiento: 0.4340, Precisión de Entrenamiento: 0.8603\n",
      "Iteración 85606 - Lote 70/352 - Pérdida de Entrenamiento: 0.4198, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 85641 - Lote 105/352 - Pérdida de Entrenamiento: 0.4108, Precisión de Entrenamiento: 0.8657\n",
      "Iteración 85676 - Lote 140/352 - Pérdida de Entrenamiento: 0.4092, Precisión de Entrenamiento: 0.8655\n",
      "Iteración 85711 - Lote 175/352 - Pérdida de Entrenamiento: 0.4084, Precisión de Entrenamiento: 0.8660\n",
      "Iteración 85746 - Lote 210/352 - Pérdida de Entrenamiento: 0.4108, Precisión de Entrenamiento: 0.8645\n",
      "Iteración 85781 - Lote 245/352 - Pérdida de Entrenamiento: 0.4126, Precisión de Entrenamiento: 0.8631\n",
      "Iteración 85816 - Lote 280/352 - Pérdida de Entrenamiento: 0.4166, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 85851 - Lote 315/352 - Pérdida de Entrenamiento: 0.4167, Precisión de Entrenamiento: 0.8620\n",
      "Iteración 85886 - Lote 350/352 - Pérdida de Entrenamiento: 0.4151, Precisión de Entrenamiento: 0.8627\n",
      "Val loss: 0.4677, Val acc: 0.8422\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_244.pth\n",
      "Checkpoint del mejor modelo guardado en la época 244\n",
      "4.8828125e-05\n",
      "Epoch 245/400\n",
      "Iteración 85923 - Lote 35/352 - Pérdida de Entrenamiento: 0.4201, Precisión de Entrenamiento: 0.8598\n",
      "Iteración 85958 - Lote 70/352 - Pérdida de Entrenamiento: 0.4116, Precisión de Entrenamiento: 0.8648\n",
      "Iteración 85993 - Lote 105/352 - Pérdida de Entrenamiento: 0.4128, Precisión de Entrenamiento: 0.8636\n",
      "Iteración 86028 - Lote 140/352 - Pérdida de Entrenamiento: 0.4185, Precisión de Entrenamiento: 0.8610\n",
      "Iteración 86063 - Lote 175/352 - Pérdida de Entrenamiento: 0.4175, Precisión de Entrenamiento: 0.8609\n",
      "Iteración 86098 - Lote 210/352 - Pérdida de Entrenamiento: 0.4178, Precisión de Entrenamiento: 0.8607\n",
      "Iteración 86133 - Lote 245/352 - Pérdida de Entrenamiento: 0.4180, Precisión de Entrenamiento: 0.8605\n",
      "Iteración 86168 - Lote 280/352 - Pérdida de Entrenamiento: 0.4179, Precisión de Entrenamiento: 0.8607\n",
      "Iteración 86203 - Lote 315/352 - Pérdida de Entrenamiento: 0.4142, Precisión de Entrenamiento: 0.8617\n",
      "Iteración 86238 - Lote 350/352 - Pérdida de Entrenamiento: 0.4145, Precisión de Entrenamiento: 0.8614\n",
      "Val loss: 0.4714, Val acc: 0.8400\n",
      "Gradientes para features.0.0.weight: min=-0.21538208425045013, max=0.0998634323477745, mean=-0.00449311314150691, std=0.04040791839361191\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.09624572843313217, max=0.10069083422422409, mean=5.919578597968211e-06, std=0.0037015352863818407\n",
      "Gradientes para classifier.1.weight: min=-0.008233643136918545, max=0.006224100943654776, mean=1.3969838306981952e-11, std=0.0011465644929558039\n",
      "4.8828125e-05\n",
      "Epoch 246/400\n",
      "Iteración 86275 - Lote 35/352 - Pérdida de Entrenamiento: 0.3790, Precisión de Entrenamiento: 0.8743\n",
      "Iteración 86310 - Lote 70/352 - Pérdida de Entrenamiento: 0.4022, Precisión de Entrenamiento: 0.8648\n",
      "Iteración 86345 - Lote 105/352 - Pérdida de Entrenamiento: 0.4047, Precisión de Entrenamiento: 0.8640\n",
      "Iteración 86380 - Lote 140/352 - Pérdida de Entrenamiento: 0.4077, Precisión de Entrenamiento: 0.8639\n",
      "Iteración 86415 - Lote 175/352 - Pérdida de Entrenamiento: 0.4113, Precisión de Entrenamiento: 0.8637\n",
      "Iteración 86450 - Lote 210/352 - Pérdida de Entrenamiento: 0.4139, Precisión de Entrenamiento: 0.8630\n",
      "Iteración 86485 - Lote 245/352 - Pérdida de Entrenamiento: 0.4118, Precisión de Entrenamiento: 0.8640\n",
      "Iteración 86520 - Lote 280/352 - Pérdida de Entrenamiento: 0.4153, Precisión de Entrenamiento: 0.8626\n",
      "Iteración 86555 - Lote 315/352 - Pérdida de Entrenamiento: 0.4156, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 86590 - Lote 350/352 - Pérdida de Entrenamiento: 0.4130, Precisión de Entrenamiento: 0.8627\n",
      "Val loss: 0.4722, Val acc: 0.8382\n",
      "4.8828125e-05\n",
      "Epoch 247/400\n",
      "Iteración 86627 - Lote 35/352 - Pérdida de Entrenamiento: 0.4192, Precisión de Entrenamiento: 0.8558\n",
      "Iteración 86662 - Lote 70/352 - Pérdida de Entrenamiento: 0.4099, Precisión de Entrenamiento: 0.8615\n",
      "Iteración 86697 - Lote 105/352 - Pérdida de Entrenamiento: 0.4141, Precisión de Entrenamiento: 0.8600\n",
      "Iteración 86732 - Lote 140/352 - Pérdida de Entrenamiento: 0.4166, Precisión de Entrenamiento: 0.8598\n",
      "Iteración 86767 - Lote 175/352 - Pérdida de Entrenamiento: 0.4139, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 86802 - Lote 210/352 - Pérdida de Entrenamiento: 0.4114, Precisión de Entrenamiento: 0.8627\n",
      "Iteración 86837 - Lote 245/352 - Pérdida de Entrenamiento: 0.4128, Precisión de Entrenamiento: 0.8620\n",
      "Iteración 86872 - Lote 280/352 - Pérdida de Entrenamiento: 0.4128, Precisión de Entrenamiento: 0.8620\n",
      "Iteración 86907 - Lote 315/352 - Pérdida de Entrenamiento: 0.4138, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 86942 - Lote 350/352 - Pérdida de Entrenamiento: 0.4164, Precisión de Entrenamiento: 0.8606\n",
      "Val loss: 0.4705, Val acc: 0.8426\n",
      "4.8828125e-05\n",
      "Epoch 248/400\n",
      "Iteración 86979 - Lote 35/352 - Pérdida de Entrenamiento: 0.4174, Precisión de Entrenamiento: 0.8587\n",
      "Iteración 87014 - Lote 70/352 - Pérdida de Entrenamiento: 0.4222, Precisión de Entrenamiento: 0.8589\n",
      "Iteración 87049 - Lote 105/352 - Pérdida de Entrenamiento: 0.4232, Precisión de Entrenamiento: 0.8580\n",
      "Iteración 87084 - Lote 140/352 - Pérdida de Entrenamiento: 0.4182, Precisión de Entrenamiento: 0.8576\n",
      "Iteración 87119 - Lote 175/352 - Pérdida de Entrenamiento: 0.4167, Precisión de Entrenamiento: 0.8593\n",
      "Iteración 87154 - Lote 210/352 - Pérdida de Entrenamiento: 0.4156, Precisión de Entrenamiento: 0.8611\n",
      "Iteración 87189 - Lote 245/352 - Pérdida de Entrenamiento: 0.4158, Precisión de Entrenamiento: 0.8619\n",
      "Iteración 87224 - Lote 280/352 - Pérdida de Entrenamiento: 0.4170, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 87259 - Lote 315/352 - Pérdida de Entrenamiento: 0.4166, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 87294 - Lote 350/352 - Pérdida de Entrenamiento: 0.4180, Precisión de Entrenamiento: 0.8612\n",
      "Val loss: 0.4735, Val acc: 0.8382\n",
      "4.8828125e-05\n",
      "Epoch 249/400\n",
      "Iteración 87331 - Lote 35/352 - Pérdida de Entrenamiento: 0.4292, Precisión de Entrenamiento: 0.8578\n",
      "Iteración 87366 - Lote 70/352 - Pérdida de Entrenamiento: 0.4140, Precisión de Entrenamiento: 0.8614\n",
      "Iteración 87401 - Lote 105/352 - Pérdida de Entrenamiento: 0.4215, Precisión de Entrenamiento: 0.8599\n",
      "Iteración 87436 - Lote 140/352 - Pérdida de Entrenamiento: 0.4187, Precisión de Entrenamiento: 0.8592\n",
      "Iteración 87471 - Lote 175/352 - Pérdida de Entrenamiento: 0.4195, Precisión de Entrenamiento: 0.8595\n",
      "Iteración 87506 - Lote 210/352 - Pérdida de Entrenamiento: 0.4187, Precisión de Entrenamiento: 0.8593\n",
      "Iteración 87541 - Lote 245/352 - Pérdida de Entrenamiento: 0.4196, Precisión de Entrenamiento: 0.8594\n",
      "Iteración 87576 - Lote 280/352 - Pérdida de Entrenamiento: 0.4190, Precisión de Entrenamiento: 0.8584\n",
      "Iteración 87611 - Lote 315/352 - Pérdida de Entrenamiento: 0.4178, Precisión de Entrenamiento: 0.8589\n",
      "Iteración 87646 - Lote 350/352 - Pérdida de Entrenamiento: 0.4187, Precisión de Entrenamiento: 0.8590\n",
      "Val loss: 0.4700, Val acc: 0.8384\n",
      "4.8828125e-05\n",
      "Epoch 250/400\n",
      "Iteración 87683 - Lote 35/352 - Pérdida de Entrenamiento: 0.4153, Precisión de Entrenamiento: 0.8569\n",
      "Iteración 87718 - Lote 70/352 - Pérdida de Entrenamiento: 0.4264, Precisión de Entrenamiento: 0.8573\n",
      "Iteración 87753 - Lote 105/352 - Pérdida de Entrenamiento: 0.4240, Precisión de Entrenamiento: 0.8575\n",
      "Iteración 87788 - Lote 140/352 - Pérdida de Entrenamiento: 0.4185, Precisión de Entrenamiento: 0.8601\n",
      "Iteración 87823 - Lote 175/352 - Pérdida de Entrenamiento: 0.4191, Precisión de Entrenamiento: 0.8593\n",
      "Iteración 87858 - Lote 210/352 - Pérdida de Entrenamiento: 0.4178, Precisión de Entrenamiento: 0.8594\n",
      "Iteración 87893 - Lote 245/352 - Pérdida de Entrenamiento: 0.4164, Precisión de Entrenamiento: 0.8599\n",
      "Iteración 87928 - Lote 280/352 - Pérdida de Entrenamiento: 0.4179, Precisión de Entrenamiento: 0.8596\n",
      "Iteración 87963 - Lote 315/352 - Pérdida de Entrenamiento: 0.4173, Precisión de Entrenamiento: 0.8596\n",
      "Iteración 87998 - Lote 350/352 - Pérdida de Entrenamiento: 0.4179, Precisión de Entrenamiento: 0.8594\n",
      "Val loss: 0.4714, Val acc: 0.8390\n",
      "Gradientes para features.0.0.weight: min=-0.18290261924266815, max=0.21459020674228668, mean=0.004674441181123257, std=0.05155634880065918\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.09539154917001724, max=0.11583606898784637, mean=-4.261026788299205e-06, std=0.003617687849327922\n",
      "Gradientes para classifier.1.weight: min=-0.010745367035269737, max=0.012060861103236675, mean=-1.8626451075975936e-11, std=0.0014144096057862043\n",
      "4.8828125e-05\n",
      "Epoch 251/400\n",
      "Iteración 88035 - Lote 35/352 - Pérdida de Entrenamiento: 0.4246, Precisión de Entrenamiento: 0.8558\n",
      "Iteración 88070 - Lote 70/352 - Pérdida de Entrenamiento: 0.4138, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 88105 - Lote 105/352 - Pérdida de Entrenamiento: 0.4163, Precisión de Entrenamiento: 0.8624\n",
      "Iteración 88140 - Lote 140/352 - Pérdida de Entrenamiento: 0.4177, Precisión de Entrenamiento: 0.8604\n",
      "Iteración 88175 - Lote 175/352 - Pérdida de Entrenamiento: 0.4163, Precisión de Entrenamiento: 0.8599\n",
      "Iteración 88210 - Lote 210/352 - Pérdida de Entrenamiento: 0.4174, Precisión de Entrenamiento: 0.8591\n",
      "Iteración 88245 - Lote 245/352 - Pérdida de Entrenamiento: 0.4133, Precisión de Entrenamiento: 0.8602\n",
      "Iteración 88280 - Lote 280/352 - Pérdida de Entrenamiento: 0.4121, Precisión de Entrenamiento: 0.8605\n",
      "Iteración 88315 - Lote 315/352 - Pérdida de Entrenamiento: 0.4124, Precisión de Entrenamiento: 0.8605\n",
      "Iteración 88350 - Lote 350/352 - Pérdida de Entrenamiento: 0.4131, Precisión de Entrenamiento: 0.8602\n",
      "Val loss: 0.4707, Val acc: 0.8388\n",
      "4.8828125e-05\n",
      "Epoch 252/400\n",
      "Iteración 88387 - Lote 35/352 - Pérdida de Entrenamiento: 0.4259, Precisión de Entrenamiento: 0.8583\n",
      "Iteración 88422 - Lote 70/352 - Pérdida de Entrenamiento: 0.4244, Precisión de Entrenamiento: 0.8580\n",
      "Iteración 88457 - Lote 105/352 - Pérdida de Entrenamiento: 0.4204, Precisión de Entrenamiento: 0.8586\n",
      "Iteración 88492 - Lote 140/352 - Pérdida de Entrenamiento: 0.4158, Precisión de Entrenamiento: 0.8607\n",
      "Iteración 88527 - Lote 175/352 - Pérdida de Entrenamiento: 0.4162, Precisión de Entrenamiento: 0.8605\n",
      "Iteración 88562 - Lote 210/352 - Pérdida de Entrenamiento: 0.4117, Precisión de Entrenamiento: 0.8614\n",
      "Iteración 88597 - Lote 245/352 - Pérdida de Entrenamiento: 0.4131, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 88632 - Lote 280/352 - Pérdida de Entrenamiento: 0.4154, Precisión de Entrenamiento: 0.8599\n",
      "Iteración 88667 - Lote 315/352 - Pérdida de Entrenamiento: 0.4155, Precisión de Entrenamiento: 0.8600\n",
      "Iteración 88702 - Lote 350/352 - Pérdida de Entrenamiento: 0.4164, Precisión de Entrenamiento: 0.8602\n",
      "Val loss: 0.4703, Val acc: 0.8414\n",
      "4.8828125e-05\n",
      "Epoch 253/400\n",
      "Iteración 88739 - Lote 35/352 - Pérdida de Entrenamiento: 0.4100, Precisión de Entrenamiento: 0.8636\n",
      "Iteración 88774 - Lote 70/352 - Pérdida de Entrenamiento: 0.4148, Precisión de Entrenamiento: 0.8610\n",
      "Iteración 88809 - Lote 105/352 - Pérdida de Entrenamiento: 0.4272, Precisión de Entrenamiento: 0.8581\n",
      "Iteración 88844 - Lote 140/352 - Pérdida de Entrenamiento: 0.4204, Precisión de Entrenamiento: 0.8597\n",
      "Iteración 88879 - Lote 175/352 - Pérdida de Entrenamiento: 0.4195, Precisión de Entrenamiento: 0.8594\n",
      "Iteración 88914 - Lote 210/352 - Pérdida de Entrenamiento: 0.4188, Precisión de Entrenamiento: 0.8601\n",
      "Iteración 88949 - Lote 245/352 - Pérdida de Entrenamiento: 0.4217, Precisión de Entrenamiento: 0.8588\n",
      "Iteración 88984 - Lote 280/352 - Pérdida de Entrenamiento: 0.4231, Precisión de Entrenamiento: 0.8586\n",
      "Iteración 89019 - Lote 315/352 - Pérdida de Entrenamiento: 0.4220, Precisión de Entrenamiento: 0.8592\n",
      "Iteración 89054 - Lote 350/352 - Pérdida de Entrenamiento: 0.4214, Precisión de Entrenamiento: 0.8593\n",
      "Val loss: 0.4714, Val acc: 0.8374\n",
      "2.44140625e-05\n",
      "Epoch 254/400\n",
      "Iteración 89091 - Lote 35/352 - Pérdida de Entrenamiento: 0.4134, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 89126 - Lote 70/352 - Pérdida de Entrenamiento: 0.4178, Precisión de Entrenamiento: 0.8587\n",
      "Iteración 89161 - Lote 105/352 - Pérdida de Entrenamiento: 0.4186, Precisión de Entrenamiento: 0.8600\n",
      "Iteración 89196 - Lote 140/352 - Pérdida de Entrenamiento: 0.4116, Precisión de Entrenamiento: 0.8620\n",
      "Iteración 89231 - Lote 175/352 - Pérdida de Entrenamiento: 0.4103, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 89266 - Lote 210/352 - Pérdida de Entrenamiento: 0.4082, Precisión de Entrenamiento: 0.8637\n",
      "Iteración 89301 - Lote 245/352 - Pérdida de Entrenamiento: 0.4115, Precisión de Entrenamiento: 0.8623\n",
      "Iteración 89336 - Lote 280/352 - Pérdida de Entrenamiento: 0.4147, Precisión de Entrenamiento: 0.8615\n",
      "Iteración 89371 - Lote 315/352 - Pérdida de Entrenamiento: 0.4128, Precisión de Entrenamiento: 0.8624\n",
      "Iteración 89406 - Lote 350/352 - Pérdida de Entrenamiento: 0.4139, Precisión de Entrenamiento: 0.8620\n",
      "Val loss: 0.4703, Val acc: 0.8400\n",
      "2.44140625e-05\n",
      "Epoch 255/400\n",
      "Iteración 89443 - Lote 35/352 - Pérdida de Entrenamiento: 0.4257, Precisión de Entrenamiento: 0.8580\n",
      "Iteración 89478 - Lote 70/352 - Pérdida de Entrenamiento: 0.4208, Precisión de Entrenamiento: 0.8586\n",
      "Iteración 89513 - Lote 105/352 - Pérdida de Entrenamiento: 0.4150, Precisión de Entrenamiento: 0.8597\n",
      "Iteración 89548 - Lote 140/352 - Pérdida de Entrenamiento: 0.4177, Precisión de Entrenamiento: 0.8582\n",
      "Iteración 89583 - Lote 175/352 - Pérdida de Entrenamiento: 0.4195, Precisión de Entrenamiento: 0.8587\n",
      "Iteración 89618 - Lote 210/352 - Pérdida de Entrenamiento: 0.4200, Precisión de Entrenamiento: 0.8591\n",
      "Iteración 89653 - Lote 245/352 - Pérdida de Entrenamiento: 0.4191, Precisión de Entrenamiento: 0.8595\n",
      "Iteración 89688 - Lote 280/352 - Pérdida de Entrenamiento: 0.4202, Precisión de Entrenamiento: 0.8586\n",
      "Iteración 89723 - Lote 315/352 - Pérdida de Entrenamiento: 0.4216, Precisión de Entrenamiento: 0.8583\n",
      "Iteración 89758 - Lote 350/352 - Pérdida de Entrenamiento: 0.4215, Precisión de Entrenamiento: 0.8588\n",
      "Val loss: 0.4706, Val acc: 0.8388\n",
      "Gradientes para features.0.0.weight: min=-0.1331522911787033, max=0.1252918690443039, mean=-0.0019663108978420496, std=0.03151152655482292\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.08792071044445038, max=0.06171027570962906, mean=-9.338901918454212e-07, std=0.0021290637087076902\n",
      "Gradientes para classifier.1.weight: min=-0.013274374417960644, max=0.007384969387203455, mean=2.561137066314778e-11, std=0.0011662826873362064\n",
      "2.44140625e-05\n",
      "Epoch 256/400\n",
      "Iteración 89795 - Lote 35/352 - Pérdida de Entrenamiento: 0.4252, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 89830 - Lote 70/352 - Pérdida de Entrenamiento: 0.4179, Precisión de Entrenamiento: 0.8636\n",
      "Iteración 89865 - Lote 105/352 - Pérdida de Entrenamiento: 0.4147, Precisión de Entrenamiento: 0.8651\n",
      "Iteración 89900 - Lote 140/352 - Pérdida de Entrenamiento: 0.4171, Precisión de Entrenamiento: 0.8647\n",
      "Iteración 89935 - Lote 175/352 - Pérdida de Entrenamiento: 0.4161, Precisión de Entrenamiento: 0.8639\n",
      "Iteración 89970 - Lote 210/352 - Pérdida de Entrenamiento: 0.4151, Precisión de Entrenamiento: 0.8631\n",
      "Iteración 90005 - Lote 245/352 - Pérdida de Entrenamiento: 0.4156, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 90040 - Lote 280/352 - Pérdida de Entrenamiento: 0.4186, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 90075 - Lote 315/352 - Pérdida de Entrenamiento: 0.4162, Precisión de Entrenamiento: 0.8609\n",
      "Iteración 90110 - Lote 350/352 - Pérdida de Entrenamiento: 0.4158, Precisión de Entrenamiento: 0.8609\n",
      "Val loss: 0.4689, Val acc: 0.8384\n",
      "2.44140625e-05\n",
      "Epoch 257/400\n",
      "Iteración 90147 - Lote 35/352 - Pérdida de Entrenamiento: 0.3961, Precisión de Entrenamiento: 0.8694\n",
      "Iteración 90182 - Lote 70/352 - Pérdida de Entrenamiento: 0.3950, Precisión de Entrenamiento: 0.8695\n",
      "Iteración 90217 - Lote 105/352 - Pérdida de Entrenamiento: 0.4057, Precisión de Entrenamiento: 0.8653\n",
      "Iteración 90252 - Lote 140/352 - Pérdida de Entrenamiento: 0.4103, Precisión de Entrenamiento: 0.8641\n",
      "Iteración 90287 - Lote 175/352 - Pérdida de Entrenamiento: 0.4145, Precisión de Entrenamiento: 0.8626\n",
      "Iteración 90322 - Lote 210/352 - Pérdida de Entrenamiento: 0.4112, Precisión de Entrenamiento: 0.8628\n",
      "Iteración 90357 - Lote 245/352 - Pérdida de Entrenamiento: 0.4107, Precisión de Entrenamiento: 0.8630\n",
      "Iteración 90392 - Lote 280/352 - Pérdida de Entrenamiento: 0.4127, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 90427 - Lote 315/352 - Pérdida de Entrenamiento: 0.4128, Precisión de Entrenamiento: 0.8625\n",
      "Iteración 90462 - Lote 350/352 - Pérdida de Entrenamiento: 0.4142, Precisión de Entrenamiento: 0.8621\n",
      "Val loss: 0.4686, Val acc: 0.8376\n",
      "2.44140625e-05\n",
      "Epoch 258/400\n",
      "Iteración 90499 - Lote 35/352 - Pérdida de Entrenamiento: 0.4120, Precisión de Entrenamiento: 0.8569\n",
      "Iteración 90534 - Lote 70/352 - Pérdida de Entrenamiento: 0.4103, Precisión de Entrenamiento: 0.8597\n",
      "Iteración 90569 - Lote 105/352 - Pérdida de Entrenamiento: 0.4128, Precisión de Entrenamiento: 0.8609\n",
      "Iteración 90604 - Lote 140/352 - Pérdida de Entrenamiento: 0.4132, Precisión de Entrenamiento: 0.8610\n",
      "Iteración 90639 - Lote 175/352 - Pérdida de Entrenamiento: 0.4150, Precisión de Entrenamiento: 0.8601\n",
      "Iteración 90674 - Lote 210/352 - Pérdida de Entrenamiento: 0.4128, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 90709 - Lote 245/352 - Pérdida de Entrenamiento: 0.4138, Precisión de Entrenamiento: 0.8607\n",
      "Iteración 90744 - Lote 280/352 - Pérdida de Entrenamiento: 0.4124, Precisión de Entrenamiento: 0.8619\n",
      "Iteración 90779 - Lote 315/352 - Pérdida de Entrenamiento: 0.4123, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 90814 - Lote 350/352 - Pérdida de Entrenamiento: 0.4130, Precisión de Entrenamiento: 0.8616\n",
      "Val loss: 0.4687, Val acc: 0.8380\n",
      "2.44140625e-05\n",
      "Epoch 259/400\n",
      "Iteración 90851 - Lote 35/352 - Pérdida de Entrenamiento: 0.4058, Precisión de Entrenamiento: 0.8587\n",
      "Iteración 90886 - Lote 70/352 - Pérdida de Entrenamiento: 0.4162, Precisión de Entrenamiento: 0.8568\n",
      "Iteración 90921 - Lote 105/352 - Pérdida de Entrenamiento: 0.4248, Precisión de Entrenamiento: 0.8538\n",
      "Iteración 90956 - Lote 140/352 - Pérdida de Entrenamiento: 0.4180, Precisión de Entrenamiento: 0.8576\n",
      "Iteración 90991 - Lote 175/352 - Pérdida de Entrenamiento: 0.4140, Precisión de Entrenamiento: 0.8601\n",
      "Iteración 91026 - Lote 210/352 - Pérdida de Entrenamiento: 0.4141, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 91061 - Lote 245/352 - Pérdida de Entrenamiento: 0.4165, Precisión de Entrenamiento: 0.8604\n",
      "Iteración 91096 - Lote 280/352 - Pérdida de Entrenamiento: 0.4161, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 91131 - Lote 315/352 - Pérdida de Entrenamiento: 0.4158, Precisión de Entrenamiento: 0.8606\n",
      "Iteración 91166 - Lote 350/352 - Pérdida de Entrenamiento: 0.4152, Precisión de Entrenamiento: 0.8608\n",
      "Val loss: 0.4714, Val acc: 0.8400\n",
      "2.44140625e-05\n",
      "Epoch 260/400\n",
      "Iteración 91203 - Lote 35/352 - Pérdida de Entrenamiento: 0.4113, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 91238 - Lote 70/352 - Pérdida de Entrenamiento: 0.4089, Precisión de Entrenamiento: 0.8628\n",
      "Iteración 91273 - Lote 105/352 - Pérdida de Entrenamiento: 0.4042, Precisión de Entrenamiento: 0.8639\n",
      "Iteración 91308 - Lote 140/352 - Pérdida de Entrenamiento: 0.4113, Precisión de Entrenamiento: 0.8597\n",
      "Iteración 91343 - Lote 175/352 - Pérdida de Entrenamiento: 0.4104, Precisión de Entrenamiento: 0.8603\n",
      "Iteración 91378 - Lote 210/352 - Pérdida de Entrenamiento: 0.4111, Precisión de Entrenamiento: 0.8609\n",
      "Iteración 91413 - Lote 245/352 - Pérdida de Entrenamiento: 0.4130, Precisión de Entrenamiento: 0.8604\n",
      "Iteración 91448 - Lote 280/352 - Pérdida de Entrenamiento: 0.4128, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 91483 - Lote 315/352 - Pérdida de Entrenamiento: 0.4136, Precisión de Entrenamiento: 0.8610\n",
      "Iteración 91518 - Lote 350/352 - Pérdida de Entrenamiento: 0.4134, Precisión de Entrenamiento: 0.8618\n",
      "Val loss: 0.4699, Val acc: 0.8410\n",
      "Gradientes para features.0.0.weight: min=-0.25680485367774963, max=0.08665554225444794, mean=-0.004080437123775482, std=0.04651004448533058\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.08982937037944794, max=0.09642425179481506, mean=-3.49404763255734e-06, std=0.003068596590310335\n",
      "Gradientes para classifier.1.weight: min=-0.01485477201640606, max=0.01333640143275261, mean=-4.656612768993984e-12, std=0.0018943962641060352\n",
      "2.44140625e-05\n",
      "Epoch 261/400\n",
      "Iteración 91555 - Lote 35/352 - Pérdida de Entrenamiento: 0.4465, Precisión de Entrenamiento: 0.8480\n",
      "Iteración 91590 - Lote 70/352 - Pérdida de Entrenamiento: 0.4359, Precisión de Entrenamiento: 0.8520\n",
      "Iteración 91625 - Lote 105/352 - Pérdida de Entrenamiento: 0.4239, Precisión de Entrenamiento: 0.8587\n",
      "Iteración 91660 - Lote 140/352 - Pérdida de Entrenamiento: 0.4204, Precisión de Entrenamiento: 0.8593\n",
      "Iteración 91695 - Lote 175/352 - Pérdida de Entrenamiento: 0.4206, Precisión de Entrenamiento: 0.8594\n",
      "Iteración 91730 - Lote 210/352 - Pérdida de Entrenamiento: 0.4181, Precisión de Entrenamiento: 0.8605\n",
      "Iteración 91765 - Lote 245/352 - Pérdida de Entrenamiento: 0.4146, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 91800 - Lote 280/352 - Pérdida de Entrenamiento: 0.4135, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 91835 - Lote 315/352 - Pérdida de Entrenamiento: 0.4153, Precisión de Entrenamiento: 0.8606\n",
      "Iteración 91870 - Lote 350/352 - Pérdida de Entrenamiento: 0.4148, Precisión de Entrenamiento: 0.8607\n",
      "Val loss: 0.4699, Val acc: 0.8416\n",
      "2.44140625e-05\n",
      "Epoch 262/400\n",
      "Iteración 91907 - Lote 35/352 - Pérdida de Entrenamiento: 0.4070, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 91942 - Lote 70/352 - Pérdida de Entrenamiento: 0.4205, Precisión de Entrenamiento: 0.8574\n",
      "Iteración 91977 - Lote 105/352 - Pérdida de Entrenamiento: 0.4133, Precisión de Entrenamiento: 0.8606\n",
      "Iteración 92012 - Lote 140/352 - Pérdida de Entrenamiento: 0.4100, Precisión de Entrenamiento: 0.8614\n",
      "Iteración 92047 - Lote 175/352 - Pérdida de Entrenamiento: 0.4080, Precisión de Entrenamiento: 0.8628\n",
      "Iteración 92082 - Lote 210/352 - Pérdida de Entrenamiento: 0.4114, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 92117 - Lote 245/352 - Pérdida de Entrenamiento: 0.4114, Precisión de Entrenamiento: 0.8610\n",
      "Iteración 92152 - Lote 280/352 - Pérdida de Entrenamiento: 0.4116, Precisión de Entrenamiento: 0.8614\n",
      "Iteración 92187 - Lote 315/352 - Pérdida de Entrenamiento: 0.4110, Precisión de Entrenamiento: 0.8616\n",
      "Iteración 92222 - Lote 350/352 - Pérdida de Entrenamiento: 0.4110, Precisión de Entrenamiento: 0.8615\n",
      "Val loss: 0.4704, Val acc: 0.8404\n",
      "1.220703125e-05\n",
      "Epoch 263/400\n",
      "Iteración 92259 - Lote 35/352 - Pérdida de Entrenamiento: 0.4170, Precisión de Entrenamiento: 0.8583\n",
      "Iteración 92294 - Lote 70/352 - Pérdida de Entrenamiento: 0.4155, Precisión de Entrenamiento: 0.8593\n",
      "Iteración 92329 - Lote 105/352 - Pérdida de Entrenamiento: 0.4215, Precisión de Entrenamiento: 0.8580\n",
      "Iteración 92364 - Lote 140/352 - Pérdida de Entrenamiento: 0.4218, Precisión de Entrenamiento: 0.8580\n",
      "Iteración 92399 - Lote 175/352 - Pérdida de Entrenamiento: 0.4210, Precisión de Entrenamiento: 0.8587\n",
      "Iteración 92434 - Lote 210/352 - Pérdida de Entrenamiento: 0.4169, Precisión de Entrenamiento: 0.8599\n",
      "Iteración 92469 - Lote 245/352 - Pérdida de Entrenamiento: 0.4142, Precisión de Entrenamiento: 0.8614\n",
      "Iteración 92504 - Lote 280/352 - Pérdida de Entrenamiento: 0.4150, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 92539 - Lote 315/352 - Pérdida de Entrenamiento: 0.4155, Precisión de Entrenamiento: 0.8611\n",
      "Iteración 92574 - Lote 350/352 - Pérdida de Entrenamiento: 0.4165, Precisión de Entrenamiento: 0.8609\n",
      "Val loss: 0.4711, Val acc: 0.8388\n",
      "1.220703125e-05\n",
      "Epoch 264/400\n",
      "Iteración 92611 - Lote 35/352 - Pérdida de Entrenamiento: 0.4164, Precisión de Entrenamiento: 0.8592\n",
      "Iteración 92646 - Lote 70/352 - Pérdida de Entrenamiento: 0.4109, Precisión de Entrenamiento: 0.8627\n",
      "Iteración 92681 - Lote 105/352 - Pérdida de Entrenamiento: 0.4068, Precisión de Entrenamiento: 0.8658\n",
      "Iteración 92716 - Lote 140/352 - Pérdida de Entrenamiento: 0.4036, Precisión de Entrenamiento: 0.8665\n",
      "Iteración 92751 - Lote 175/352 - Pérdida de Entrenamiento: 0.4071, Precisión de Entrenamiento: 0.8652\n",
      "Iteración 92786 - Lote 210/352 - Pérdida de Entrenamiento: 0.4089, Precisión de Entrenamiento: 0.8637\n",
      "Iteración 92821 - Lote 245/352 - Pérdida de Entrenamiento: 0.4091, Precisión de Entrenamiento: 0.8634\n",
      "Iteración 92856 - Lote 280/352 - Pérdida de Entrenamiento: 0.4116, Precisión de Entrenamiento: 0.8631\n",
      "Iteración 92891 - Lote 315/352 - Pérdida de Entrenamiento: 0.4142, Precisión de Entrenamiento: 0.8620\n",
      "Iteración 92926 - Lote 350/352 - Pérdida de Entrenamiento: 0.4153, Precisión de Entrenamiento: 0.8615\n",
      "Val loss: 0.4705, Val acc: 0.8400\n",
      "1.220703125e-05\n",
      "Epoch 265/400\n",
      "Iteración 92963 - Lote 35/352 - Pérdida de Entrenamiento: 0.4189, Precisión de Entrenamiento: 0.8580\n",
      "Iteración 92998 - Lote 70/352 - Pérdida de Entrenamiento: 0.4102, Precisión de Entrenamiento: 0.8635\n",
      "Iteración 93033 - Lote 105/352 - Pérdida de Entrenamiento: 0.4157, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 93068 - Lote 140/352 - Pérdida de Entrenamiento: 0.4211, Precisión de Entrenamiento: 0.8598\n",
      "Iteración 93103 - Lote 175/352 - Pérdida de Entrenamiento: 0.4197, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 93138 - Lote 210/352 - Pérdida de Entrenamiento: 0.4168, Precisión de Entrenamiento: 0.8619\n",
      "Iteración 93173 - Lote 245/352 - Pérdida de Entrenamiento: 0.4174, Precisión de Entrenamiento: 0.8617\n",
      "Iteración 93208 - Lote 280/352 - Pérdida de Entrenamiento: 0.4150, Precisión de Entrenamiento: 0.8614\n",
      "Iteración 93243 - Lote 315/352 - Pérdida de Entrenamiento: 0.4162, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 93278 - Lote 350/352 - Pérdida de Entrenamiento: 0.4145, Precisión de Entrenamiento: 0.8618\n",
      "Val loss: 0.4703, Val acc: 0.8388\n",
      "Gradientes para features.0.0.weight: min=-0.1416274756193161, max=0.20800958573818207, mean=0.0005908561288379133, std=0.048609767109155655\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.09936056286096573, max=0.10307718068361282, mean=-2.5429335437365808e-06, std=0.0033981401938945055\n",
      "Gradientes para classifier.1.weight: min=-0.012844433076679707, max=0.009633898735046387, mean=1.9790604918745736e-11, std=0.0013036017771810293\n",
      "1.220703125e-05\n",
      "Epoch 266/400\n",
      "Iteración 93315 - Lote 35/352 - Pérdida de Entrenamiento: 0.4230, Precisión de Entrenamiento: 0.8556\n",
      "Iteración 93350 - Lote 70/352 - Pérdida de Entrenamiento: 0.4065, Precisión de Entrenamiento: 0.8628\n",
      "Iteración 93385 - Lote 105/352 - Pérdida de Entrenamiento: 0.4008, Precisión de Entrenamiento: 0.8641\n",
      "Iteración 93420 - Lote 140/352 - Pérdida de Entrenamiento: 0.4038, Precisión de Entrenamiento: 0.8627\n",
      "Iteración 93455 - Lote 175/352 - Pérdida de Entrenamiento: 0.4056, Precisión de Entrenamiento: 0.8628\n",
      "Iteración 93490 - Lote 210/352 - Pérdida de Entrenamiento: 0.4103, Precisión de Entrenamiento: 0.8617\n",
      "Iteración 93525 - Lote 245/352 - Pérdida de Entrenamiento: 0.4110, Precisión de Entrenamiento: 0.8615\n",
      "Iteración 93560 - Lote 280/352 - Pérdida de Entrenamiento: 0.4106, Precisión de Entrenamiento: 0.8627\n",
      "Iteración 93595 - Lote 315/352 - Pérdida de Entrenamiento: 0.4112, Precisión de Entrenamiento: 0.8624\n",
      "Iteración 93630 - Lote 350/352 - Pérdida de Entrenamiento: 0.4126, Precisión de Entrenamiento: 0.8623\n",
      "Val loss: 0.4694, Val acc: 0.8418\n",
      "1.220703125e-05\n",
      "Epoch 267/400\n",
      "Iteración 93667 - Lote 35/352 - Pérdida de Entrenamiento: 0.4261, Precisión de Entrenamiento: 0.8609\n",
      "Iteración 93702 - Lote 70/352 - Pérdida de Entrenamiento: 0.4153, Precisión de Entrenamiento: 0.8614\n",
      "Iteración 93737 - Lote 105/352 - Pérdida de Entrenamiento: 0.4175, Precisión de Entrenamiento: 0.8600\n",
      "Iteración 93772 - Lote 140/352 - Pérdida de Entrenamiento: 0.4157, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 93807 - Lote 175/352 - Pérdida de Entrenamiento: 0.4138, Precisión de Entrenamiento: 0.8614\n",
      "Iteración 93842 - Lote 210/352 - Pérdida de Entrenamiento: 0.4179, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 93877 - Lote 245/352 - Pérdida de Entrenamiento: 0.4137, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 93912 - Lote 280/352 - Pérdida de Entrenamiento: 0.4123, Precisión de Entrenamiento: 0.8622\n",
      "Iteración 93947 - Lote 315/352 - Pérdida de Entrenamiento: 0.4110, Precisión de Entrenamiento: 0.8626\n",
      "Iteración 93982 - Lote 350/352 - Pérdida de Entrenamiento: 0.4119, Precisión de Entrenamiento: 0.8623\n",
      "Val loss: 0.4683, Val acc: 0.8390\n",
      "1.220703125e-05\n",
      "Epoch 268/400\n",
      "Iteración 94019 - Lote 35/352 - Pérdida de Entrenamiento: 0.4265, Precisión de Entrenamiento: 0.8589\n",
      "Iteración 94054 - Lote 70/352 - Pérdida de Entrenamiento: 0.4206, Precisión de Entrenamiento: 0.8581\n",
      "Iteración 94089 - Lote 105/352 - Pérdida de Entrenamiento: 0.4206, Precisión de Entrenamiento: 0.8568\n",
      "Iteración 94124 - Lote 140/352 - Pérdida de Entrenamiento: 0.4214, Precisión de Entrenamiento: 0.8578\n",
      "Iteración 94159 - Lote 175/352 - Pérdida de Entrenamiento: 0.4191, Precisión de Entrenamiento: 0.8577\n",
      "Iteración 94194 - Lote 210/352 - Pérdida de Entrenamiento: 0.4145, Precisión de Entrenamiento: 0.8592\n",
      "Iteración 94229 - Lote 245/352 - Pérdida de Entrenamiento: 0.4147, Precisión de Entrenamiento: 0.8600\n",
      "Iteración 94264 - Lote 280/352 - Pérdida de Entrenamiento: 0.4148, Precisión de Entrenamiento: 0.8595\n",
      "Iteración 94299 - Lote 315/352 - Pérdida de Entrenamiento: 0.4148, Precisión de Entrenamiento: 0.8593\n",
      "Iteración 94334 - Lote 350/352 - Pérdida de Entrenamiento: 0.4135, Precisión de Entrenamiento: 0.8601\n",
      "Val loss: 0.4673, Val acc: 0.8388\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_268.pth\n",
      "Checkpoint del mejor modelo guardado en la época 268\n",
      "1.220703125e-05\n",
      "Epoch 269/400\n",
      "Iteración 94371 - Lote 35/352 - Pérdida de Entrenamiento: 0.4094, Precisión de Entrenamiento: 0.8632\n",
      "Iteración 94406 - Lote 70/352 - Pérdida de Entrenamiento: 0.3981, Precisión de Entrenamiento: 0.8655\n",
      "Iteración 94441 - Lote 105/352 - Pérdida de Entrenamiento: 0.4041, Precisión de Entrenamiento: 0.8632\n",
      "Iteración 94476 - Lote 140/352 - Pérdida de Entrenamiento: 0.4111, Precisión de Entrenamiento: 0.8627\n",
      "Iteración 94511 - Lote 175/352 - Pérdida de Entrenamiento: 0.4101, Precisión de Entrenamiento: 0.8631\n",
      "Iteración 94546 - Lote 210/352 - Pérdida de Entrenamiento: 0.4142, Precisión de Entrenamiento: 0.8625\n",
      "Iteración 94581 - Lote 245/352 - Pérdida de Entrenamiento: 0.4138, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 94616 - Lote 280/352 - Pérdida de Entrenamiento: 0.4150, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 94651 - Lote 315/352 - Pérdida de Entrenamiento: 0.4157, Precisión de Entrenamiento: 0.8617\n",
      "Iteración 94686 - Lote 350/352 - Pérdida de Entrenamiento: 0.4158, Precisión de Entrenamiento: 0.8614\n",
      "Val loss: 0.4674, Val acc: 0.8406\n",
      "1.220703125e-05\n",
      "Epoch 270/400\n",
      "Iteración 94723 - Lote 35/352 - Pérdida de Entrenamiento: 0.3928, Precisión de Entrenamiento: 0.8679\n",
      "Iteración 94758 - Lote 70/352 - Pérdida de Entrenamiento: 0.4000, Precisión de Entrenamiento: 0.8663\n",
      "Iteración 94793 - Lote 105/352 - Pérdida de Entrenamiento: 0.4059, Precisión de Entrenamiento: 0.8644\n",
      "Iteración 94828 - Lote 140/352 - Pérdida de Entrenamiento: 0.4114, Precisión de Entrenamiento: 0.8637\n",
      "Iteración 94863 - Lote 175/352 - Pérdida de Entrenamiento: 0.4087, Precisión de Entrenamiento: 0.8651\n",
      "Iteración 94898 - Lote 210/352 - Pérdida de Entrenamiento: 0.4092, Precisión de Entrenamiento: 0.8650\n",
      "Iteración 94933 - Lote 245/352 - Pérdida de Entrenamiento: 0.4107, Precisión de Entrenamiento: 0.8641\n",
      "Iteración 94968 - Lote 280/352 - Pérdida de Entrenamiento: 0.4128, Precisión de Entrenamiento: 0.8628\n",
      "Iteración 95003 - Lote 315/352 - Pérdida de Entrenamiento: 0.4144, Precisión de Entrenamiento: 0.8620\n",
      "Iteración 95038 - Lote 350/352 - Pérdida de Entrenamiento: 0.4143, Precisión de Entrenamiento: 0.8623\n",
      "Val loss: 0.4672, Val acc: 0.8390\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_270.pth\n",
      "Checkpoint del mejor modelo guardado en la época 270\n",
      "Gradientes para features.0.0.weight: min=-0.2774680256843567, max=0.17614978551864624, mean=-0.0065885442309081554, std=0.0601896196603775\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.11737928539514542, max=0.1084822565317154, mean=-4.982714926882181e-06, std=0.004367835819721222\n",
      "Gradientes para classifier.1.weight: min=-0.012917783111333847, max=0.009996635839343071, mean=9.313225537987968e-12, std=0.0015927520580589771\n",
      "1.220703125e-05\n",
      "Epoch 271/400\n",
      "Iteración 95075 - Lote 35/352 - Pérdida de Entrenamiento: 0.4076, Precisión de Entrenamiento: 0.8654\n",
      "Iteración 95110 - Lote 70/352 - Pérdida de Entrenamiento: 0.4160, Precisión de Entrenamiento: 0.8598\n",
      "Iteración 95145 - Lote 105/352 - Pérdida de Entrenamiento: 0.4150, Precisión de Entrenamiento: 0.8603\n",
      "Iteración 95180 - Lote 140/352 - Pérdida de Entrenamiento: 0.4139, Precisión de Entrenamiento: 0.8605\n",
      "Iteración 95215 - Lote 175/352 - Pérdida de Entrenamiento: 0.4129, Precisión de Entrenamiento: 0.8617\n",
      "Iteración 95250 - Lote 210/352 - Pérdida de Entrenamiento: 0.4141, Precisión de Entrenamiento: 0.8600\n",
      "Iteración 95285 - Lote 245/352 - Pérdida de Entrenamiento: 0.4138, Precisión de Entrenamiento: 0.8602\n",
      "Iteración 95320 - Lote 280/352 - Pérdida de Entrenamiento: 0.4144, Precisión de Entrenamiento: 0.8603\n",
      "Iteración 95355 - Lote 315/352 - Pérdida de Entrenamiento: 0.4113, Precisión de Entrenamiento: 0.8615\n",
      "Iteración 95390 - Lote 350/352 - Pérdida de Entrenamiento: 0.4133, Precisión de Entrenamiento: 0.8616\n",
      "Val loss: 0.4683, Val acc: 0.8416\n",
      "1.220703125e-05\n",
      "Epoch 272/400\n",
      "Iteración 95427 - Lote 35/352 - Pérdida de Entrenamiento: 0.4138, Precisión de Entrenamiento: 0.8658\n",
      "Iteración 95462 - Lote 70/352 - Pérdida de Entrenamiento: 0.4150, Precisión de Entrenamiento: 0.8642\n",
      "Iteración 95497 - Lote 105/352 - Pérdida de Entrenamiento: 0.4145, Precisión de Entrenamiento: 0.8632\n",
      "Iteración 95532 - Lote 140/352 - Pérdida de Entrenamiento: 0.4136, Precisión de Entrenamiento: 0.8641\n",
      "Iteración 95567 - Lote 175/352 - Pérdida de Entrenamiento: 0.4165, Precisión de Entrenamiento: 0.8627\n",
      "Iteración 95602 - Lote 210/352 - Pérdida de Entrenamiento: 0.4157, Precisión de Entrenamiento: 0.8628\n",
      "Iteración 95637 - Lote 245/352 - Pérdida de Entrenamiento: 0.4175, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 95672 - Lote 280/352 - Pérdida de Entrenamiento: 0.4177, Precisión de Entrenamiento: 0.8617\n",
      "Iteración 95707 - Lote 315/352 - Pérdida de Entrenamiento: 0.4168, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 95742 - Lote 350/352 - Pérdida de Entrenamiento: 0.4188, Precisión de Entrenamiento: 0.8613\n",
      "Val loss: 0.4683, Val acc: 0.8408\n",
      "1.220703125e-05\n",
      "Epoch 273/400\n",
      "Iteración 95779 - Lote 35/352 - Pérdida de Entrenamiento: 0.4062, Precisión de Entrenamiento: 0.8667\n",
      "Iteración 95814 - Lote 70/352 - Pérdida de Entrenamiento: 0.4060, Precisión de Entrenamiento: 0.8681\n",
      "Iteración 95849 - Lote 105/352 - Pérdida de Entrenamiento: 0.4123, Precisión de Entrenamiento: 0.8649\n",
      "Iteración 95884 - Lote 140/352 - Pérdida de Entrenamiento: 0.4077, Precisión de Entrenamiento: 0.8660\n",
      "Iteración 95919 - Lote 175/352 - Pérdida de Entrenamiento: 0.4103, Precisión de Entrenamiento: 0.8652\n",
      "Iteración 95954 - Lote 210/352 - Pérdida de Entrenamiento: 0.4134, Precisión de Entrenamiento: 0.8633\n",
      "Iteración 95989 - Lote 245/352 - Pérdida de Entrenamiento: 0.4102, Precisión de Entrenamiento: 0.8643\n",
      "Iteración 96024 - Lote 280/352 - Pérdida de Entrenamiento: 0.4111, Precisión de Entrenamiento: 0.8638\n",
      "Iteración 96059 - Lote 315/352 - Pérdida de Entrenamiento: 0.4111, Precisión de Entrenamiento: 0.8636\n",
      "Iteración 96094 - Lote 350/352 - Pérdida de Entrenamiento: 0.4108, Precisión de Entrenamiento: 0.8631\n",
      "Val loss: 0.4670, Val acc: 0.8388\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_273.pth\n",
      "Checkpoint del mejor modelo guardado en la época 273\n",
      "1.220703125e-05\n",
      "Epoch 274/400\n",
      "Iteración 96131 - Lote 35/352 - Pérdida de Entrenamiento: 0.3985, Precisión de Entrenamiento: 0.8625\n",
      "Iteración 96166 - Lote 70/352 - Pérdida de Entrenamiento: 0.4257, Precisión de Entrenamiento: 0.8535\n",
      "Iteración 96201 - Lote 105/352 - Pérdida de Entrenamiento: 0.4213, Precisión de Entrenamiento: 0.8556\n",
      "Iteración 96236 - Lote 140/352 - Pérdida de Entrenamiento: 0.4208, Precisión de Entrenamiento: 0.8574\n",
      "Iteración 96271 - Lote 175/352 - Pérdida de Entrenamiento: 0.4179, Precisión de Entrenamiento: 0.8579\n",
      "Iteración 96306 - Lote 210/352 - Pérdida de Entrenamiento: 0.4176, Precisión de Entrenamiento: 0.8586\n",
      "Iteración 96341 - Lote 245/352 - Pérdida de Entrenamiento: 0.4162, Precisión de Entrenamiento: 0.8584\n",
      "Iteración 96376 - Lote 280/352 - Pérdida de Entrenamiento: 0.4142, Precisión de Entrenamiento: 0.8588\n",
      "Iteración 96411 - Lote 315/352 - Pérdida de Entrenamiento: 0.4140, Precisión de Entrenamiento: 0.8589\n",
      "Iteración 96446 - Lote 350/352 - Pérdida de Entrenamiento: 0.4157, Precisión de Entrenamiento: 0.8586\n",
      "Val loss: 0.4687, Val acc: 0.8398\n",
      "1.220703125e-05\n",
      "Epoch 275/400\n",
      "Iteración 96483 - Lote 35/352 - Pérdida de Entrenamiento: 0.3974, Precisión de Entrenamiento: 0.8663\n",
      "Iteración 96518 - Lote 70/352 - Pérdida de Entrenamiento: 0.4066, Precisión de Entrenamiento: 0.8619\n",
      "Iteración 96553 - Lote 105/352 - Pérdida de Entrenamiento: 0.4045, Precisión de Entrenamiento: 0.8646\n",
      "Iteración 96588 - Lote 140/352 - Pérdida de Entrenamiento: 0.4118, Precisión de Entrenamiento: 0.8631\n",
      "Iteración 96623 - Lote 175/352 - Pérdida de Entrenamiento: 0.4114, Precisión de Entrenamiento: 0.8640\n",
      "Iteración 96658 - Lote 210/352 - Pérdida de Entrenamiento: 0.4123, Precisión de Entrenamiento: 0.8635\n",
      "Iteración 96693 - Lote 245/352 - Pérdida de Entrenamiento: 0.4124, Precisión de Entrenamiento: 0.8635\n",
      "Iteración 96728 - Lote 280/352 - Pérdida de Entrenamiento: 0.4137, Precisión de Entrenamiento: 0.8631\n",
      "Iteración 96763 - Lote 315/352 - Pérdida de Entrenamiento: 0.4118, Precisión de Entrenamiento: 0.8634\n",
      "Iteración 96798 - Lote 350/352 - Pérdida de Entrenamiento: 0.4131, Precisión de Entrenamiento: 0.8629\n",
      "Val loss: 0.4683, Val acc: 0.8408\n",
      "Gradientes para features.0.0.weight: min=-0.18311303853988647, max=0.13983826339244843, mean=-0.004775347653776407, std=0.048963695764541626\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.09901473671197891, max=0.12056257575750351, mean=-1.6185362028409145e-06, std=0.0031910683028399944\n",
      "Gradientes para classifier.1.weight: min=-0.008681442588567734, max=0.007771010044962168, mean=2.328306384496992e-12, std=0.0012185892555862665\n",
      "1.220703125e-05\n",
      "Epoch 276/400\n",
      "Iteración 96835 - Lote 35/352 - Pérdida de Entrenamiento: 0.4201, Precisión de Entrenamiento: 0.8625\n",
      "Iteración 96870 - Lote 70/352 - Pérdida de Entrenamiento: 0.4124, Precisión de Entrenamiento: 0.8626\n",
      "Iteración 96905 - Lote 105/352 - Pérdida de Entrenamiento: 0.4152, Precisión de Entrenamiento: 0.8628\n",
      "Iteración 96940 - Lote 140/352 - Pérdida de Entrenamiento: 0.4116, Precisión de Entrenamiento: 0.8630\n",
      "Iteración 96975 - Lote 175/352 - Pérdida de Entrenamiento: 0.4135, Precisión de Entrenamiento: 0.8620\n",
      "Iteración 97010 - Lote 210/352 - Pérdida de Entrenamiento: 0.4125, Precisión de Entrenamiento: 0.8613\n",
      "Iteración 97045 - Lote 245/352 - Pérdida de Entrenamiento: 0.4118, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 97080 - Lote 280/352 - Pérdida de Entrenamiento: 0.4114, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 97115 - Lote 315/352 - Pérdida de Entrenamiento: 0.4137, Precisión de Entrenamiento: 0.8610\n",
      "Iteración 97150 - Lote 350/352 - Pérdida de Entrenamiento: 0.4126, Precisión de Entrenamiento: 0.8614\n",
      "Val loss: 0.4692, Val acc: 0.8372\n",
      "1.220703125e-05\n",
      "Epoch 277/400\n",
      "Iteración 97187 - Lote 35/352 - Pérdida de Entrenamiento: 0.4188, Precisión de Entrenamiento: 0.8616\n",
      "Iteración 97222 - Lote 70/352 - Pérdida de Entrenamiento: 0.4099, Precisión de Entrenamiento: 0.8651\n",
      "Iteración 97257 - Lote 105/352 - Pérdida de Entrenamiento: 0.4099, Precisión de Entrenamiento: 0.8644\n",
      "Iteración 97292 - Lote 140/352 - Pérdida de Entrenamiento: 0.4166, Precisión de Entrenamiento: 0.8627\n",
      "Iteración 97327 - Lote 175/352 - Pérdida de Entrenamiento: 0.4164, Precisión de Entrenamiento: 0.8625\n",
      "Iteración 97362 - Lote 210/352 - Pérdida de Entrenamiento: 0.4147, Precisión de Entrenamiento: 0.8628\n",
      "Iteración 97397 - Lote 245/352 - Pérdida de Entrenamiento: 0.4170, Precisión de Entrenamiento: 0.8619\n",
      "Iteración 97432 - Lote 280/352 - Pérdida de Entrenamiento: 0.4155, Precisión de Entrenamiento: 0.8626\n",
      "Iteración 97467 - Lote 315/352 - Pérdida de Entrenamiento: 0.4145, Precisión de Entrenamiento: 0.8626\n",
      "Iteración 97502 - Lote 350/352 - Pérdida de Entrenamiento: 0.4150, Precisión de Entrenamiento: 0.8624\n",
      "Val loss: 0.4710, Val acc: 0.8388\n",
      "1.220703125e-05\n",
      "Epoch 278/400\n",
      "Iteración 97539 - Lote 35/352 - Pérdida de Entrenamiento: 0.4125, Precisión de Entrenamiento: 0.8614\n",
      "Iteración 97574 - Lote 70/352 - Pérdida de Entrenamiento: 0.4094, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 97609 - Lote 105/352 - Pérdida de Entrenamiento: 0.3996, Precisión de Entrenamiento: 0.8666\n",
      "Iteración 97644 - Lote 140/352 - Pérdida de Entrenamiento: 0.4028, Precisión de Entrenamiento: 0.8649\n",
      "Iteración 97679 - Lote 175/352 - Pérdida de Entrenamiento: 0.4058, Precisión de Entrenamiento: 0.8637\n",
      "Iteración 97714 - Lote 210/352 - Pérdida de Entrenamiento: 0.4016, Precisión de Entrenamiento: 0.8648\n",
      "Iteración 97749 - Lote 245/352 - Pérdida de Entrenamiento: 0.4043, Precisión de Entrenamiento: 0.8637\n",
      "Iteración 97784 - Lote 280/352 - Pérdida de Entrenamiento: 0.4080, Precisión de Entrenamiento: 0.8630\n",
      "Iteración 97819 - Lote 315/352 - Pérdida de Entrenamiento: 0.4106, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 97854 - Lote 350/352 - Pérdida de Entrenamiento: 0.4103, Precisión de Entrenamiento: 0.8626\n",
      "Val loss: 0.4683, Val acc: 0.8414\n",
      "1.220703125e-05\n",
      "Epoch 279/400\n",
      "Iteración 97891 - Lote 35/352 - Pérdida de Entrenamiento: 0.4078, Precisión de Entrenamiento: 0.8616\n",
      "Iteración 97926 - Lote 70/352 - Pérdida de Entrenamiento: 0.4054, Precisión de Entrenamiento: 0.8628\n",
      "Iteración 97961 - Lote 105/352 - Pérdida de Entrenamiento: 0.4060, Precisión de Entrenamiento: 0.8649\n",
      "Iteración 97996 - Lote 140/352 - Pérdida de Entrenamiento: 0.4089, Precisión de Entrenamiento: 0.8633\n",
      "Iteración 98031 - Lote 175/352 - Pérdida de Entrenamiento: 0.4111, Precisión de Entrenamiento: 0.8624\n",
      "Iteración 98066 - Lote 210/352 - Pérdida de Entrenamiento: 0.4113, Precisión de Entrenamiento: 0.8625\n",
      "Iteración 98101 - Lote 245/352 - Pérdida de Entrenamiento: 0.4124, Precisión de Entrenamiento: 0.8620\n",
      "Iteración 98136 - Lote 280/352 - Pérdida de Entrenamiento: 0.4112, Precisión de Entrenamiento: 0.8625\n",
      "Iteración 98171 - Lote 315/352 - Pérdida de Entrenamiento: 0.4105, Precisión de Entrenamiento: 0.8632\n",
      "Iteración 98206 - Lote 350/352 - Pérdida de Entrenamiento: 0.4110, Precisión de Entrenamiento: 0.8633\n",
      "Val loss: 0.4696, Val acc: 0.8376\n",
      "1.220703125e-05\n",
      "Epoch 280/400\n",
      "Iteración 98243 - Lote 35/352 - Pérdida de Entrenamiento: 0.4021, Precisión de Entrenamiento: 0.8656\n",
      "Iteración 98278 - Lote 70/352 - Pérdida de Entrenamiento: 0.4073, Precisión de Entrenamiento: 0.8624\n",
      "Iteración 98313 - Lote 105/352 - Pérdida de Entrenamiento: 0.4105, Precisión de Entrenamiento: 0.8615\n",
      "Iteración 98348 - Lote 140/352 - Pérdida de Entrenamiento: 0.4119, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 98383 - Lote 175/352 - Pérdida de Entrenamiento: 0.4127, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 98418 - Lote 210/352 - Pérdida de Entrenamiento: 0.4109, Precisión de Entrenamiento: 0.8626\n",
      "Iteración 98453 - Lote 245/352 - Pérdida de Entrenamiento: 0.4102, Precisión de Entrenamiento: 0.8631\n",
      "Iteración 98488 - Lote 280/352 - Pérdida de Entrenamiento: 0.4132, Precisión de Entrenamiento: 0.8626\n",
      "Iteración 98523 - Lote 315/352 - Pérdida de Entrenamiento: 0.4131, Precisión de Entrenamiento: 0.8627\n",
      "Iteración 98558 - Lote 350/352 - Pérdida de Entrenamiento: 0.4133, Precisión de Entrenamiento: 0.8617\n",
      "Val loss: 0.4698, Val acc: 0.8388\n",
      "Gradientes para features.0.0.weight: min=-0.0943632647395134, max=0.11511121690273285, mean=0.0049115512520074844, std=0.03269601985812187\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.08240818977355957, max=0.05783376842737198, mean=6.7150144786864985e-06, std=0.0024623433127999306\n",
      "Gradientes para classifier.1.weight: min=-0.008248250931501389, max=0.006883275229483843, mean=1.6298145125159813e-11, std=0.0011766632087528706\n",
      "1.220703125e-05\n",
      "Epoch 281/400\n",
      "Iteración 98595 - Lote 35/352 - Pérdida de Entrenamiento: 0.4128, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 98630 - Lote 70/352 - Pérdida de Entrenamiento: 0.4137, Precisión de Entrenamiento: 0.8622\n",
      "Iteración 98665 - Lote 105/352 - Pérdida de Entrenamiento: 0.4112, Precisión de Entrenamiento: 0.8633\n",
      "Iteración 98700 - Lote 140/352 - Pérdida de Entrenamiento: 0.4134, Precisión de Entrenamiento: 0.8627\n",
      "Iteración 98735 - Lote 175/352 - Pérdida de Entrenamiento: 0.4130, Precisión de Entrenamiento: 0.8633\n",
      "Iteración 98770 - Lote 210/352 - Pérdida de Entrenamiento: 0.4131, Precisión de Entrenamiento: 0.8624\n",
      "Iteración 98805 - Lote 245/352 - Pérdida de Entrenamiento: 0.4132, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 98840 - Lote 280/352 - Pérdida de Entrenamiento: 0.4123, Precisión de Entrenamiento: 0.8619\n",
      "Iteración 98875 - Lote 315/352 - Pérdida de Entrenamiento: 0.4126, Precisión de Entrenamiento: 0.8623\n",
      "Iteración 98910 - Lote 350/352 - Pérdida de Entrenamiento: 0.4124, Precisión de Entrenamiento: 0.8620\n",
      "Val loss: 0.4668, Val acc: 0.8418\n",
      "Checkpoint guardado en effnet/checkpoints\\best_checkpoint_epoch_281.pth\n",
      "Checkpoint del mejor modelo guardado en la época 281\n",
      "1.220703125e-05\n",
      "Epoch 282/400\n",
      "Iteración 98947 - Lote 35/352 - Pérdida de Entrenamiento: 0.4172, Precisión de Entrenamiento: 0.8607\n",
      "Iteración 98982 - Lote 70/352 - Pérdida de Entrenamiento: 0.4143, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 99017 - Lote 105/352 - Pérdida de Entrenamiento: 0.4077, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 99052 - Lote 140/352 - Pérdida de Entrenamiento: 0.4125, Precisión de Entrenamiento: 0.8622\n",
      "Iteración 99087 - Lote 175/352 - Pérdida de Entrenamiento: 0.4146, Precisión de Entrenamiento: 0.8595\n",
      "Iteración 99122 - Lote 210/352 - Pérdida de Entrenamiento: 0.4149, Precisión de Entrenamiento: 0.8586\n",
      "Iteración 99157 - Lote 245/352 - Pérdida de Entrenamiento: 0.4128, Precisión de Entrenamiento: 0.8594\n",
      "Iteración 99192 - Lote 280/352 - Pérdida de Entrenamiento: 0.4140, Precisión de Entrenamiento: 0.8597\n",
      "Iteración 99227 - Lote 315/352 - Pérdida de Entrenamiento: 0.4145, Precisión de Entrenamiento: 0.8601\n",
      "Iteración 99262 - Lote 350/352 - Pérdida de Entrenamiento: 0.4146, Precisión de Entrenamiento: 0.8601\n",
      "Val loss: 0.4675, Val acc: 0.8416\n",
      "1.220703125e-05\n",
      "Epoch 283/400\n",
      "Iteración 99299 - Lote 35/352 - Pérdida de Entrenamiento: 0.4055, Precisión de Entrenamiento: 0.8609\n",
      "Iteración 99334 - Lote 70/352 - Pérdida de Entrenamiento: 0.4064, Precisión de Entrenamiento: 0.8609\n",
      "Iteración 99369 - Lote 105/352 - Pérdida de Entrenamiento: 0.4150, Precisión de Entrenamiento: 0.8587\n",
      "Iteración 99404 - Lote 140/352 - Pérdida de Entrenamiento: 0.4119, Precisión de Entrenamiento: 0.8597\n",
      "Iteración 99439 - Lote 175/352 - Pérdida de Entrenamiento: 0.4133, Precisión de Entrenamiento: 0.8596\n",
      "Iteración 99474 - Lote 210/352 - Pérdida de Entrenamiento: 0.4099, Precisión de Entrenamiento: 0.8610\n",
      "Iteración 99509 - Lote 245/352 - Pérdida de Entrenamiento: 0.4099, Precisión de Entrenamiento: 0.8611\n",
      "Iteración 99544 - Lote 280/352 - Pérdida de Entrenamiento: 0.4115, Precisión de Entrenamiento: 0.8605\n",
      "Iteración 99579 - Lote 315/352 - Pérdida de Entrenamiento: 0.4114, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 99614 - Lote 350/352 - Pérdida de Entrenamiento: 0.4116, Precisión de Entrenamiento: 0.8608\n",
      "Val loss: 0.4702, Val acc: 0.8372\n",
      "1.220703125e-05\n",
      "Epoch 284/400\n",
      "Iteración 99651 - Lote 35/352 - Pérdida de Entrenamiento: 0.3983, Precisión de Entrenamiento: 0.8712\n",
      "Iteración 99686 - Lote 70/352 - Pérdida de Entrenamiento: 0.4090, Precisión de Entrenamiento: 0.8666\n",
      "Iteración 99721 - Lote 105/352 - Pérdida de Entrenamiento: 0.4065, Precisión de Entrenamiento: 0.8661\n",
      "Iteración 99756 - Lote 140/352 - Pérdida de Entrenamiento: 0.4101, Precisión de Entrenamiento: 0.8647\n",
      "Iteración 99791 - Lote 175/352 - Pérdida de Entrenamiento: 0.4089, Precisión de Entrenamiento: 0.8648\n",
      "Iteración 99826 - Lote 210/352 - Pérdida de Entrenamiento: 0.4097, Precisión de Entrenamiento: 0.8642\n",
      "Iteración 99861 - Lote 245/352 - Pérdida de Entrenamiento: 0.4111, Precisión de Entrenamiento: 0.8633\n",
      "Iteración 99896 - Lote 280/352 - Pérdida de Entrenamiento: 0.4126, Precisión de Entrenamiento: 0.8623\n",
      "Iteración 99931 - Lote 315/352 - Pérdida de Entrenamiento: 0.4123, Precisión de Entrenamiento: 0.8622\n",
      "Iteración 99966 - Lote 350/352 - Pérdida de Entrenamiento: 0.4125, Precisión de Entrenamiento: 0.8623\n",
      "Val loss: 0.4693, Val acc: 0.8400\n",
      "1.220703125e-05\n",
      "Epoch 285/400\n",
      "Iteración 100003 - Lote 35/352 - Pérdida de Entrenamiento: 0.4358, Precisión de Entrenamiento: 0.8542\n",
      "Iteración 100038 - Lote 70/352 - Pérdida de Entrenamiento: 0.4262, Precisión de Entrenamiento: 0.8581\n",
      "Iteración 100073 - Lote 105/352 - Pérdida de Entrenamiento: 0.4166, Precisión de Entrenamiento: 0.8615\n",
      "Iteración 100108 - Lote 140/352 - Pérdida de Entrenamiento: 0.4163, Precisión de Entrenamiento: 0.8609\n",
      "Iteración 100143 - Lote 175/352 - Pérdida de Entrenamiento: 0.4141, Precisión de Entrenamiento: 0.8612\n",
      "Iteración 100178 - Lote 210/352 - Pérdida de Entrenamiento: 0.4094, Precisión de Entrenamiento: 0.8634\n",
      "Iteración 100213 - Lote 245/352 - Pérdida de Entrenamiento: 0.4108, Precisión de Entrenamiento: 0.8633\n",
      "Iteración 100248 - Lote 280/352 - Pérdida de Entrenamiento: 0.4107, Precisión de Entrenamiento: 0.8633\n",
      "Iteración 100283 - Lote 315/352 - Pérdida de Entrenamiento: 0.4117, Precisión de Entrenamiento: 0.8627\n",
      "Iteración 100318 - Lote 350/352 - Pérdida de Entrenamiento: 0.4108, Precisión de Entrenamiento: 0.8630\n",
      "Val loss: 0.4703, Val acc: 0.8388\n",
      "Gradientes para features.0.0.weight: min=-0.16626273095607758, max=0.1470988392829895, mean=0.00589375477284193, std=0.04238385707139969\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.12346763908863068, max=0.14249396324157715, mean=-7.710466888966039e-06, std=0.003696322673931718\n",
      "Gradientes para classifier.1.weight: min=-0.011033347807824612, max=0.007046757265925407, mean=-1.8626451075975936e-11, std=0.0014613228850066662\n",
      "1.220703125e-05\n",
      "Epoch 286/400\n",
      "Iteración 100355 - Lote 35/352 - Pérdida de Entrenamiento: 0.4131, Precisión de Entrenamiento: 0.8603\n",
      "Iteración 100390 - Lote 70/352 - Pérdida de Entrenamiento: 0.4114, Precisión de Entrenamiento: 0.8631\n",
      "Iteración 100425 - Lote 105/352 - Pérdida de Entrenamiento: 0.4070, Precisión de Entrenamiento: 0.8641\n",
      "Iteración 100460 - Lote 140/352 - Pérdida de Entrenamiento: 0.4068, Precisión de Entrenamiento: 0.8638\n",
      "Iteración 100495 - Lote 175/352 - Pérdida de Entrenamiento: 0.4085, Precisión de Entrenamiento: 0.8634\n",
      "Iteración 100530 - Lote 210/352 - Pérdida de Entrenamiento: 0.4080, Precisión de Entrenamiento: 0.8630\n",
      "Iteración 100565 - Lote 245/352 - Pérdida de Entrenamiento: 0.4085, Precisión de Entrenamiento: 0.8632\n",
      "Iteración 100600 - Lote 280/352 - Pérdida de Entrenamiento: 0.4136, Precisión de Entrenamiento: 0.8607\n",
      "Iteración 100635 - Lote 315/352 - Pérdida de Entrenamiento: 0.4130, Precisión de Entrenamiento: 0.8605\n",
      "Iteración 100670 - Lote 350/352 - Pérdida de Entrenamiento: 0.4143, Precisión de Entrenamiento: 0.8599\n",
      "Val loss: 0.4687, Val acc: 0.8394\n",
      "1.220703125e-05\n",
      "Epoch 287/400\n",
      "Iteración 100707 - Lote 35/352 - Pérdida de Entrenamiento: 0.4093, Precisión de Entrenamiento: 0.8699\n",
      "Iteración 100742 - Lote 70/352 - Pérdida de Entrenamiento: 0.4163, Precisión de Entrenamiento: 0.8623\n",
      "Iteración 100777 - Lote 105/352 - Pérdida de Entrenamiento: 0.4178, Precisión de Entrenamiento: 0.8609\n",
      "Iteración 100812 - Lote 140/352 - Pérdida de Entrenamiento: 0.4124, Precisión de Entrenamiento: 0.8630\n",
      "Iteración 100847 - Lote 175/352 - Pérdida de Entrenamiento: 0.4094, Precisión de Entrenamiento: 0.8648\n",
      "Iteración 100882 - Lote 210/352 - Pérdida de Entrenamiento: 0.4127, Precisión de Entrenamiento: 0.8635\n",
      "Iteración 100917 - Lote 245/352 - Pérdida de Entrenamiento: 0.4130, Precisión de Entrenamiento: 0.8632\n",
      "Iteración 100952 - Lote 280/352 - Pérdida de Entrenamiento: 0.4132, Precisión de Entrenamiento: 0.8625\n",
      "Iteración 100987 - Lote 315/352 - Pérdida de Entrenamiento: 0.4117, Precisión de Entrenamiento: 0.8630\n",
      "Iteración 101022 - Lote 350/352 - Pérdida de Entrenamiento: 0.4108, Precisión de Entrenamiento: 0.8633\n",
      "Val loss: 0.4675, Val acc: 0.8382\n",
      "1.220703125e-05\n",
      "Epoch 288/400\n",
      "Iteración 101059 - Lote 35/352 - Pérdida de Entrenamiento: 0.4180, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 101094 - Lote 70/352 - Pérdida de Entrenamiento: 0.4156, Precisión de Entrenamiento: 0.8605\n",
      "Iteración 101129 - Lote 105/352 - Pérdida de Entrenamiento: 0.4096, Precisión de Entrenamiento: 0.8620\n",
      "Iteración 101164 - Lote 140/352 - Pérdida de Entrenamiento: 0.4059, Precisión de Entrenamiento: 0.8631\n",
      "Iteración 101199 - Lote 175/352 - Pérdida de Entrenamiento: 0.4118, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 101234 - Lote 210/352 - Pérdida de Entrenamiento: 0.4124, Precisión de Entrenamiento: 0.8613\n",
      "Iteración 101269 - Lote 245/352 - Pérdida de Entrenamiento: 0.4113, Precisión de Entrenamiento: 0.8617\n",
      "Iteración 101304 - Lote 280/352 - Pérdida de Entrenamiento: 0.4102, Precisión de Entrenamiento: 0.8622\n",
      "Iteración 101339 - Lote 315/352 - Pérdida de Entrenamiento: 0.4105, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 101374 - Lote 350/352 - Pérdida de Entrenamiento: 0.4116, Precisión de Entrenamiento: 0.8621\n",
      "Val loss: 0.4710, Val acc: 0.8380\n",
      "1.220703125e-05\n",
      "Epoch 289/400\n",
      "Iteración 101411 - Lote 35/352 - Pérdida de Entrenamiento: 0.4170, Precisión de Entrenamiento: 0.8603\n",
      "Iteración 101446 - Lote 70/352 - Pérdida de Entrenamiento: 0.4149, Precisión de Entrenamiento: 0.8604\n",
      "Iteración 101481 - Lote 105/352 - Pérdida de Entrenamiento: 0.4085, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 101516 - Lote 140/352 - Pérdida de Entrenamiento: 0.4102, Precisión de Entrenamiento: 0.8602\n",
      "Iteración 101551 - Lote 175/352 - Pérdida de Entrenamiento: 0.4125, Precisión de Entrenamiento: 0.8606\n",
      "Iteración 101586 - Lote 210/352 - Pérdida de Entrenamiento: 0.4130, Precisión de Entrenamiento: 0.8603\n",
      "Iteración 101621 - Lote 245/352 - Pérdida de Entrenamiento: 0.4145, Precisión de Entrenamiento: 0.8603\n",
      "Iteración 101656 - Lote 280/352 - Pérdida de Entrenamiento: 0.4169, Precisión de Entrenamiento: 0.8599\n",
      "Iteración 101691 - Lote 315/352 - Pérdida de Entrenamiento: 0.4151, Precisión de Entrenamiento: 0.8607\n",
      "Iteración 101726 - Lote 350/352 - Pérdida de Entrenamiento: 0.4139, Precisión de Entrenamiento: 0.8611\n",
      "Val loss: 0.4701, Val acc: 0.8388\n",
      "1.220703125e-05\n",
      "Epoch 290/400\n",
      "Iteración 101763 - Lote 35/352 - Pérdida de Entrenamiento: 0.3937, Precisión de Entrenamiento: 0.8692\n",
      "Iteración 101798 - Lote 70/352 - Pérdida de Entrenamiento: 0.4007, Precisión de Entrenamiento: 0.8686\n",
      "Iteración 101833 - Lote 105/352 - Pérdida de Entrenamiento: 0.4087, Precisión de Entrenamiento: 0.8644\n",
      "Iteración 101868 - Lote 140/352 - Pérdida de Entrenamiento: 0.4076, Precisión de Entrenamiento: 0.8642\n",
      "Iteración 101903 - Lote 175/352 - Pérdida de Entrenamiento: 0.4075, Precisión de Entrenamiento: 0.8644\n",
      "Iteración 101938 - Lote 210/352 - Pérdida de Entrenamiento: 0.4099, Precisión de Entrenamiento: 0.8632\n",
      "Iteración 101973 - Lote 245/352 - Pérdida de Entrenamiento: 0.4107, Precisión de Entrenamiento: 0.8626\n",
      "Iteración 102008 - Lote 280/352 - Pérdida de Entrenamiento: 0.4116, Precisión de Entrenamiento: 0.8616\n",
      "Iteración 102043 - Lote 315/352 - Pérdida de Entrenamiento: 0.4098, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 102078 - Lote 350/352 - Pérdida de Entrenamiento: 0.4089, Precisión de Entrenamiento: 0.8624\n",
      "Val loss: 0.4712, Val acc: 0.8392\n",
      "Gradientes para features.0.0.weight: min=-0.1544669270515442, max=0.15946398675441742, mean=0.009997243992984295, std=0.04561739042401314\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.15319909155368805, max=0.19729138910770416, mean=5.311188942869194e-06, std=0.004496096633374691\n",
      "Gradientes para classifier.1.weight: min=-0.012278749607503414, max=0.010874154977500439, mean=2.7939676613963904e-11, std=0.0013929366832599044\n",
      "6.103515625e-06\n",
      "Epoch 291/400\n",
      "Iteración 102115 - Lote 35/352 - Pérdida de Entrenamiento: 0.3986, Precisión de Entrenamiento: 0.8670\n",
      "Iteración 102150 - Lote 70/352 - Pérdida de Entrenamiento: 0.4107, Precisión de Entrenamiento: 0.8643\n",
      "Iteración 102185 - Lote 105/352 - Pérdida de Entrenamiento: 0.4100, Precisión de Entrenamiento: 0.8650\n",
      "Iteración 102220 - Lote 140/352 - Pérdida de Entrenamiento: 0.4093, Precisión de Entrenamiento: 0.8643\n",
      "Iteración 102255 - Lote 175/352 - Pérdida de Entrenamiento: 0.4129, Precisión de Entrenamiento: 0.8636\n",
      "Iteración 102290 - Lote 210/352 - Pérdida de Entrenamiento: 0.4098, Precisión de Entrenamiento: 0.8638\n",
      "Iteración 102325 - Lote 245/352 - Pérdida de Entrenamiento: 0.4095, Precisión de Entrenamiento: 0.8632\n",
      "Iteración 102360 - Lote 280/352 - Pérdida de Entrenamiento: 0.4086, Precisión de Entrenamiento: 0.8636\n",
      "Iteración 102395 - Lote 315/352 - Pérdida de Entrenamiento: 0.4095, Precisión de Entrenamiento: 0.8629\n",
      "Iteración 102430 - Lote 350/352 - Pérdida de Entrenamiento: 0.4089, Precisión de Entrenamiento: 0.8632\n",
      "Val loss: 0.4698, Val acc: 0.8384\n",
      "6.103515625e-06\n",
      "Epoch 292/400\n",
      "Iteración 102467 - Lote 35/352 - Pérdida de Entrenamiento: 0.3968, Precisión de Entrenamiento: 0.8694\n",
      "Iteración 102502 - Lote 70/352 - Pérdida de Entrenamiento: 0.3990, Precisión de Entrenamiento: 0.8685\n",
      "Iteración 102537 - Lote 105/352 - Pérdida de Entrenamiento: 0.4060, Precisión de Entrenamiento: 0.8664\n",
      "Iteración 102572 - Lote 140/352 - Pérdida de Entrenamiento: 0.4046, Precisión de Entrenamiento: 0.8657\n",
      "Iteración 102607 - Lote 175/352 - Pérdida de Entrenamiento: 0.4074, Precisión de Entrenamiento: 0.8661\n",
      "Iteración 102642 - Lote 210/352 - Pérdida de Entrenamiento: 0.4042, Precisión de Entrenamiento: 0.8669\n",
      "Iteración 102677 - Lote 245/352 - Pérdida de Entrenamiento: 0.4060, Precisión de Entrenamiento: 0.8657\n",
      "Iteración 102712 - Lote 280/352 - Pérdida de Entrenamiento: 0.4083, Precisión de Entrenamiento: 0.8648\n",
      "Iteración 102747 - Lote 315/352 - Pérdida de Entrenamiento: 0.4114, Precisión de Entrenamiento: 0.8639\n",
      "Iteración 102782 - Lote 350/352 - Pérdida de Entrenamiento: 0.4129, Precisión de Entrenamiento: 0.8624\n",
      "Val loss: 0.4687, Val acc: 0.8398\n",
      "6.103515625e-06\n",
      "Epoch 293/400\n",
      "Iteración 102819 - Lote 35/352 - Pérdida de Entrenamiento: 0.4033, Precisión de Entrenamiento: 0.8665\n",
      "Iteración 102854 - Lote 70/352 - Pérdida de Entrenamiento: 0.4098, Precisión de Entrenamiento: 0.8629\n",
      "Iteración 102889 - Lote 105/352 - Pérdida de Entrenamiento: 0.4120, Precisión de Entrenamiento: 0.8616\n",
      "Iteración 102924 - Lote 140/352 - Pérdida de Entrenamiento: 0.4120, Precisión de Entrenamiento: 0.8626\n",
      "Iteración 102959 - Lote 175/352 - Pérdida de Entrenamiento: 0.4180, Precisión de Entrenamiento: 0.8607\n",
      "Iteración 102994 - Lote 210/352 - Pérdida de Entrenamiento: 0.4202, Precisión de Entrenamiento: 0.8598\n",
      "Iteración 103029 - Lote 245/352 - Pérdida de Entrenamiento: 0.4187, Precisión de Entrenamiento: 0.8611\n",
      "Iteración 103064 - Lote 280/352 - Pérdida de Entrenamiento: 0.4155, Precisión de Entrenamiento: 0.8617\n",
      "Iteración 103099 - Lote 315/352 - Pérdida de Entrenamiento: 0.4159, Precisión de Entrenamiento: 0.8613\n",
      "Iteración 103134 - Lote 350/352 - Pérdida de Entrenamiento: 0.4141, Precisión de Entrenamiento: 0.8617\n",
      "Val loss: 0.4709, Val acc: 0.8402\n",
      "6.103515625e-06\n",
      "Epoch 294/400\n",
      "Iteración 103171 - Lote 35/352 - Pérdida de Entrenamiento: 0.3998, Precisión de Entrenamiento: 0.8665\n",
      "Iteración 103206 - Lote 70/352 - Pérdida de Entrenamiento: 0.4150, Precisión de Entrenamiento: 0.8625\n",
      "Iteración 103241 - Lote 105/352 - Pérdida de Entrenamiento: 0.4110, Precisión de Entrenamiento: 0.8626\n",
      "Iteración 103276 - Lote 140/352 - Pérdida de Entrenamiento: 0.4114, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 103311 - Lote 175/352 - Pérdida de Entrenamiento: 0.4129, Precisión de Entrenamiento: 0.8617\n",
      "Iteración 103346 - Lote 210/352 - Pérdida de Entrenamiento: 0.4108, Precisión de Entrenamiento: 0.8625\n",
      "Iteración 103381 - Lote 245/352 - Pérdida de Entrenamiento: 0.4114, Precisión de Entrenamiento: 0.8619\n",
      "Iteración 103416 - Lote 280/352 - Pérdida de Entrenamiento: 0.4121, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 103451 - Lote 315/352 - Pérdida de Entrenamiento: 0.4112, Precisión de Entrenamiento: 0.8625\n",
      "Iteración 103486 - Lote 350/352 - Pérdida de Entrenamiento: 0.4107, Precisión de Entrenamiento: 0.8625\n",
      "Val loss: 0.4720, Val acc: 0.8398\n",
      "6.103515625e-06\n",
      "Epoch 295/400\n",
      "Iteración 103523 - Lote 35/352 - Pérdida de Entrenamiento: 0.4152, Precisión de Entrenamiento: 0.8634\n",
      "Iteración 103558 - Lote 70/352 - Pérdida de Entrenamiento: 0.4221, Precisión de Entrenamiento: 0.8607\n",
      "Iteración 103593 - Lote 105/352 - Pérdida de Entrenamiento: 0.4239, Precisión de Entrenamiento: 0.8592\n",
      "Iteración 103628 - Lote 140/352 - Pérdida de Entrenamiento: 0.4222, Precisión de Entrenamiento: 0.8593\n",
      "Iteración 103663 - Lote 175/352 - Pérdida de Entrenamiento: 0.4212, Precisión de Entrenamiento: 0.8591\n",
      "Iteración 103698 - Lote 210/352 - Pérdida de Entrenamiento: 0.4193, Precisión de Entrenamiento: 0.8600\n",
      "Iteración 103733 - Lote 245/352 - Pérdida de Entrenamiento: 0.4173, Precisión de Entrenamiento: 0.8601\n",
      "Iteración 103768 - Lote 280/352 - Pérdida de Entrenamiento: 0.4181, Precisión de Entrenamiento: 0.8596\n",
      "Iteración 103803 - Lote 315/352 - Pérdida de Entrenamiento: 0.4144, Precisión de Entrenamiento: 0.8603\n",
      "Iteración 103838 - Lote 350/352 - Pérdida de Entrenamiento: 0.4139, Precisión de Entrenamiento: 0.8607\n",
      "Val loss: 0.4683, Val acc: 0.8404\n",
      "Gradientes para features.0.0.weight: min=-0.2589016854763031, max=0.17041762173175812, mean=0.001266404753550887, std=0.056580621749162674\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.23707756400108337, max=0.08245060592889786, mean=-1.806234104151372e-05, std=0.0029120140243321657\n",
      "Gradientes para classifier.1.weight: min=-0.006430106237530708, max=0.0066819218918681145, mean=2.328306384496992e-12, std=0.0008874629274941981\n",
      "6.103515625e-06\n",
      "Epoch 296/400\n",
      "Iteración 103875 - Lote 35/352 - Pérdida de Entrenamiento: 0.4105, Precisión de Entrenamiento: 0.8627\n",
      "Iteración 103910 - Lote 70/352 - Pérdida de Entrenamiento: 0.4092, Precisión de Entrenamiento: 0.8607\n",
      "Iteración 103945 - Lote 105/352 - Pérdida de Entrenamiento: 0.4086, Precisión de Entrenamiento: 0.8615\n",
      "Iteración 103980 - Lote 140/352 - Pérdida de Entrenamiento: 0.4131, Precisión de Entrenamiento: 0.8606\n",
      "Iteración 104015 - Lote 175/352 - Pérdida de Entrenamiento: 0.4131, Precisión de Entrenamiento: 0.8602\n",
      "Iteración 104050 - Lote 210/352 - Pérdida de Entrenamiento: 0.4128, Precisión de Entrenamiento: 0.8606\n",
      "Iteración 104085 - Lote 245/352 - Pérdida de Entrenamiento: 0.4100, Precisión de Entrenamiento: 0.8614\n",
      "Iteración 104120 - Lote 280/352 - Pérdida de Entrenamiento: 0.4070, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 104155 - Lote 315/352 - Pérdida de Entrenamiento: 0.4094, Precisión de Entrenamiento: 0.8613\n",
      "Iteración 104190 - Lote 350/352 - Pérdida de Entrenamiento: 0.4092, Precisión de Entrenamiento: 0.8614\n",
      "Val loss: 0.4704, Val acc: 0.8408\n",
      "6.103515625e-06\n",
      "Epoch 297/400\n",
      "Iteración 104227 - Lote 35/352 - Pérdida de Entrenamiento: 0.4063, Precisión de Entrenamiento: 0.8667\n",
      "Iteración 104262 - Lote 70/352 - Pérdida de Entrenamiento: 0.4117, Precisión de Entrenamiento: 0.8627\n",
      "Iteración 104297 - Lote 105/352 - Pérdida de Entrenamiento: 0.4113, Precisión de Entrenamiento: 0.8624\n",
      "Iteración 104332 - Lote 140/352 - Pérdida de Entrenamiento: 0.4082, Precisión de Entrenamiento: 0.8624\n",
      "Iteración 104367 - Lote 175/352 - Pérdida de Entrenamiento: 0.4096, Precisión de Entrenamiento: 0.8623\n",
      "Iteración 104402 - Lote 210/352 - Pérdida de Entrenamiento: 0.4124, Precisión de Entrenamiento: 0.8624\n",
      "Iteración 104437 - Lote 245/352 - Pérdida de Entrenamiento: 0.4115, Precisión de Entrenamiento: 0.8628\n",
      "Iteración 104472 - Lote 280/352 - Pérdida de Entrenamiento: 0.4114, Precisión de Entrenamiento: 0.8624\n",
      "Iteración 104507 - Lote 315/352 - Pérdida de Entrenamiento: 0.4128, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 104542 - Lote 350/352 - Pérdida de Entrenamiento: 0.4143, Precisión de Entrenamiento: 0.8614\n",
      "Val loss: 0.4700, Val acc: 0.8398\n",
      "6.103515625e-06\n",
      "Epoch 298/400\n",
      "Iteración 104579 - Lote 35/352 - Pérdida de Entrenamiento: 0.4288, Precisión de Entrenamiento: 0.8551\n",
      "Iteración 104614 - Lote 70/352 - Pérdida de Entrenamiento: 0.4278, Precisión de Entrenamiento: 0.8537\n",
      "Iteración 104649 - Lote 105/352 - Pérdida de Entrenamiento: 0.4179, Precisión de Entrenamiento: 0.8579\n",
      "Iteración 104684 - Lote 140/352 - Pérdida de Entrenamiento: 0.4121, Precisión de Entrenamiento: 0.8611\n",
      "Iteración 104719 - Lote 175/352 - Pérdida de Entrenamiento: 0.4073, Precisión de Entrenamiento: 0.8634\n",
      "Iteración 104754 - Lote 210/352 - Pérdida de Entrenamiento: 0.4083, Precisión de Entrenamiento: 0.8624\n",
      "Iteración 104789 - Lote 245/352 - Pérdida de Entrenamiento: 0.4097, Precisión de Entrenamiento: 0.8620\n",
      "Iteración 104824 - Lote 280/352 - Pérdida de Entrenamiento: 0.4123, Precisión de Entrenamiento: 0.8608\n",
      "Iteración 104859 - Lote 315/352 - Pérdida de Entrenamiento: 0.4131, Precisión de Entrenamiento: 0.8605\n",
      "Iteración 104894 - Lote 350/352 - Pérdida de Entrenamiento: 0.4116, Precisión de Entrenamiento: 0.8616\n",
      "Val loss: 0.4714, Val acc: 0.8384\n",
      "6.103515625e-06\n",
      "Epoch 299/400\n",
      "Iteración 104931 - Lote 35/352 - Pérdida de Entrenamiento: 0.4107, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 104966 - Lote 70/352 - Pérdida de Entrenamiento: 0.4070, Precisión de Entrenamiento: 0.8647\n",
      "Iteración 105001 - Lote 105/352 - Pérdida de Entrenamiento: 0.4068, Precisión de Entrenamiento: 0.8643\n",
      "Iteración 105036 - Lote 140/352 - Pérdida de Entrenamiento: 0.4110, Precisión de Entrenamiento: 0.8623\n",
      "Iteración 105071 - Lote 175/352 - Pérdida de Entrenamiento: 0.4136, Precisión de Entrenamiento: 0.8625\n",
      "Iteración 105106 - Lote 210/352 - Pérdida de Entrenamiento: 0.4139, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 105141 - Lote 245/352 - Pérdida de Entrenamiento: 0.4143, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 105176 - Lote 280/352 - Pérdida de Entrenamiento: 0.4120, Precisión de Entrenamiento: 0.8623\n",
      "Iteración 105211 - Lote 315/352 - Pérdida de Entrenamiento: 0.4115, Precisión de Entrenamiento: 0.8624\n",
      "Iteración 105246 - Lote 350/352 - Pérdida de Entrenamiento: 0.4116, Precisión de Entrenamiento: 0.8623\n",
      "Val loss: 0.4703, Val acc: 0.8394\n",
      "3.0517578125e-06\n",
      "Epoch 300/400\n",
      "Iteración 105283 - Lote 35/352 - Pérdida de Entrenamiento: 0.4190, Precisión de Entrenamiento: 0.8623\n",
      "Iteración 105318 - Lote 70/352 - Pérdida de Entrenamiento: 0.4063, Precisión de Entrenamiento: 0.8663\n",
      "Iteración 105353 - Lote 105/352 - Pérdida de Entrenamiento: 0.4130, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 105388 - Lote 140/352 - Pérdida de Entrenamiento: 0.4163, Precisión de Entrenamiento: 0.8603\n",
      "Iteración 105423 - Lote 175/352 - Pérdida de Entrenamiento: 0.4167, Precisión de Entrenamiento: 0.8601\n",
      "Iteración 105458 - Lote 210/352 - Pérdida de Entrenamiento: 0.4158, Precisión de Entrenamiento: 0.8605\n",
      "Iteración 105493 - Lote 245/352 - Pérdida de Entrenamiento: 0.4172, Precisión de Entrenamiento: 0.8601\n",
      "Iteración 105528 - Lote 280/352 - Pérdida de Entrenamiento: 0.4148, Precisión de Entrenamiento: 0.8614\n",
      "Iteración 105563 - Lote 315/352 - Pérdida de Entrenamiento: 0.4125, Precisión de Entrenamiento: 0.8615\n",
      "Iteración 105598 - Lote 350/352 - Pérdida de Entrenamiento: 0.4130, Precisión de Entrenamiento: 0.8608\n",
      "Val loss: 0.4712, Val acc: 0.8392\n",
      "Gradientes para features.0.0.weight: min=-0.21443483233451843, max=0.16258078813552856, mean=0.016175389289855957, std=0.055985335260629654\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.16269820928573608, max=0.10285724699497223, mean=-7.1282593125943094e-06, std=0.004290223121643066\n",
      "Gradientes para classifier.1.weight: min=-0.012388616800308228, max=0.009821849875152111, mean=2.3283064712331658e-11, std=0.0016866675578057766\n",
      "3.0517578125e-06\n",
      "Epoch 301/400\n",
      "Iteración 105635 - Lote 35/352 - Pérdida de Entrenamiento: 0.4083, Precisión de Entrenamiento: 0.8652\n",
      "Iteración 105670 - Lote 70/352 - Pérdida de Entrenamiento: 0.4155, Precisión de Entrenamiento: 0.8637\n",
      "Iteración 105705 - Lote 105/352 - Pérdida de Entrenamiento: 0.4162, Precisión de Entrenamiento: 0.8626\n",
      "Iteración 105740 - Lote 140/352 - Pérdida de Entrenamiento: 0.4111, Precisión de Entrenamiento: 0.8638\n",
      "Iteración 105775 - Lote 175/352 - Pérdida de Entrenamiento: 0.4120, Precisión de Entrenamiento: 0.8637\n",
      "Iteración 105810 - Lote 210/352 - Pérdida de Entrenamiento: 0.4114, Precisión de Entrenamiento: 0.8636\n",
      "Iteración 105845 - Lote 245/352 - Pérdida de Entrenamiento: 0.4119, Precisión de Entrenamiento: 0.8639\n",
      "Iteración 105880 - Lote 280/352 - Pérdida de Entrenamiento: 0.4105, Precisión de Entrenamiento: 0.8642\n",
      "Iteración 105915 - Lote 315/352 - Pérdida de Entrenamiento: 0.4092, Precisión de Entrenamiento: 0.8639\n",
      "Iteración 105950 - Lote 350/352 - Pérdida de Entrenamiento: 0.4097, Precisión de Entrenamiento: 0.8639\n",
      "Val loss: 0.4696, Val acc: 0.8404\n",
      "3.0517578125e-06\n",
      "Epoch 302/400\n",
      "Iteración 105987 - Lote 35/352 - Pérdida de Entrenamiento: 0.4089, Precisión de Entrenamiento: 0.8667\n",
      "Iteración 106022 - Lote 70/352 - Pérdida de Entrenamiento: 0.4170, Precisión de Entrenamiento: 0.8641\n",
      "Iteración 106057 - Lote 105/352 - Pérdida de Entrenamiento: 0.4132, Precisión de Entrenamiento: 0.8645\n",
      "Iteración 106092 - Lote 140/352 - Pérdida de Entrenamiento: 0.4149, Precisión de Entrenamiento: 0.8629\n",
      "Iteración 106127 - Lote 175/352 - Pérdida de Entrenamiento: 0.4156, Precisión de Entrenamiento: 0.8625\n",
      "Iteración 106162 - Lote 210/352 - Pérdida de Entrenamiento: 0.4124, Precisión de Entrenamiento: 0.8631\n",
      "Iteración 106197 - Lote 245/352 - Pérdida de Entrenamiento: 0.4099, Precisión de Entrenamiento: 0.8629\n",
      "Iteración 106232 - Lote 280/352 - Pérdida de Entrenamiento: 0.4082, Precisión de Entrenamiento: 0.8635\n",
      "Iteración 106267 - Lote 315/352 - Pérdida de Entrenamiento: 0.4092, Precisión de Entrenamiento: 0.8629\n",
      "Iteración 106302 - Lote 350/352 - Pérdida de Entrenamiento: 0.4088, Precisión de Entrenamiento: 0.8630\n",
      "Val loss: 0.4694, Val acc: 0.8386\n",
      "3.0517578125e-06\n",
      "Epoch 303/400\n",
      "Iteración 106339 - Lote 35/352 - Pérdida de Entrenamiento: 0.4250, Precisión de Entrenamiento: 0.8507\n",
      "Iteración 106374 - Lote 70/352 - Pérdida de Entrenamiento: 0.4201, Precisión de Entrenamiento: 0.8545\n",
      "Iteración 106409 - Lote 105/352 - Pérdida de Entrenamiento: 0.4233, Precisión de Entrenamiento: 0.8548\n",
      "Iteración 106444 - Lote 140/352 - Pérdida de Entrenamiento: 0.4197, Precisión de Entrenamiento: 0.8567\n",
      "Iteración 106479 - Lote 175/352 - Pérdida de Entrenamiento: 0.4172, Precisión de Entrenamiento: 0.8575\n",
      "Iteración 106514 - Lote 210/352 - Pérdida de Entrenamiento: 0.4163, Precisión de Entrenamiento: 0.8578\n",
      "Iteración 106549 - Lote 245/352 - Pérdida de Entrenamiento: 0.4150, Precisión de Entrenamiento: 0.8577\n",
      "Iteración 106584 - Lote 280/352 - Pérdida de Entrenamiento: 0.4162, Precisión de Entrenamiento: 0.8576\n",
      "Iteración 106619 - Lote 315/352 - Pérdida de Entrenamiento: 0.4150, Precisión de Entrenamiento: 0.8589\n",
      "Iteración 106654 - Lote 350/352 - Pérdida de Entrenamiento: 0.4151, Precisión de Entrenamiento: 0.8588\n",
      "Val loss: 0.4703, Val acc: 0.8410\n",
      "3.0517578125e-06\n",
      "Epoch 304/400\n",
      "Iteración 106691 - Lote 35/352 - Pérdida de Entrenamiento: 0.4166, Precisión de Entrenamiento: 0.8554\n",
      "Iteración 106726 - Lote 70/352 - Pérdida de Entrenamiento: 0.4090, Precisión de Entrenamiento: 0.8588\n",
      "Iteración 106761 - Lote 105/352 - Pérdida de Entrenamiento: 0.4198, Precisión de Entrenamiento: 0.8564\n",
      "Iteración 106796 - Lote 140/352 - Pérdida de Entrenamiento: 0.4160, Precisión de Entrenamiento: 0.8590\n",
      "Iteración 106831 - Lote 175/352 - Pérdida de Entrenamiento: 0.4102, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 106866 - Lote 210/352 - Pérdida de Entrenamiento: 0.4104, Precisión de Entrenamiento: 0.8621\n",
      "Iteración 106901 - Lote 245/352 - Pérdida de Entrenamiento: 0.4107, Precisión de Entrenamiento: 0.8627\n",
      "Iteración 106936 - Lote 280/352 - Pérdida de Entrenamiento: 0.4106, Precisión de Entrenamiento: 0.8634\n",
      "Iteración 106971 - Lote 315/352 - Pérdida de Entrenamiento: 0.4103, Precisión de Entrenamiento: 0.8631\n",
      "Iteración 107006 - Lote 350/352 - Pérdida de Entrenamiento: 0.4108, Precisión de Entrenamiento: 0.8624\n",
      "Val loss: 0.4700, Val acc: 0.8384\n",
      "3.0517578125e-06\n",
      "Epoch 305/400\n",
      "Iteración 107043 - Lote 35/352 - Pérdida de Entrenamiento: 0.3966, Precisión de Entrenamiento: 0.8658\n",
      "Iteración 107078 - Lote 70/352 - Pérdida de Entrenamiento: 0.3979, Precisión de Entrenamiento: 0.8619\n",
      "Iteración 107113 - Lote 105/352 - Pérdida de Entrenamiento: 0.4053, Precisión de Entrenamiento: 0.8599\n",
      "Iteración 107148 - Lote 140/352 - Pérdida de Entrenamiento: 0.4088, Precisión de Entrenamiento: 0.8605\n",
      "Iteración 107183 - Lote 175/352 - Pérdida de Entrenamiento: 0.4085, Precisión de Entrenamiento: 0.8618\n",
      "Iteración 107218 - Lote 210/352 - Pérdida de Entrenamiento: 0.4117, Precisión de Entrenamiento: 0.8611\n",
      "Iteración 107253 - Lote 245/352 - Pérdida de Entrenamiento: 0.4100, Precisión de Entrenamiento: 0.8625\n",
      "Iteración 107288 - Lote 280/352 - Pérdida de Entrenamiento: 0.4102, Precisión de Entrenamiento: 0.8624\n",
      "Iteración 107323 - Lote 315/352 - Pérdida de Entrenamiento: 0.4095, Precisión de Entrenamiento: 0.8626\n",
      "Iteración 107358 - Lote 350/352 - Pérdida de Entrenamiento: 0.4106, Precisión de Entrenamiento: 0.8625\n",
      "Val loss: 0.4712, Val acc: 0.8396\n",
      "Gradientes para features.0.0.weight: min=-0.2045588493347168, max=0.22958077490329742, mean=0.01579163782298565, std=0.05684645101428032\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.1189253181219101, max=0.17724519968032837, mean=-3.954626208724221e-06, std=0.003934685606509447\n",
      "Gradientes para classifier.1.weight: min=-0.008309452794492245, max=0.009776334278285503, mean=2.7939676613963904e-11, std=0.0014632599195465446\n",
      "3.0517578125e-06\n",
      "Epoch 306/400\n",
      "Iteración 107395 - Lote 35/352 - Pérdida de Entrenamiento: 0.4226, Precisión de Entrenamiento: 0.8540\n",
      "Iteración 107430 - Lote 70/352 - Pérdida de Entrenamiento: 0.4229, Precisión de Entrenamiento: 0.8567\n",
      "Iteración 107465 - Lote 105/352 - Pérdida de Entrenamiento: 0.4148, Precisión de Entrenamiento: 0.8603\n",
      "Iteración 107500 - Lote 140/352 - Pérdida de Entrenamiento: 0.4205, Precisión de Entrenamiento: 0.8590\n",
      "Iteración 107535 - Lote 175/352 - Pérdida de Entrenamiento: 0.4204, Precisión de Entrenamiento: 0.8581\n",
      "Iteración 107570 - Lote 210/352 - Pérdida de Entrenamiento: 0.4167, Precisión de Entrenamiento: 0.8593\n",
      "Iteración 107605 - Lote 245/352 - Pérdida de Entrenamiento: 0.4153, Precisión de Entrenamiento: 0.8597\n",
      "Iteración 107640 - Lote 280/352 - Pérdida de Entrenamiento: 0.4141, Precisión de Entrenamiento: 0.8604\n",
      "Iteración 107675 - Lote 315/352 - Pérdida de Entrenamiento: 0.4128, Precisión de Entrenamiento: 0.8609\n",
      "Iteración 107710 - Lote 350/352 - Pérdida de Entrenamiento: 0.4141, Precisión de Entrenamiento: 0.8601\n",
      "Val loss: 0.4724, Val acc: 0.8378\n",
      "Gradientes para features.0.0.weight: min=-0.18037618696689606, max=0.15918058156967163, mean=0.014023222960531712, std=0.04841699078679085\n",
      "Gradientes para features.5.0.block.0.0.weight: min=-0.10749306529760361, max=0.11973533034324646, mean=-1.1041388461308088e-06, std=0.003032119944691658\n",
      "Gradientes para classifier.1.weight: min=-0.014641856774687767, max=0.00916765071451664, mean=2.211891086956186e-11, std=0.0013013334246352315\n",
      "Early stopping at epoch 306\n",
      "\n",
      "Tiempo total de entrenamiento: 11145.39 segundos\n",
      "\n",
      "Entrenamiento completado exitosamente\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from eff import _efficientnet, _efficientnet_conf\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, filename):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(filename, 'a', encoding='utf-8')\n",
    "        \n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "        self.log.flush()\n",
    "        \n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        self.log.flush()\n",
    "\n",
    "\n",
    "# Transformaciones para entrenamiento y validación\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomCrop(32, padding=4),  # Recorte aleatorio después del padding\n",
    "    torchvision.transforms.RandomHorizontalFlip(),      # Volteo horizontal aleatorio\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "])\n",
    "\n",
    "val_test_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "])\n",
    "\n",
    "# Cargar datasets CIFAR-10 con transformaciones\n",
    "train_cifar10 = torchvision.datasets.CIFAR10(\n",
    "    root=\"./cifar10\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=train_transform  # Se aplica data augmentation aquí\n",
    ")\n",
    "\n",
    "test_cifar10 = torchvision.datasets.CIFAR10(\n",
    "    root=\"./cifar10\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=val_test_transform  # Solo normalización para testing\n",
    ")\n",
    "\n",
    "# Dividir dataset de entrenamiento y validación\n",
    "train_cifar10, _ = torch.utils.data.random_split(train_cifar10, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
    "_, val_cifar10 = torch.utils.data.random_split(\n",
    "    torchvision.datasets.CIFAR10(root=\"./cifar10\", train=True, transform=val_test_transform),\n",
    "    [45000, 5000], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "def show_gradients_EfficientNet(model, epoch):\n",
    "    # Crear directorio para los gradientes si no existe\n",
    "    gradients_dir = os.path.join(checkpoint_dir, 'gradients')\n",
    "    epoch_dir = os.path.join(gradients_dir, f'epoch_{epoch}')\n",
    "    os.makedirs(epoch_dir, exist_ok=True)\n",
    "\n",
    "    layers_to_show = ['features.0.0.weight', 'features.5.0.block.0.0.weight', 'classifier.1.weight']\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(layer in name for layer in layers_to_show) and param.requires_grad and param.grad is not None:\n",
    "            grad = param.grad.cpu().numpy()\n",
    "            print(f\"Gradientes para {name}: min={grad.min()}, max={grad.max()}, mean={grad.mean()}, std={grad.std()}\")\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.hist(grad.flatten(), bins=50)\n",
    "            plt.title(f'Gradientes para {name}')\n",
    "            plt.xlabel('Valor del gradiente')\n",
    "            plt.ylabel('Frecuencia')\n",
    "            \n",
    "            # Guardar el gráfico en lugar de mostrarlo\n",
    "            filename = f'{name.replace(\".\", \"_\")}.png'\n",
    "            plt.savefig(os.path.join(epoch_dir, filename))\n",
    "            plt.close()  # Cerrar la figura para liberar memoria\n",
    "\n",
    "\n",
    "\n",
    "def show_curves(curves):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "    fig.set_facecolor('white')\n",
    "\n",
    "    # Asegúrate de que los datos estén en la CPU antes de convertirlos a NumPy\n",
    "    epochs = np.arange(len(curves[\"val_loss\"])) + 1\n",
    "\n",
    "    ax[0].plot(epochs, np.array(curves['val_loss']), label='validation')\n",
    "    ax[0].plot(epochs, np.array(curves['train_loss']), label='training')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_title('Loss evolution during training')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(epochs, np.array(curves['val_acc']), label='validation')\n",
    "    ax[1].plot(epochs, np.array(curves['train_acc']), label='training')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_title('Accuracy evolution during training')\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize feature maps\n",
    "def show_feature_maps(feature_maps, epoch):\n",
    "    if not feature_maps:  # Si no hay feature maps, salir\n",
    "        print(\"No hay feature maps para mostrar\")\n",
    "        return\n",
    "        \n",
    "    feature_maps_dir = os.path.join(checkpoint_dir, 'feature_maps')\n",
    "    epoch_dir = os.path.join(feature_maps_dir, f'epoch_{epoch}')\n",
    "    os.makedirs(epoch_dir, exist_ok=True)\n",
    "\n",
    "    for layer_name, feature_map in feature_maps.items():\n",
    "        # Asegurarse de que feature_map está en CPU y es numpy array\n",
    "        fmap = feature_map.cpu().numpy()\n",
    "        \n",
    "        # Para visualización, tomar el primer batch\n",
    "        if len(fmap.shape) == 4:  # (batch, channels, height, width)\n",
    "            fmap = fmap[0]  # Tomar primer elemento del batch\n",
    "            \n",
    "        # Seleccionar solo los primeros 8 canales para visualización\n",
    "        num_channels = min(8, fmap.shape[0])\n",
    "        \n",
    "        fig, axes = plt.subplots(1, num_channels, figsize=(20, 5))\n",
    "        fig.suptitle(f\"Feature Maps - {layer_name} (Epoch {epoch})\")\n",
    "        \n",
    "        for i in range(num_channels):\n",
    "            axes[i].imshow(fmap[i], cmap='viridis')\n",
    "            axes[i].axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        filename = f'feature_maps_{layer_name}_epoch_{epoch}.png'\n",
    "        plt.savefig(os.path.join(epoch_dir, filename))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def checkpoint_save(model, optimizer, epoch, filename):\n",
    "    checkpoint_data = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint_data, os.path.join(checkpoint_dir, filename))\n",
    "    print(f\"Checkpoint guardado en {os.path.join(checkpoint_dir, filename)}\")\n",
    "\n",
    "def checkpoint_resume(model, optimizer, filename):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        checkpoint_data = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint_data['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint_data['optimizer_state_dict'])\n",
    "        epoch = checkpoint_data['epoch']\n",
    "        print(f\"Checkpoint cargado desde '{checkpoint_path}' (época {epoch})\")\n",
    "        return epoch\n",
    "    else:\n",
    "        print(f\"No se encontró ningún checkpoint en '{checkpoint_path}'\")\n",
    "        return None\n",
    "\n",
    "# Training step function\n",
    "def train_step(x_batch, y_batch, model, optimizer, criterion, device):\n",
    "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "    y_predicted = model(x_batch)\n",
    "    loss = criterion(y_predicted, y_batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return y_predicted, loss, model.feature_maps\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(val_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    cumulative_loss = 0\n",
    "    cumulative_corrects = 0\n",
    "    data_count = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            final_output = model(imgs)\n",
    "            loss = criterion(final_output, labels)\n",
    "            cumulative_loss += loss.item() * len(labels)\n",
    "            data_count += len(labels)\n",
    "            _, pred_class = final_output.max(1)\n",
    "            cumulative_corrects += (pred_class == labels).sum().item()\n",
    "    val_acc = cumulative_corrects / data_count\n",
    "    val_loss = cumulative_loss / data_count\n",
    "    return val_acc, val_loss\n",
    "\n",
    "#Segunda Resnet\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    epochs,\n",
    "    max_iterations,\n",
    "    criterion,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    n_evaluations_per_epoch,\n",
    "    early_stop_thresh,  # Early stopping threshold\n",
    "    show_gradients,\n",
    "    patience,\n",
    "    use_gpu=True,\n",
    "    data_augmentation=False,\n",
    "    resume_checkpoint=None\n",
    "):\n",
    "    original_transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "    ])\n",
    "\n",
    "    if data_augmentation:\n",
    "        train_dataset.dataset.transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.RandomCrop(32, padding=4),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "        ])\n",
    "    else:\n",
    "        train_dataset.dataset.transform = original_transform\n",
    "\n",
    "    print(f\"Using train transform: {train_dataset.dataset.transform}\")\n",
    "    print(f\"Using validation transform: {val_dataset.dataset.transform}\")\n",
    "\n",
    "\n",
    "    # Usar GPU si está disponible\n",
    "    device = 'cuda:0'#torch.device('cuda' if use_gpu else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=use_gpu)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=128, shuffle=False,pin_memory=use_gpu)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience, threshold=0.0001, threshold_mode='abs')\n",
    "    scaler = torch.amp.GradScaler()\n",
    "\n",
    "    curves = {\"train_acc\": [], \"val_acc\": [], \"train_loss\": [], \"val_loss\": []}\n",
    "    t0 = time.perf_counter()\n",
    "    iteration = 0\n",
    "    n_batches = len(train_loader)\n",
    "    start_epoch = 0\n",
    "    if resume_checkpoint is not None:\n",
    "        start_epoch = checkpoint_resume(model, optimizer, resume_checkpoint)\n",
    "        print(f\"Reanudando desde la época {start_epoch}\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = -1\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(optimizer.param_groups[0][\"lr\"])\n",
    "        print(f\"\\rEpoch {epoch + 1}/{epochs}\")\n",
    "        cumulative_train_loss = 0\n",
    "        cumulative_train_corrects = 0\n",
    "        train_loss_count = 0\n",
    "        train_acc_count = 0\n",
    "\n",
    "        model.train()\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            y_predicted, loss, batch_feature_maps = train_step(x_batch, y_batch, model, optimizer, criterion, device)\n",
    "    \n",
    "\n",
    "            cumulative_train_loss += loss.item()\n",
    "            train_loss_count += 1\n",
    "            train_acc_count += y_batch.shape[0]\n",
    "\n",
    "            # Accuracy calculation\n",
    "            class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
    "            cumulative_train_corrects += (y_batch == class_prediction).sum().item()\n",
    "\n",
    "\n",
    "            # Registro de métricas\n",
    "            if (i + 1) % (n_batches // n_evaluations_per_epoch) == 0:\n",
    "                train_loss = cumulative_train_loss / train_loss_count\n",
    "                train_acc = cumulative_train_corrects / train_acc_count\n",
    "\n",
    "                print(\n",
    "                    f\"Iteración {iteration + 1} - Lote {i + 1}/{n_batches} - \"\n",
    "                    f\"Pérdida de Entrenamiento: {train_loss:.4f}, Precisión de Entrenamiento: {train_acc:.4f}\"\n",
    "                )\n",
    "\n",
    "\n",
    "            iteration += 1\n",
    "            if iteration >= max_iterations:\n",
    "                print(f\"Número máximo de iteraciones alcanzado: {max_iterations}\")\n",
    "                break\n",
    "\n",
    "        val_acc, val_loss = evaluate(val_loader, model, criterion, device)\n",
    "        print(f\"Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n",
    "\n",
    "        train_loss = cumulative_train_loss / train_loss_count\n",
    "        train_acc = cumulative_train_corrects / train_acc_count\n",
    "\n",
    "        curves[\"train_acc\"].append(train_acc)\n",
    "        curves[\"val_acc\"].append(val_acc)\n",
    "        curves[\"train_loss\"].append(train_loss)\n",
    "        curves[\"val_loss\"].append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Checkpointing the best model based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            early_stop_counter = 0\n",
    "            checkpoint_filename = f\"best_checkpoint_epoch_{epoch + 1}.pth\"\n",
    "            checkpoint_save(model, optimizer, epoch, checkpoint_filename)\n",
    "            print(f\"Checkpoint del mejor modelo guardado en la época {epoch + 1}\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if epoch + 1 == 1 or (epoch + 1) % 5 == 0 or early_stop_counter >= early_stop_thresh:\n",
    "            show_gradients(model, epoch + 1) \n",
    "            show_feature_maps(batch_feature_maps, epoch + 1)\n",
    "            \n",
    "        if early_stop_counter >= early_stop_thresh:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        if iteration >= max_iterations:\n",
    "            break\n",
    "\n",
    "    total_time = time.perf_counter() - t0\n",
    "    print(f\"\\nTiempo total de entrenamiento: {total_time:.2f} segundos\")\n",
    "\n",
    "    # Ensure the model is on CPU after training\n",
    "    model.cpu()\n",
    "\n",
    "    if data_augmentation:\n",
    "        train_dataset.dataset.transform = original_transform\n",
    "\n",
    "    return curves\n",
    "\n",
    "\n",
    "use_gpu = True\n",
    "\n",
    "inverted_residual_setting, last_channel = _efficientnet_conf(width_mult=1.0, depth_mult=1.0)\n",
    "# Checkpointing functions\n",
    "\n",
    "checkpoint_dir = r\"effnet/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize feature map dictionary\n",
    "feature_maps = {}\n",
    "if __name__ == \"__main__\":\n",
    "    # Hiperparámetros\n",
    "    batch_size = 128\n",
    "    epochs = 400\n",
    "    max_iterations = 600000\n",
    "    learning_rate = 0.1\n",
    "    n_evaluations_per_epoch = 10\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    data_augmentation = True\n",
    "    weight_decay = 1e-5\n",
    "    early_stop_thresh = 25\n",
    "    patience = 8\n",
    "    \n",
    "    # Crear directorio para logs si no existe\n",
    "    log_dir = os.path.join(checkpoint_dir, 'logs')\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Crear nombre de archivo de log con timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_filename = os.path.join(log_dir, f'training_log_{timestamp}.txt')\n",
    "    \n",
    "    # Inicializar el logger\n",
    "    sys.stdout = Logger(log_filename)\n",
    "    \n",
    "    print(f\"Iniciando entrenamiento - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"Configuración del entrenamiento:\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Weight decay: {weight_decay}\")\n",
    "    print(f\"Data augmentation: {data_augmentation}\")\n",
    "    print(f\"GPU disponible: {use_gpu}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    model = _efficientnet(inverted_residual_setting=inverted_residual_setting, dropout=0.2, last_channel=last_channel)\n",
    "    print(\"\\nArquitectura del modelo:\")\n",
    "    print(model)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Definir función de pérdida\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    try:\n",
    "        # Entrenar el modelo\n",
    "        curves = train_model(\n",
    "            model=model,\n",
    "            train_dataset=train_cifar10,\n",
    "            val_dataset=val_cifar10,\n",
    "            epochs=epochs,\n",
    "            max_iterations=max_iterations,\n",
    "            criterion=criterion,\n",
    "            batch_size=batch_size,\n",
    "            lr=learning_rate,\n",
    "            n_evaluations_per_epoch=n_evaluations_per_epoch,\n",
    "            early_stop_thresh=early_stop_thresh,\n",
    "            show_gradients=show_gradients_EfficientNet,\n",
    "            patience=patience,\n",
    "            use_gpu=use_gpu,\n",
    "            data_augmentation=data_augmentation,\n",
    "            resume_checkpoint=None,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "        print(\"\\nEntrenamiento completado exitosamente\")\n",
    "        \n",
    "        # Plotear curvas de entrenamiento\n",
    "        show_curves(curves)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError durante el entrenamiento: {str(e)}\")\n",
    "        raise e\n",
    "    \n",
    "    finally:\n",
    "        # Restaurar la salida estándar original\n",
    "        sys.stdout = sys.stdout.terminal\n",
    "\n",
    "use_gpu = True\n",
    "\n",
    "inverted_residual_setting, last_channel = _efficientnet_conf(width_mult=1.0, depth_mult=1.0)\n",
    "# Checkpointing functions\n",
    "\n",
    "checkpoint_dir = r\"effnet/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize feature map dictionary\n",
    "feature_maps = {}\n",
    "if __name__ == \"__main__\":\n",
    "    # Hiperparámetros\n",
    "    batch_size = 128\n",
    "    epochs = 400\n",
    "    max_iterations = 600000\n",
    "    learning_rate = 0.1\n",
    "    n_evaluations_per_epoch = 10\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    data_augmentation = True\n",
    "    weight_decay = 1e-3\n",
    "    early_stop_thresh = 25\n",
    "    patience = 8\n",
    "    \n",
    "    # Crear directorio para logs si no existe\n",
    "    log_dir = os.path.join(checkpoint_dir, 'logs')\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Crear nombre de archivo de log con timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_filename = os.path.join(log_dir, f'training_log_{timestamp}.txt')\n",
    "    \n",
    "    # Inicializar el logger\n",
    "    sys.stdout = Logger(log_filename)\n",
    "    \n",
    "    print(f\"Iniciando entrenamiento - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"Configuración del entrenamiento:\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Weight decay: {weight_decay}\")\n",
    "    print(f\"Data augmentation: {data_augmentation}\")\n",
    "    print(f\"GPU disponible: {use_gpu}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    model = _efficientnet(inverted_residual_setting=inverted_residual_setting, dropout=0.2, last_channel=last_channel)\n",
    "    print(\"\\nArquitectura del modelo:\")\n",
    "    print(model)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Definir función de pérdida\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    try:\n",
    "        # Entrenar el modelo\n",
    "        curves = train_model(\n",
    "            model=model,\n",
    "            train_dataset=train_cifar10,\n",
    "            val_dataset=val_cifar10,\n",
    "            epochs=epochs,\n",
    "            max_iterations=max_iterations,\n",
    "            criterion=criterion,\n",
    "            batch_size=batch_size,\n",
    "            lr=learning_rate,\n",
    "            n_evaluations_per_epoch=n_evaluations_per_epoch,\n",
    "            early_stop_thresh=early_stop_thresh,\n",
    "            show_gradients=show_gradients_EfficientNet,\n",
    "            patience=patience,\n",
    "            use_gpu=use_gpu,\n",
    "            data_augmentation=data_augmentation,\n",
    "            resume_checkpoint=None,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "        print(\"\\nEntrenamiento completado exitosamente\")\n",
    "        \n",
    "        # Plotear curvas de entrenamiento\n",
    "        show_curves(curves)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError durante el entrenamiento: {str(e)}\")\n",
    "        raise e\n",
    "    \n",
    "    finally:\n",
    "        # Restaurar la salida estándar original\n",
    "        sys.stdout = sys.stdout.terminal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EL4106",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
