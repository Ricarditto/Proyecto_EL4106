{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet32: Pruebas con distintas metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import umap.umap_ as umap\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.manifold import TSNE\n",
    "from torch import Tensor\n",
    "from typing import Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones para entrenamiento y validación\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomCrop(32, padding=4),  # Recorte aleatorio después del padding\n",
    "    torchvision.transforms.RandomHorizontalFlip(),      # Volteo horizontal aleatorio\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "])\n",
    "\n",
    "val_test_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "])\n",
    "\n",
    "# Cargar datasets CIFAR-10 con transformaciones\n",
    "train_cifar10 = torchvision.datasets.CIFAR10(\n",
    "    root=\"./cifar10\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=train_transform  # Se aplica data augmentation aquí\n",
    ")\n",
    "\n",
    "test_cifar10 = torchvision.datasets.CIFAR10(\n",
    "    root=\"./cifar10\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=val_test_transform  # Solo normalización para testing\n",
    ")\n",
    "\n",
    "# Dividir dataset de entrenamiento y validación\n",
    "train_cifar10, _ = torch.utils.data.random_split(train_cifar10, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
    "_, val_cifar10 = torch.utils.data.random_split(\n",
    "    torchvision.datasets.CIFAR10(root=\"./cifar10\", train=True, transform=val_test_transform),\n",
    "    [45000, 5000], generator=torch.Generator().manual_seed(42)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # Conexión de atajo\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Función residual\n",
    "        out = F.relu(self.bn1(self.conv1(x)))  # Aplica BN y luego ReLU\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# Modificar el modelo para devolver los embeddings de todas las capas\n",
    "class ResNet32(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet32, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # Capa de convolución inicial\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Capas residuales\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        # Pooling global y capa completamente conectada\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(block(self.in_planes, planes, s))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolución inicial\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        feature_maps['ReLUconv1'] = out\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        feature_maps['Layer2'] = out\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        feature_maps['Layer4'] = out\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        # Capa completamente conectada\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet_32():\n",
    "    return ResNet32(BasicBlock, [3, 4, 6, 2])\n",
    "\n",
    "model = ResNet_32()\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "\n",
    "shortcut_count = 0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, BasicBlock):\n",
    "        shortcut_count += 1\n",
    "\n",
    "print(f\"Total de shortcuts en el modelo: {shortcut_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gradients_ResNet_32(model):\n",
    "    layers_to_show = ['conv1', 'bn1', 'layer2.1.conv1', 'layer2.1.bn1', 'linear']  # Capas inicial, intermedia y final\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(layer in name for layer in layers_to_show) and param.requires_grad and param.grad is not None:\n",
    "            grad = param.grad.cpu().numpy()\n",
    "            print(f\"Gradientes para {name}: min={grad.min()}, max={grad.max()}, mean={grad.mean()}, std={grad.std()}\")\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.hist(grad.flatten(), bins=50)\n",
    "            plt.title(f'Gradientes para {name}')\n",
    "            plt.xlabel('Valor del gradiente')\n",
    "            plt.ylabel('Frecuencia')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Initialize He weights\n",
    "def initialize_weights_he(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out', nonlinearity='relu')\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def show_curves(curves):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "    fig.set_facecolor('white')\n",
    "\n",
    "    # Asegúrate de que los datos estén en la CPU antes de convertirlos a NumPy\n",
    "    epochs = np.arange(len(curves[\"val_loss\"])) + 1\n",
    "\n",
    "    ax[0].plot(epochs, np.array(curves['val_loss']), label='validation')\n",
    "    ax[0].plot(epochs, np.array(curves['train_loss']), label='training')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_title('Loss evolution during training')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(epochs, np.array(curves['val_acc']), label='validation')\n",
    "    ax[1].plot(epochs, np.array(curves['train_acc']), label='training')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_title('Accuracy evolution during training')\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize feature maps\n",
    "def show_feature_maps(feature_maps):\n",
    "    for layer_name, feature_map in feature_maps.items():\n",
    "        fmap = feature_map[0].cpu().numpy()  # Convert to CPU and NumPy for visualization\n",
    "        num_feature_maps = fmap.shape[0]\n",
    "\n",
    "        # Plot a grid of feature maps (first 8 feature maps)\n",
    "        fig, axes = plt.subplots(1, min(8, num_feature_maps), figsize=(20, 5))\n",
    "        fig.suptitle(f\"Feature Maps from Layer {layer_name}\", fontsize=16)\n",
    "\n",
    "        for i in range(min(8, num_feature_maps)):\n",
    "            axes[i].imshow(fmap[i], cmap='viridis')\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "def checkpoint_save(model, optimizer, epoch, filename):\n",
    "    checkpoint_data = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint_data, os.path.join(checkpoint_dir, filename))\n",
    "    print(f\"Checkpoint guardado en {os.path.join(checkpoint_dir, filename)}\")\n",
    "\n",
    "def checkpoint_resume(model, optimizer, filename):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        checkpoint_data = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint_data['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint_data['optimizer_state_dict'])\n",
    "        epoch = checkpoint_data['epoch']\n",
    "        print(f\"Checkpoint cargado desde '{checkpoint_path}' (época {epoch})\")\n",
    "        return epoch\n",
    "    else:\n",
    "        print(f\"No se encontró ningún checkpoint en '{checkpoint_path}'\")\n",
    "        return None\n",
    "\n",
    "# Training step function\n",
    "def train_step(x_batch, y_batch, model, optimizer, criterion, device):\n",
    "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "    y_predicted = model(x_batch)\n",
    "    loss = criterion(y_predicted, y_batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return y_predicted, loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(val_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    cumulative_loss = 0\n",
    "    cumulative_corrects = 0\n",
    "    data_count = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            final_output = model(imgs)\n",
    "            loss = criterion(final_output, labels)\n",
    "            cumulative_loss += loss.item() * len(labels)\n",
    "            data_count += len(labels)\n",
    "            _, pred_class = final_output.max(1)\n",
    "            cumulative_corrects += (pred_class == labels).sum().item()\n",
    "    val_acc = cumulative_corrects / data_count\n",
    "    val_loss = cumulative_loss / data_count\n",
    "    return val_acc, val_loss\n",
    "\n",
    "#Segunda Resnet\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    epochs,\n",
    "    max_iterations,\n",
    "    criterion,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    weight_decay,\n",
    "    n_evaluations_per_epoch,\n",
    "    early_stop_thresh,  # Early stopping threshold\n",
    "    show_gradients,\n",
    "    patience,\n",
    "    use_gpu=True,\n",
    "    data_augmentation=False,\n",
    "    resume_checkpoint=None\n",
    "):\n",
    "    original_transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "    ])\n",
    "\n",
    "    if data_augmentation:\n",
    "        train_dataset.dataset.transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.RandomCrop(32, padding=4),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "        ])\n",
    "    else:\n",
    "        train_dataset.dataset.transform = original_transform\n",
    "\n",
    "    print(f\"Using train transform: {train_dataset.dataset.transform}\")\n",
    "    print(f\"Using validation transform: {val_dataset.dataset.transform}\")\n",
    "\n",
    "\n",
    "    # Usar GPU si está disponible\n",
    "    device = 'cuda:0'#torch.device('cuda' if use_gpu else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=use_gpu)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False, pin_memory=use_gpu)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=patience, threshold=0.0001, threshold_mode='abs')\n",
    "    scaler = torch.amp.GradScaler()\n",
    "\n",
    "    curves = {\"train_acc\": [], \"val_acc\": [], \"train_loss\": [], \"val_loss\": []}\n",
    "    t0 = time.perf_counter()\n",
    "    iteration = 0\n",
    "    n_batches = len(train_loader)\n",
    "    start_epoch = 0\n",
    "    if resume_checkpoint is not None:\n",
    "        start_epoch = checkpoint_resume(model, optimizer, resume_checkpoint)\n",
    "        print(f\"Reanudando desde la época {start_epoch}\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = -1\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(optimizer.param_groups[0][\"lr\"])\n",
    "        print(f\"\\rEpoch {epoch + 1}/{epochs}\")\n",
    "        cumulative_train_loss = 0\n",
    "        cumulative_train_corrects = 0\n",
    "        train_loss_count = 0\n",
    "        train_acc_count = 0\n",
    "\n",
    "        model.train()\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            y_predicted, loss = train_step(x_batch, y_batch, model, optimizer, criterion, device)\n",
    "\n",
    "            cumulative_train_loss += loss.item()\n",
    "            train_loss_count += 1\n",
    "            train_acc_count += y_batch.shape[0]\n",
    "\n",
    "            # Accuracy calculation\n",
    "            class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
    "            cumulative_train_corrects += (y_batch == class_prediction).sum().item()\n",
    "\n",
    "\n",
    "            # Registro de métricas\n",
    "            if (i + 1) % (n_batches // n_evaluations_per_epoch) == 0:\n",
    "                train_loss = cumulative_train_loss / train_loss_count\n",
    "                train_acc = cumulative_train_corrects / train_acc_count\n",
    "\n",
    "                print(\n",
    "                    f\"Iteración {iteration + 1} - Lote {i + 1}/{n_batches} - \"\n",
    "                    f\"Pérdida de Entrenamiento: {train_loss:.4f}, Precisión de Entrenamiento: {train_acc:.4f}\"\n",
    "                )\n",
    "\n",
    "\n",
    "            iteration += 1\n",
    "            if iteration >= max_iterations:\n",
    "                print(f\"Número máximo de iteraciones alcanzado: {max_iterations}\")\n",
    "                break\n",
    "\n",
    "        val_acc, val_loss = evaluate(val_loader, model, criterion, device)\n",
    "        print(f\"Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n",
    "\n",
    "        train_loss = cumulative_train_loss / train_loss_count\n",
    "        train_acc = cumulative_train_corrects / train_acc_count\n",
    "\n",
    "        curves[\"train_acc\"].append(train_acc)\n",
    "        curves[\"val_acc\"].append(val_acc)\n",
    "        curves[\"train_loss\"].append(train_loss)\n",
    "        curves[\"val_loss\"].append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Checkpointing the best model based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            early_stop_counter = 0\n",
    "            checkpoint_filename = f\"best_checkpoint_epoch_{epoch + 1}.pth\"\n",
    "            checkpoint_save(model, optimizer, epoch, checkpoint_filename)\n",
    "            print(f\"Checkpoint del mejor modelo guardado en la época {epoch + 1}\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if epoch + 1 == 1 or (epoch + 1) % 5 == 0 or early_stop_counter >= early_stop_thresh:\n",
    "            show_gradients(model)\n",
    "            show_feature_maps(feature_maps)\n",
    "\n",
    "        if early_stop_counter >= early_stop_thresh:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        if iteration >= max_iterations:\n",
    "            break\n",
    "\n",
    "    total_time = time.perf_counter() - t0\n",
    "    print(f\"\\nTiempo total de entrenamiento: {total_time:.2f} segundos\")\n",
    "\n",
    "    # Ensure the model is on CPU after training\n",
    "    model.cpu()\n",
    "\n",
    "    if data_augmentation:\n",
    "        train_dataset.dataset.transform = original_transform\n",
    "\n",
    "    return curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "\n",
    "# Cambia checkpoint_dir para que apunte al directorio que deseas\n",
    "checkpoint_dir = r\"C:\\Users\\benit\\OneDrive\\Escritorio\\4to Semestre Electrica\\Inteligencia Computacional\\Proyecto Inteligencia\\ResNet32_Prueba_Batch64\"\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize feature map dictionary\n",
    "feature_maps = {}\n",
    "if __name__ == \"__main__\":\n",
    "    # Hiperparámetros\n",
    "    batch_size = 64\n",
    "    epochs = 70\n",
    "    max_iterations = 600000  # Ajusta según sea necesario\n",
    "    learning_rate = 0.1\n",
    "    n_evaluations_per_epoch = 10\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    data_augmentation = True\n",
    "    weight_decay = 0.0001\n",
    "    early_stop_thresh = 15\n",
    "    patience = 3\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    model = ResNet_32()\n",
    "    initialize_weights_he(model)\n",
    "    print(model)\n",
    "\n",
    "    # Definir función de pérdida\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    curves = train_model(\n",
    "        model=model,\n",
    "        train_dataset=train_cifar10,\n",
    "        val_dataset=val_cifar10,\n",
    "        epochs=epochs,\n",
    "        max_iterations=max_iterations,\n",
    "        criterion=criterion,\n",
    "        batch_size=batch_size,\n",
    "        lr=learning_rate,\n",
    "        n_evaluations_per_epoch=n_evaluations_per_epoch,\n",
    "        early_stop_thresh=early_stop_thresh,\n",
    "        show_gradients=show_gradients_ResNet_32,\n",
    "        patience=patience,\n",
    "        use_gpu=use_gpu,\n",
    "        data_augmentation=data_augmentation,\n",
    "        resume_checkpoint=None,  # Establece a una cadena de caracteres para reanudar, por ejemplo, \"best_checkpoint_epoch_10.pth\"\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Plotear curvas de entrenamiento\n",
    "    show_curves(curves)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "\n",
    "# Cambia checkpoint_dir para que apunte al directorio que deseas\n",
    "checkpoint_dir = r\"C:\\Users\\benit\\OneDrive\\Escritorio\\4to Semestre Electrica\\Inteligencia Computacional\\Proyecto Inteligencia\\ResNet32_Prueba_Batch256\"\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize feature map dictionary\n",
    "feature_maps = {}\n",
    "if __name__ == \"__main__\":\n",
    "    # Hiperparámetros\n",
    "    batch_size = 256\n",
    "    epochs = 70\n",
    "    max_iterations = 600000  # Ajusta según sea necesario\n",
    "    learning_rate = 0.1\n",
    "    n_evaluations_per_epoch = 10\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    data_augmentation = True\n",
    "    weight_decay = 0.0001\n",
    "    early_stop_thresh = 15\n",
    "    patience = 3\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    model = ResNet_32()\n",
    "    initialize_weights_he(model)\n",
    "    print(model)\n",
    "\n",
    "    # Definir función de pérdida\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    curves = train_model(\n",
    "        model=model,\n",
    "        train_dataset=train_cifar10,\n",
    "        val_dataset=val_cifar10,\n",
    "        epochs=epochs,\n",
    "        max_iterations=max_iterations,\n",
    "        criterion=criterion,\n",
    "        batch_size=batch_size,\n",
    "        lr=learning_rate,\n",
    "        n_evaluations_per_epoch=n_evaluations_per_epoch,\n",
    "        early_stop_thresh=early_stop_thresh,\n",
    "        show_gradients=show_gradients_ResNet_32,\n",
    "        patience=patience,\n",
    "        use_gpu=use_gpu,\n",
    "        data_augmentation=data_augmentation,\n",
    "        resume_checkpoint=None,  # Establece a una cadena de caracteres para reanudar, por ejemplo, \"best_checkpoint_epoch_10.pth\"\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Plotear curvas de entrenamiento\n",
    "    show_curves(curves)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "\n",
    "# Cambia checkpoint_dir para que apunte al directorio que deseas\n",
    "checkpoint_dir = r\"C:\\Users\\benit\\OneDrive\\Escritorio\\4to Semestre Electrica\\Inteligencia Computacional\\Proyecto Inteligencia\\ResNet32_Prueba_LR_2\"\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize feature map dictionary\n",
    "feature_maps = {}\n",
    "if __name__ == \"__main__\":\n",
    "    # Hiperparámetros\n",
    "    batch_size = 128\n",
    "    epochs = 70\n",
    "    max_iterations = 600000  # Ajusta según sea necesario\n",
    "    learning_rate = 0.01\n",
    "    n_evaluations_per_epoch = 10\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    data_augmentation = True\n",
    "    weight_decay = 0.0001\n",
    "    early_stop_thresh = 15\n",
    "    patience = 3\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    model = ResNet_32()\n",
    "    initialize_weights_he(model)\n",
    "    print(model)\n",
    "\n",
    "    # Definir función de pérdida\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    curves = train_model(\n",
    "        model=model,\n",
    "        train_dataset=train_cifar10,\n",
    "        val_dataset=val_cifar10,\n",
    "        epochs=epochs,\n",
    "        max_iterations=max_iterations,\n",
    "        criterion=criterion,\n",
    "        batch_size=batch_size,\n",
    "        lr=learning_rate,\n",
    "        n_evaluations_per_epoch=n_evaluations_per_epoch,\n",
    "        early_stop_thresh=early_stop_thresh,\n",
    "        show_gradients=show_gradients_ResNet_32,\n",
    "        patience=patience,\n",
    "        use_gpu=use_gpu,\n",
    "        data_augmentation=data_augmentation,\n",
    "        resume_checkpoint=None,  # Establece a una cadena de caracteres para reanudar, por ejemplo, \"best_checkpoint_epoch_10.pth\"\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Plotear curvas de entrenamiento\n",
    "    show_curves(curves)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "\n",
    "# Cambia checkpoint_dir para que apunte al directorio que deseas\n",
    "checkpoint_dir = r\"C:\\Users\\benit\\OneDrive\\Escritorio\\4to Semestre Electrica\\Inteligencia Computacional\\Proyecto Inteligencia\\ResNet32_Prueba_LR_3\"\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize feature map dictionary\n",
    "feature_maps = {}\n",
    "if __name__ == \"__main__\":\n",
    "    # Hiperparámetros\n",
    "    batch_size = 128\n",
    "    epochs = 70\n",
    "    max_iterations = 600000  # Ajusta según sea necesario\n",
    "    learning_rate = 0.001\n",
    "    n_evaluations_per_epoch = 10\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    data_augmentation = True\n",
    "    weight_decay = 0.0001\n",
    "    early_stop_thresh = 15\n",
    "    patience = 3\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    model = ResNet_32()\n",
    "    initialize_weights_he(model)\n",
    "    print(model)\n",
    "\n",
    "    # Definir función de pérdida\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    curves = train_model(\n",
    "        model=model,\n",
    "        train_dataset=train_cifar10,\n",
    "        val_dataset=val_cifar10,\n",
    "        epochs=epochs,\n",
    "        max_iterations=max_iterations,\n",
    "        criterion=criterion,\n",
    "        batch_size=batch_size,\n",
    "        lr=learning_rate,\n",
    "        n_evaluations_per_epoch=n_evaluations_per_epoch,\n",
    "        early_stop_thresh=early_stop_thresh,\n",
    "        show_gradients=show_gradients_ResNet_32,\n",
    "        patience=patience,\n",
    "        use_gpu=use_gpu,\n",
    "        data_augmentation=data_augmentation,\n",
    "        resume_checkpoint=None,  # Establece a una cadena de caracteres para reanudar, por ejemplo, \"best_checkpoint_epoch_10.pth\"\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Plotear curvas de entrenamiento\n",
    "    show_curves(curves)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "\n",
    "# Cambia checkpoint_dir para que apunte al directorio que deseas\n",
    "checkpoint_dir = r\"C:\\Users\\benit\\OneDrive\\Escritorio\\4to Semestre Electrica\\Inteligencia Computacional\\Proyecto Inteligencia\\ResNet32_Prueba_WD_1\"\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize feature map dictionary\n",
    "feature_maps = {}\n",
    "if __name__ == \"__main__\":\n",
    "    # Hiperparámetros\n",
    "    batch_size = 128\n",
    "    epochs = 70\n",
    "    max_iterations = 600000  # Ajusta según sea necesario\n",
    "    learning_rate = 0.01\n",
    "    n_evaluations_per_epoch = 10\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    data_augmentation = True\n",
    "    weight_decay = 0.01\n",
    "    early_stop_thresh = 15\n",
    "    patience = 3\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    model = ResNet_32()\n",
    "    initialize_weights_he(model)\n",
    "    print(model)\n",
    "\n",
    "    # Definir función de pérdida\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    curves = train_model(\n",
    "        model=model,\n",
    "        train_dataset=train_cifar10,\n",
    "        val_dataset=val_cifar10,\n",
    "        epochs=epochs,\n",
    "        max_iterations=max_iterations,\n",
    "        criterion=criterion,\n",
    "        batch_size=batch_size,\n",
    "        lr=learning_rate,\n",
    "        n_evaluations_per_epoch=n_evaluations_per_epoch,\n",
    "        early_stop_thresh=early_stop_thresh,\n",
    "        show_gradients=show_gradients_ResNet_32,\n",
    "        patience=patience,\n",
    "        use_gpu=use_gpu,\n",
    "        data_augmentation=data_augmentation,\n",
    "        resume_checkpoint=None,  # Establece a una cadena de caracteres para reanudar, por ejemplo, \"best_checkpoint_epoch_10.pth\"\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Plotear curvas de entrenamiento\n",
    "    show_curves(curves)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "\n",
    "# Cambia checkpoint_dir para que apunte al directorio que deseas\n",
    "checkpoint_dir = r\"C:\\Users\\benit\\OneDrive\\Escritorio\\4to Semestre Electrica\\Inteligencia Computacional\\Proyecto Inteligencia\\ResNet32_Prueba_WD_1\"\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize feature map dictionary\n",
    "feature_maps = {}\n",
    "if __name__ == \"__main__\":\n",
    "    # Hiperparámetros\n",
    "    batch_size = 128\n",
    "    epochs = 70\n",
    "    max_iterations = 600000  # Ajusta según sea necesario\n",
    "    learning_rate = 0.01\n",
    "    n_evaluations_per_epoch = 10\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    data_augmentation = True\n",
    "    weight_decay = 0.001\n",
    "    early_stop_thresh = 15\n",
    "    patience = 3\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    model = ResNet_32()\n",
    "    initialize_weights_he(model)\n",
    "    print(model)\n",
    "\n",
    "    # Definir función de pérdida\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    curves = train_model(\n",
    "        model=model,\n",
    "        train_dataset=train_cifar10,\n",
    "        val_dataset=val_cifar10,\n",
    "        epochs=epochs,\n",
    "        max_iterations=max_iterations,\n",
    "        criterion=criterion,\n",
    "        batch_size=batch_size,\n",
    "        lr=learning_rate,\n",
    "        n_evaluations_per_epoch=n_evaluations_per_epoch,\n",
    "        early_stop_thresh=early_stop_thresh,\n",
    "        show_gradients=show_gradients_ResNet_32,\n",
    "        patience=patience,\n",
    "        use_gpu=use_gpu,\n",
    "        data_augmentation=data_augmentation,\n",
    "        resume_checkpoint=None,  # Establece a una cadena de caracteres para reanudar, por ejemplo, \"best_checkpoint_epoch_10.pth\"\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Plotear curvas de entrenamiento\n",
    "    show_curves(curves)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "\n",
    "# Cambia checkpoint_dir para que apunte al directorio que deseas\n",
    "checkpoint_dir = r\"C:\\Users\\benit\\OneDrive\\Escritorio\\4to Semestre Electrica\\Inteligencia Computacional\\Proyecto Inteligencia\\ResNet32_Prueba_WD_1\"\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Initialize feature map dictionary\n",
    "feature_maps = {}\n",
    "if __name__ == \"__main__\":\n",
    "    # Hiperparámetros\n",
    "    batch_size = 128\n",
    "    epochs = 70\n",
    "    max_iterations = 600000  # Ajusta según sea necesario\n",
    "    learning_rate = 0.01\n",
    "    n_evaluations_per_epoch = 10\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    data_augmentation = True\n",
    "    weight_decay = 0.00001\n",
    "    early_stop_thresh = 15\n",
    "    patience = 3\n",
    "\n",
    "    # Inicializar el modelo\n",
    "    model = ResNet_32()\n",
    "    initialize_weights_he(model)\n",
    "    print(model)\n",
    "\n",
    "    # Definir función de pérdida\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    curves = train_model(\n",
    "        model=model,\n",
    "        train_dataset=train_cifar10,\n",
    "        val_dataset=val_cifar10,\n",
    "        epochs=epochs,\n",
    "        max_iterations=max_iterations,\n",
    "        criterion=criterion,\n",
    "        batch_size=batch_size,\n",
    "        lr=learning_rate,\n",
    "        n_evaluations_per_epoch=n_evaluations_per_epoch,\n",
    "        early_stop_thresh=early_stop_thresh,\n",
    "        show_gradients=show_gradients_ResNet_32,\n",
    "        patience=patience,\n",
    "        use_gpu=use_gpu,\n",
    "        data_augmentation=data_augmentation,\n",
    "        resume_checkpoint=None,  # Establece a una cadena de caracteres para reanudar, por ejemplo, \"best_checkpoint_epoch_10.pth\"\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Plotear curvas de entrenamiento\n",
    "    show_curves(curves)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
